<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where to Go Next for Recommender Systems? ID-vs. Modality-based Recommender Models Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-02">2 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
							<email>yuanzheng@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
							<email>yuanfajie@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Song</surname></persName>
							<email>songyu@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youhua</forename><surname>Li</surname></persName>
							<email>liyouhua@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchen</forename><surname>Fu</surname></persName>
							<email>fujunchen@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Yang</surname></persName>
							<email>yangf@zhejianglab.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunzhu</forename><surname>Pan</surname></persName>
							<email>panyunzhu@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongxin</forename><surname>Ni</surname></persName>
							<email>niyongxin@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution" key="instit1">Westlake University</orgName>
								<orgName type="institution" key="instit2">Westlake University</orgName>
								<orgName type="institution" key="instit3">Westlake University</orgName>
								<orgName type="institution" key="instit4">Westlake University</orgName>
								<orgName type="institution" key="instit5">Westlake University</orgName>
								<orgName type="institution" key="instit6">Zhejiang</orgName>
								<orgName type="institution" key="instit7">Westlake University</orgName>
								<orgName type="institution" key="instit8">Westlake University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Where to Go Next for Recommender Systems? ID-vs. Modality-based Recommender Models Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-02">2 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">8466E515DBCC3C338863CB690BF7A441</idno>
					<idno type="DOI">10.1145/3539618.3591932</idno>
					<idno type="arXiv">arXiv:2303.13835v4[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-12T00:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information systems â†’ Recommender systems Recommender Systems</term>
					<term>ID-based Recommendation</term>
					<term>Modality-based Recommendation</term>
					<term>End-to-end Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommendation models that utilize unique identities (IDs for short) to represent distinct users and items have been state-of-theart (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT [9] and Vision Transformer [11], have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency.</p><p>We aim to revisit this 'old' question and systematically study MoRec from several aspects. Specifically, we study several subquestions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and warm item scenarios where IDRec has a strong advantage? does this hold for items with different modality features? (ii) can the latest technical advances from other communities (i.e., natural language processing and computer vision) translate into accuracy improvement for MoRec? (iii) how to effectively utilize item modality representation, can we use it directly or do we have to adjust it with new data? (iv) are there any key challenges that MoRec needs to address in practical applications? To answer them, we conduct rigorous experiments for item recommendations with two popular modalities, i.e., text and vision. We provide the first empirical evidence that MoRec is already comparable to its IDRec counterpart with an expensive end-to-end training method, even for warm item</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender system (RS) models learn the historical interactions of users and items and recommend items that users may interact with in the future. RS is playing a key role in search engines, advertising systems, e-commerce websites, video and music streaming services, and various other Internet platforms. The modern recommendation models usually use unique identities (ID) to represent users and items, which are subsequently converted to embedding vectors as learnable parameters. These ID-based recommendation models (IDRec) have been well-established and dominated the RS field for over a decade until now <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b76">77]</ref>.</p><p>Despite that, IDRec has key weaknesses that can not be ignored. First, IDRec highly relies on the ID interactions, which fails to provide recommendations when users and items have few interactions <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b75">76]</ref>, a.k.a. the cold-start setting. Second, pre-trained IDRec is not transferable across platforms given that userIDs and itemIDs are in general not shareable in practice. This issue seriously limits the development of big &amp; general-purpose RS models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b60">61]</ref>, an emerging paradigm in the deep learning community. Third, pure IDRec cannot benefit from technical advances in other communities, such as powerful foundation models (FM) <ref type="bibr" target="#b0">[1]</ref> developed in NLP (natural language processing) and CV (computer vision) areas. Moreover, maintaining a large and frequently updated ID embedding matrix for users and items remains a key challenge in industrial applications <ref type="bibr" target="#b55">[56]</ref>. Last but not the least, recommendation models leveraging ID features have obvious drawbacks in terms of interpretability, visualization and evaluation.</p><p>One way to address these issues is to replace the ID embedding (of IDRec) with an item modality encoder (ME), especially when item modality features such as images and text are available. We refer to such recommendation models as MoRec. In fact, such MoRec appeared in literature many years ago but it was mainly used to solve cold-start or cross-domain recommendation problems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b57">58]</ref>. In other words, MoRec is rarely adopted when recommending non-cold or popular items unless combined with other effective features, such as the itemID features, e.g., in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b59">60]</ref>. A key reason might be that these item ME developed in the past years (e.g., word embedding <ref type="bibr" target="#b39">[40]</ref> and some shallow neural networks <ref type="bibr" target="#b57">[58]</ref>) are not as expressive as typical itemID embeddings. Today, however, given the recent great success of FM, we think it is time to revisit the key comparison between modern MoRec and IDRec, especially for regular (or non cold-item) recommendation. For example, BERT <ref type="bibr" target="#b8">[9]</ref>, GPT-3 <ref type="bibr" target="#b1">[2]</ref> and various Vision Transformers (ViT) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref> have revolutionized the NLP and CV fields when representing textual and visual features. Whether item representations learned by them are better suited for the regular recommendation task than ID features remains unknown.</p><p>In this paper, we intend to rethink the potential of MoRec and investigate a key question: should we still stick to the IDRec paradigm for future recommender systems? We concentrate on item recommendation based on the text and vision modalities -the two most common modalities in literature. To be concise, we attempt to address the following sub-questions:</p><p>Q(i): Equipped with strong modality encoders (ME), can MoRec be comparable to or even surpass IDRec in regular, especially in warm-start item recommendation scenario? To answer this question, we conduct empirical studies by taking into account the two most representative recommendation architectures (i.e., two-tower based DSSM <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref> and session-based SASRec <ref type="bibr" target="#b24">[25]</ref>) equipped with four powerful ME evaluated on three large-scale recommendation datasets with two modalities (text and vision).</p><p>Novelty clarification: Though much previous literature has studied MoRec and compared with many baselines <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b78">79]</ref>, unfortunately none of them provided a fair or rigorous comparison between their proposed MoRec and the corresponding IDRec counterparts in regular or even warm item recommendation setting. Fair comparison here means that MoRec and IDRec should at least be compared with the same backbone network and experimental settings, such as samplers and loss functions. Without a fair comparison, the community can not truly assess the progress of MoRec and the expressive power of ME for recommendation.</p><p>Q(ii): If Q(i) is yes, can the recent technical advances developed in NLP and CV fields translate into accuracy improvement in MoRec when using text and visual features? We address this question by performing three experiments. First, we evaluate MoRec by comparing smaller vs larger ME given that pretrained ME with larger model sizes tends to perform better than their smaller counterparts in various downstream tasks; second, we evaluate MoRec by comparing weaker vs stronger ME where weaker and stronger are determined by NLP and CV tasks; third, we evaluate MoRec by comparing ME with vs without pre-training on corresponding NLP and CV datasets.</p><p>Q(iii): Are the representations learned by these foundation models as general as claimed? How can we effectively use item modality representations derived from an NLP or CV encoder network? A desirable goal of FM research is to develop models that generate universal representations that can be directly used for various downstream tasks <ref type="bibr" target="#b33">[34]</ref>. We examine this by first extracting frozen modality features from well-known ME and then adding them as common features for recommendation models, often referred to as the two-stage (TS) paradigm. This is a common practice for large-scale industrial recommender systems due to training efficiency consideration <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. We then compare TS with joint or end-to-end (E2E) training of both the recommendation architecture and ME.</p><p>Novelty clarification: Though several recent literature has explored E2E learning <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> for recommendation, few of them explicitly discussed the substantial accuracy and efficiency gap (more than 100x) between TS and E2E paradigms. More importantly, most of them only discussed the DSSM architecture (or other two-tower variants) without considering more powerful and computationally more expensive sequence-to-sequence (seq2seq) training approach (e.g., used in SASRec and NextItNet <ref type="bibr" target="#b74">[75]</ref>). Furthermore, all of them are only for text recommendation, and so far there is no modern (last 5 years) peer-reviewed literature considering the E2E learning paradigm for image recommendation.</p><p>In addition to the aforementioned key questions, we have also identified several challenges that remain unexplored for MoRec when utilizing the end-to-end learning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IDREC &amp; MOREC</head><p>One core function of a recommendation model is to represent items and users and calculate their matching score. Denote I (of size |I|) and U (of size |U|) as the set of items and users, respectively. For an item ğ‘– âˆˆ I, we can represent it either by its unique ID ğ‘– or its modality content, such as text and visual features. Likewise, for a user ğ‘¢ âˆˆ U, we can represent her either by the unique ID ğ‘¢ or the profile of ğ‘¢, where a profile can be the demographic information or a sequence of interacted items.</p><p>In IDRec, an ID embedding matrix X I âˆˆ R |I |Ã—ğ‘‘ is initialized, where ğ‘‘ is the embedding size. Each vector in X I represents the latent space of an item ğ‘–, and can be viewed as a simple item encoder. During training and inference, IDRec retrieves X I ğ‘– âˆˆ R ğ‘‘ from X I as the embedding of ğ‘– and then feeds it to the recommendation network.</p><p>In MoRec, items are assumed to contain modality information. For item ğ‘–, MoRec uses ME to generate the representation for the raw modality feature of ğ‘– and uses it to replace the ID embedding vector in IDRec. For instance, in the news recommendation scenario, we can use the pre-trained BERT or RoBERTa <ref type="bibr" target="#b35">[36]</ref> as text ME and represent a piece of news by the output textual representation of its</p><formula xml:id="formula_0">Transformer Blocks Item Encoder ID 2 ID 3 ID L ID 1 â€¦ T 1 T 2 T 3 T L â€¦ V 1 V 2 V 3 V L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦</head><p>The best-selling Nintendo Switch games In this paper, we perform rigorous empirical studies on two most commonly adopted recommendation paradigms: DSSM <ref type="bibr" target="#b23">[24]</ref> and SASRec <ref type="bibr" target="#b24">[25]</ref>. 1 The original DSSM model is a two-tower based architecture where users/items are encoded by their own encoder networks with user and item IDs as input. SASRec is a well-known sequential recommendation model based on multi-head self-attention (MHSA) <ref type="bibr" target="#b58">[59]</ref> which describes a user by her interacted item ID sequence. As mentioned before, by replacing ID embeddings with an item ME, we obtain their MoRec versions for both DSSM and SASRec. We illustrate IDRec and MoRec in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DT: Dimension Transformation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Details</head><p>Denote R as the set of all observed interactions in the training set. For each positive &lt; ğ‘¢, ğ‘– &gt;âˆˆ R, we randomly draw a negative sample &lt; ğ‘¢, ğ‘— &gt;âˆ‰ R in each training epoch, following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49]</ref>. The positive and sampled negative interactions can form the training set R ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› . Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, we adopt the widely used binary cross entropy loss as the objective function for both DSSM and SASRec, and their MoRec versions for a fair comparison:</p><formula xml:id="formula_1">ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£³ ğ‘šğ‘–ğ‘› - âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘– âˆˆ [2,...,ğ¿] log(ğœ ( Å·ğ‘¢ğ‘– )) + log(1 -ğœ ( Å·ğ‘¢ ğ‘— )) SASRec ğ‘šğ‘–ğ‘› - âˆ‘ï¸ &lt;ğ‘¢,ğ‘–,ğ‘— &gt; âˆˆR log(ğœ ( Å·ğ‘¢ğ‘– )) + log(1 -ğœ ( Å·ğ‘¢ ğ‘— )) DSSM</formula><p>(1) where ğœ (ğ‘¥) = 1/(1+ğ‘’ -ğ‘¥ ) is the sigmoid function, ğ¿ is the interaction sequence length of user ğ‘¢. ğ‘– and ğ‘— denotes positive and negative item respectively for ğ‘¢, Å·ğ‘¢ğ‘– is the matching score between hidden vectors of user (ğ‘¢) encoder and item (ğ‘–) encoder. Note that SASRec's user encoder (by seq2seq training) produces a different hidden vector at 1 We did not study other CTR (click-through rate) prediction models, as they essentially belong to the same category as DSSM, with the key difference being that many CTR models are based on single-tower backbone networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b76">77]</ref>. Intuitively, such difference generally does not affect our subsequent conclusions (see section 4.1), since improvement from a two-tower backbone to a single-tower is often limited if having the same training manners <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b80">81]</ref>. However, DSSM or CTR models are quite different from the seq2seq-based sequential recommendation models, such as SASRec. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, SASRec has ğ¿ -1 loss functions for each interaction sequence (input: 1, 2, ..., ğ¿ -1, predict: 2, ..., ğ¿), while DSSM (or other CTR models) typically uses one loss function to predict an interaction of a &lt; ğ‘¢, ğ‘– &gt; pair.</p><p>each position of the interaction sequence. Without special mention, all parameters of the entire recommendation model are optimized during training in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUPS 3.1 Datasets</head><p>We evaluate IDRec and MoRec on three real-world datasets, namely, the MIND news clicks dataset from the Microsoft news recommendation platform <ref type="bibr" target="#b66">[67]</ref>, the HM clothing purchase dataset from the H&amp;M platform<ref type="foot" target="#foot_0">foot_0</ref> and the Bili<ref type="foot" target="#foot_1">foot_1</ref> comment dataset from an online video recommendation platform. <ref type="foot" target="#foot_2">4</ref> Purchases and comments can be considered implicit click signals, as it is reasonable to assume that the user has clicked on the item before making a purchase or leaving a comment. However, we cannot assume the opposite holds, which is a common property in most recommendation datasets, i.e. unobserved items can be either positive or negative for the user.</p><p>To ensure a fair comparison between IDRec and MoRec, the dataset used should guarantee that users' clicking decisions on an item are solely based on the modality content features of the item. Intuitively, the cover of an image or video and the title of a news article, play a crucial role in providing users with the very first impression of an item. This impression significantly influences their decision to click on the item. Therefore, in MIND, we represent items by their news article titles, while in HM &amp; Bili, we represent items using their corresponding cover images. Nevertheless, it is still possible that these datasets may not perfectly meet the requirement. Particularly, within the e-commerce context of the HM dataset, factors such as the item's cover image, price, and sales volume may collectively influence a user's decision to click on an item (refer to Figure <ref type="figure" target="#fig_2">2</ref>). This means relying solely on a cover image in the HM dataset may not be adequate for MoRec to effectively capture these non-visual features, as it is the only input to the item encoder. In contrast, IDRec is known to be able to implicitly learn such   </p><formula xml:id="formula_2">Dataset ğ‘› ğ‘š |R| ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› |R ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ | |R ğ‘¡ğ‘’ğ‘ ğ‘¡ | |R ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› |/(ğ‘›ğ‘š) MIND 630ğ¾ 80ğ¾ 8,407ğ¾ 630ğ¾ 630ğ¾ 0.0167% HM 500ğ¾ 87ğ¾ 5,500ğ¾ 500ğ¾ 500ğ¾ 0.0127% Bili 400ğ¾ 128ğ¾ 4,400ğ¾ 400ğ¾ 400ğ¾ 0.0086%</formula><p>features from the latent embedding space <ref type="bibr" target="#b27">[28]</ref>. That is, MoRec's performance may still have room for improvement if a more ideal <ref type="foot" target="#foot_3">5</ref>dataset or more useful content features were taken into account.</p><p>To construct the datasets for experiments, we randomly select around 400ğ¾, 500ğ¾ and 600ğ¾ users from Bili, HM, and MIND, respectively. Then, we perform basic pre-processing by setting the size of all images to 224 Ã— 224 and the title of all news articles to a maximum of 30 tokens (covering 99% of descriptions). For MIND, we select the latest 23 items for each user to construct the interaction sequence. For HM and Bili, we choose the 13 most recent interactions since encoding images requires much larger GPU memory (especially with the SASRec architecture). Following <ref type="bibr" target="#b48">[49]</ref>, we remove users with less than 5 interactions, simply because we do not consider cold user settings in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameters</head><p>For all methods, we employ an AdamW <ref type="bibr" target="#b37">[38]</ref> as the default optimizer and find that the dropout rate set to 0.1 (i.e., removing 10% parameters) offers the optimal results on the validation set. Regarding other hyper-parameters, we follow the common practice and perform extensive searching. For IDRec, we tune the learning rate ğ›¾ from {1ğ‘’-3, 5ğ‘’-4, 1ğ‘’-4, 5ğ‘’-5}, the embedding/hidden size ğ‘‘ from {64, 128, 256, 512, 1024, 2048, 4096}. We set batch size ğ‘ to 1024 for DSSM and 128 for SASRec. For MoRec, we set ğ‘‘ to 512 for both DSSM and SASRec, ğ‘ to 512 and 64 for DSSM and SASRec respectively due to GPU memory constraints. Given that ME (e.g., BERT and ResNet) has already well pre-trained parameters, we use relatively smaller ğ›¾ than other parts in the recommender model.</p><p>That is, we search ğ›¾ from {1ğ‘’-4, 5ğ‘’-5, 1ğ‘’-5} for the pre-trained ME networks, and set ğ›¾ to 1ğ‘’-4 for other parts with randomly initialized parameters. Finally, we tune the weight decay ğ›½ from {0.1, 0.01, 0} for both IDRec and MoRec.</p><p>For the MLPs (multilayer perceptron) used in DSSM, we initially set their middle layer size to ğ‘‘ as well and search the layer number ğ‘™ from {0, 1, 3, 5} but find that ğ‘™ = 0 (i.e., no hidden layers) always produces the best results. For the Transformer block used in SASRec, we set ğ‘™ to 2 and the head number of the multi-head attention to 2 for the optimal results. All other hyper-parameters are kept the same for IDRec and MoRec unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Settings</head><p>For a fair comparison, we ensure that IDRec and MoRec have exactly the same network architecture except for the item encoder. For both text and vision encoders, we pass their output item representations to a DT-layer (see Figure <ref type="figure" target="#fig_0">1</ref>) for dimension transformation. Regarding the hyper-parameter setting, our principle is to ensure that IDRec are fully tuned in terms of learning rate ğ›¾, embedding size ğ‘‘, layer number ğ‘™, and dropout ğœŒ. While for MoRec, we attempt to first use the same set of hyper-parameters as IDRec and then perform some basic searching around the best choices. Therefore, without special mention, we do not guarantee that the results reported by MoRec are the best, because searching all possible hyperparameters for MoRec is very expensive and time-consuming, sometimes taking more than 100x compute and training time than IDRec, especially for vision, see Table <ref type="table" target="#tab_4">6</ref>. Thereby, how to efficiently find the optimal hyper-parameters of MoRec is an important but unexplored research topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluations</head><p>We split the datasets into training, validation, and testing sets by adopting the standard leave-one-out strategy. Specifically, the latest interaction of each user was used for evaluation, while second-tolast was used as validation for hyper-parameter searching, and all others are used for training. We evaluate all models using two popular top-N ranking metrics: HR@N (Hit Ratio) and NDCG@N (Normalized Discounted Cumulative Gain), where N is set to 10. We rank the ground-truth target item by comparing it with all the left items in the item pool. Finally, we report results on the testing set, but find the best hyper-parameters via the validation set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPARATIVE STUDIES (Q(I))</head><p>According to existing literature, MoRec can easily beat IDRec in the new item or cold-start item recommendation settings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">57]</ref>. We report such results in the Appendix A.1. In this paper we focus on evaluating them in the more challenging setting: regular (mixture of warm and cold items) and warm-start item recommendation scenarios, where IDRec is usually very strong. To the best of our knowledge, such comparisons have not been explicitly discussed in the existing literature.</p><p>As mentioned, we evaluate IDRec and MoRec with the two most important recommendation architectures, i.e., DSSM and SASRec. We use pre-trained BERT and RoBERTa as ME when items are of text features, and use pre-trained ResNet and Swin Transformer <ref type="bibr" target="#b36">[37]</ref> when items are of visual features. 6 Note for BERT and RoBERTa, we add the DT-layer (see Figure <ref type="figure" target="#fig_0">1</ref>) on the final representation of the '[CLS]' token. We report results on the testing set in Table <ref type="table" target="#tab_0">2</ref> for regular setting (i.e. the original distribution) and Table <ref type="table" target="#tab_1">3</ref> for warm-start settings where cold items are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MoRec vs IDRec (Regular Setting)</head><p>As shown in Table <ref type="table" target="#tab_0">2</ref>, we observe that DSSM always substantially underperforms SASRec, regardless of the item encoding strategy used. For instance, SASRec-based IDRec is around 4.9Ã— better than DSSM-based IDRec in terms of HR@10 for news recommendation, although their training, validation, and testing sets are kept exactly the same. The performance gap for image recommendation is relatively small, around 1.4Ã— and 2.7Ã—, on HM and Bili, respectively. This is consistent with much prior literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, where representing and modeling users with their interacted item sequence is often more powerful than dealing them as individual userIDs.</p><p>Second, we notice that with the DSSM architecture, MoRec perform much worse than IDRec in all three datasets even with the state-of-the-art (SOTA) ME, in particular for the visual recommendation scenarios. By contrast, with the SASRec architecture, MoRec consistently achieve better results than IDRec on MIND using any of 6 We provide the URLs of all pre-trained modality encoders utilized in our study at <ref type="url" target="https://github.com/westlake-repl/IDvs.MoRec">https://github.com/westlake-repl/IDvs.MoRec</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MoRec vs IDRec (Warm Item Settings)</head><p>To validate the performance of MoRec and IDRec for warm item recommendation, we constructed new datasets with different item popularity. We show the item popularity distribution of the original datasets in Figure <ref type="figure" target="#fig_5">3</ref>. For each dataset, we remove items with less than 20, 50, 200 interactions from the original datasets. We report the recommendation accuracy of all three datasets in Table <ref type="table" target="#tab_1">3</ref>. It can be seen that IDRec is getting stronger and stronger from warm-20, warm-50 to warm-200. In warm-20 dataset, MoRec is slightly better than IDRec, while in warm-200, MoRec is slightly worse than IDRec for text recommendation. This is reasonable since IDRec is known to to be good at modeling popular items according to the existing literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73]</ref>. But even in these warm-start setting, MoRec is still comparable to IDRec. Such property is appealing since it is well-known that MoRec can easily beat IDRec in the coldstart setting (see Appendix) and has a natural advantage for tranfer learning or cross-domain recommendation. Even further, recent work have shown that large MoRec models have the potential to be a foundation recommendation models <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, capable of achieving the ambitious goal of "one model for all" <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>The above results shed the following insights: (1) the recommendation architecture (seq2seq SASRec or two-tower DSSM) of MoRec has a very large impact on its performance; (2) its item ME also influences the performance of MoRec; (3) (Answer for Q(i)) equipped with the most powerful ME, MoRec can basically beat its IDRec counterpart for text recommendation (both cold and warm item settings) and is on par with IDRec for visual recommendation when using the sequential neural network recommendation architecture. However, it seems that there is little chance for MoRec to replace IDRec with the typical DSSM training approach in either regular or the warm-start setting; (4) although MoRec cannot beat IDRec in terms of very popular item recommendation, they still show very competitive results. To the best of our knowledge, this is the first paper that explicitly claims that pure MoRec can be comparable to pure IDRec (when they are compared under the same sequential<ref type="foot" target="#foot_4">foot_4</ref> recommendation architecture), even for the very challenging warm item recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INHERIT ADVANCES IN NLP &amp; CV? (Q(II))</head><p>Intuitively, MoRec have the potential to bring powerful representation learning techniques from other communities, such as NLP and CV, to recommendation tasks. However, this has not been formally studied. Here, we ask: can recent advances in NLP and CV translate into improved accuracy for recommendation tasks? We aim to answer it from the following perspectives.</p><p>First, we investigate whether a larger pre-trained ME enables better recommendation accuracy since in NLP and CV larger pretrained models tend to offer higher performance in corresponding downstream tasks. As shown in Figure <ref type="figure">4</ref>, a larger vision item encoder always achieves better image recommendation accuracy, i.e., ResNet18-based MoRec &lt; ResNet34-based MoRec &lt; ResNet50based MoRec, and Swin-T based MoRec &lt; Swin-B based MoRec. Similarly, we find that BERT tiny -based MoRec &lt; BERT base -based MoRec &lt; BERT small -based MoRec. One difference is that BERT basebased MoRec do not outperform BERT small -based MoRec although the latter has a smaller-size BERT variant. We conclude that, in general, a larger and more powerful ME from NLP and CV tends to improve the recommendation accuracy, but this may not strictly apply in all cases.</p><p>Second, we investigate whether a stronger encoder network enables better recommendations. For example, it is recognized that RoBERTa outperforms BERT <ref type="bibr" target="#b35">[36]</ref>, and BERT outperforms the unidirectional GPT <ref type="bibr" target="#b44">[45]</ref>, such as OPT <ref type="bibr" target="#b79">[80]</ref>, for most NLP understanding (but not generative) tasks with similar model sizes, and that Swin Transformer often outperforms ResNet in many CV tasks <ref type="bibr" target="#b36">[37]</ref>. In addition, these modern pre-trained NLP foundation models easily outperform TexTCNN <ref type="bibr" target="#b26">[27]</ref> and GloVe <ref type="bibr" target="#b42">[43]</ref>, two well-known shallow models developed about ten years ago. As shown in Figure <ref type="figure">4</ref> Third, we investigate whether the pre-trained ME produces higher recommendation accuracy than its training-from-scratch (TFS) version (i.e., with random initialization). There is no doubt that the pre-trained BERT, ResNet, and Swin largely improve corresponding NLP and CV tasks against their TFS versions. We report the recommendation results on the testing set in Table <ref type="table" target="#tab_2">4</ref>. It can be clearly seen that pre-trained MoRec obtain better final results. In particular, MoRec achieve around 10% improvements with the pretrained ME (ResNet and Swin) on HM and Bili, which also aligns with findings in NLP and CV domains. We also construct the smaller version datasets by randomly drawing 50K users from MIND, HM, and Bili. It can be seen that the advantages of pre-trained ME over TFS are more obvious on small datasets. However, we found that the pre-trained BERT base is even worse than its TFS version on MIND-50K.</p><p>According to the above experiments, we conclude that (Answer for Q(ii)) MoRec build connections for RS and other multimedia communities, and can in general inherit the latest advances from the NLP and CV fields. This is a very good property, which means that once there are new breakthroughs in the corresponding research fields in the future, MoRec have more opportunities and greater room to be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ARE MODALITY REPRESENTATIONS</head><p>UNIVERSAL FOR RS? (Q(III))</p><p>Foundation models in NLP and CV are expected to generate generic representation, which can then be directly used for downstream tasks in the zero-shot setting. However, most of them are only evaluated in some traditional tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44]</ref>, such as image and text classification. We argue that predicting user preference is more challenging than these objective tasks.</p><p>To see this problem clearly, we evaluate two training approaches. The first approach is to pre-extract modality features by ME and then add them into a recommendation model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, referred to as a two-stage (TS) pipeline. Due to the high training efficiency, TS is especially popular in real-world industrial applications, where there are usually hundreds of millions of training examples. The second approach is the one used in all above experiments, by optimizing user and item encoders simultaneously in an E2E manner.</p><p>As shown in Table <ref type="table" target="#tab_3">5</ref>, we find that TS-based MoRec show surprisingly poor results, compared to IDRec and E2E-based MoRec. In particular, with ResNet, it achieves only around 60% and 25% performance of E2E-based MoRec on HM and Bili, respectively. For better adaption, we also add many dense layers on top of these fixed modality features. As shown, this can indeed improve the performance of TS; however, it is still much worse than IDRec and E2E-based MoRec, especially for visual recommendation.</p><p>The results indicate that the modality features learned by these NLP and CV tasks are not universal enough for the recommendation problem, and thus the recommendation results are worse compared to retraining on new data (i.e., the E2E paradigm). The good thing is that by proper adaption (i.e., TS-DNN), TS-based MoRec have some potential to compete with E2E MoRec for text recommendation in the future <ref type="bibr">(16.66 vs 18.23)</ref>.</p><p>Thereby, we want to explicitly remind RS researchers and practitioners that (Answer for Q(iii)) the popular two-stage recommendation mechanism leads to significant performance degradation (especially for image recommendation), which should not be ignored in practice. <ref type="foot" target="#foot_5">8</ref> Second, for NLP and CV researchers, we want to show them that, despite the revolutionary success of FM, until now their representation features are not universal enough, at least for item recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">KEY CHALLENGES (Q(IV))</head><p>E2E-based MoRec has been less studied before, especially for visual recommendation. Here, we present several key challenges and some unexpected findings that the community may not be aware of.</p><p>Training cost. As shown in Figure <ref type="figure">4</ref>, MoRec with larger ME tend to perform better than smaller ME, however, the training compute, time and GPU memory consumption also increase, especially for the seq2seq-based architecture with very long interaction sequence. We report the training cost details on HM (close to Bili) and MIND  </p><formula xml:id="formula_3">11M 0.63G 10m 4G V100-32G(1) BERT small 35M 16G 42m 13G V100-32G(1) BERT base 116M 107G 102m 52G V100-32G(2) HM IDRec 114M 1G 4.3m 5G V100-32G(1) ResNet18 18M 40G 95m 23G V100-32G(1) ResNet34 29M 81G 136m 30G V100-32G(1) ResNet50 31M 91G 83m 80G V100-32G(4) Swin-T 34M 96G 107m 157G A100-40G(4) Swin-B 94M 333G 102m 308G A100-40G<label>(8)</label></formula><p>in Table <ref type="table" target="#tab_4">6</ref>. In fact, it is not difficult to imagine that MoRec will consume more computing resources and time than IDRec. However, it is hard to imagine that the best MoRec (with SASRec as user encoder and Swin-B as ME) takes an astonishing more than 100x compute and training time than IDRec. <ref type="foot" target="#foot_6">9</ref> This has not been explicitly revealed in literature. This may also be the reason why there are no formal publications combining seq2seq user encoder and E2Elearned item ME for MoRec, especially for image recommendation. Note that in practice, it may not always be necessary to optimize all parameters of ME, and for some datasets, fine-tuning a few top layers of ME can achieve comparable results. On the other hand, although E2E-based MoRec is highly expensive <ref type="foot" target="#foot_7">10</ref> during training (akin to FM in NLP and CV), it has been shown to enable foundation recommendation models, which can free up more labor in training specific models <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. Extra pre-training. Performing a second round of pre-training for ME using the downstream dataset often works well in much machine learning literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b53">54]</ref>. Here, we explore whether it offers improved results for MoRec. Following the pre-training of BERT, we adopt the "masked language model" (MLM) objective to train the text encoder of MoRec (denoted by BERT base -MLM) on MIND and report results in Table <ref type="table" target="#tab_6">8</ref>. As shown, BERT base -MLM gains higher  <ref type="figure" target="#fig_2">2</ref>, we find that pictures in Bili have very diverse topics and are more challenging than HM (with only very simple fashion elements). Our conclusion is that the effectiveness of the second round of pre-training depends on individual datasets; more importantly, it seems difficult to achieve larger accuracy gains for the E2E MoRec.</p><p>Combing ID &amp; modality features. Given that IDRec and E2Ebased MoRec both work well, a natural idea is to combine the two features (i.e., ID and modality) in one model. We have evaluated this, as shown in Table <ref type="table" target="#tab_5">7</ref>. We consider two types of feature combinations: additive and concatenated. Surprisingly, we find that neither TS-nor E2E-based MoRec is improved compared to the best results between IDRec and MoRec. By adding ID features, E2E-based MoRec perform even worse than pure IDRec and pure MoRec. Our results here are somewhat inconsistent with previous publications, which often claimed to achieve better results by adding modality or multimedia features for IDRec <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. One reason might be that in the regular (vs cold-start) setting, both E2E-based MoRec and IDRec learn user preference from user-item interaction data, so they cannot complement each other, while for TS-based MoRec, since ID embeddings are too much better than frozen modality features, their combination also does not improve the results. The second reason may be that more advanced techniques are required when combining ID and modality features. In fact, from another point of view, MoRec with ID features will lose many advantages of MoRec (see Introduction). For example, with ID features MoRec are not suitable for building foundation recommendation models, because IDs are not easily transferable due to privacy and overlapping issues. 11  Model collapse. Unlike IDRec, we find a very surprising phenomenon that training MoRec without proper hyper-parameters (mainly the learning rate ğ›¾) can easily lead to model collapse. As 11 In this paper, we did not intend to study the effect of transfer learning, because a reliable pre-trained model requires a huge amount of training data and compute, see <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>.   shown in Figure <ref type="figure" target="#fig_8">5</ref>, the performance of MoRec on MIND drops drastically from 16% to 0 when ğ›¾ ğ‘€ and ğ›¾ ğ‘… are equal to 0.0001. Even worse, MoRec becomes collapsed from the beginning when ğ›¾ ğ‘€ = 0.0001 and ğ›¾ ğ‘… = 0.001. Similarly, MoRec also have this problem when making image recommendation on HM. However, by carefully searching hyper-parameters, we find that MoRec can usually be trained well with a proper ğ›¾. It is worth noting that it is sometimes necessary to set different ğ›¾ for item ME and other modules. This may be because item ME has been pre-trained on NLP and CV datasets before, and its learning stride may be different from other modules trained from scratch. By contrast, IDRec did not collapse even with many different ğ›¾. To the best of our knowledge, our findings here have not been reported in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>ID-based recommender systems (IDRec). In the existing recommendation literature, there are countless models built entirely on user/item ID, from early item-to-item collaborative filtering <ref type="bibr" target="#b34">[35]</ref>, shallow factorization models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48]</ref>, to deep neural models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. They can be roughly divided into two categories: non-sequential models (NSM) and sequential neural models (SRM). NSM further includes various recall (e.g., DSSM, and YouTube DNN <ref type="bibr" target="#b6">[7]</ref>) and CTR models (e.g., DeepFM <ref type="bibr" target="#b14">[15]</ref>, wide &amp; Deep <ref type="bibr" target="#b5">[6]</ref>, and Deep Crossing <ref type="bibr" target="#b50">[51]</ref>). These models typically take a user-item pair as input along with some additional features and predict matching scores between users and items. In contrast, a typical SRM takes a sequence of user-item interactions as input and generates the probability of the next interaction. The most representative SRM includes GRU4Rec <ref type="bibr" target="#b21">[22]</ref>, NextItNet <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>, SR-GNN <ref type="bibr" target="#b67">[68]</ref>, SASRec <ref type="bibr" target="#b24">[25]</ref> &amp; BERT4Rec <ref type="bibr" target="#b54">[55]</ref> with RNN, CNN, GNN, Transformer &amp; BERT as the backbone, respectively, among which SASRec often performs the best in literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref>.</p><p>Modality-based recommender systems (MoRec). MoRec focus on modeling the modality content features of items, such as text <ref type="bibr" target="#b66">[67]</ref>, images <ref type="bibr" target="#b38">[39]</ref>, videos <ref type="bibr" target="#b7">[8]</ref>, audio <ref type="bibr" target="#b57">[58]</ref> and text-image multimodal pairs <ref type="bibr" target="#b64">[65]</ref>. Previous work tended to adopt the two-stage (TS) mechanism by first pre-extracting item modality features from ME and then incorporating these fixed features into the recommendation model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref>. What's more, most of these work mainly use modality as side features and IDs as the main features. E2E-based MoRec is not popular until recently for several reasons: (1) the TS mechanism is architecturally very flexible for industrial applications and requires much lower compute and training cost; (2) there were few high-quality public datasets with original item modalities; (3) ME developed in past literature (e.g., word embedding) is not expressive enough even with E2E training. In the past two years, some works have begun to explore E2E-based MoRec, however, most of them focus on text recommendation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72]</ref>. A recent preprint <ref type="bibr" target="#b11">[12]</ref> introduced ResNet as ME for fashion-based recommendation but had to rely on ID features for competitive accuracy. To the best of our knowledge, none of these existing peer-reviewed literature provides an explicit and comprehensive comparative study of MoRec and its corresponding IDRec counterpart in a fair experimental setting (e.g., making sure they use the same backbone for comparison), especially in the non cold-start or even warm-start settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION AND FUTURE WORKS</head><p>In this paper, we investigated an ambitious but under-explored question, whether MoRec has the opportunity to end the dominance of IDRec. Obviously, this problem cannot be completely answered in one paper, and requires more study and efforts from the RS and even the NLP and CV communities. Yet, one major finding here is that with the SOTA and E2E-trained ME, modern MoRec could already perform on par or better than IDRec with the typical recommendation architecture (i.e., Transformer backbone) even in the non cold-start item recommendation setting. Moreover, MoRec can largely benefit from the technical advances in the NLP and CV fields, which implies that it has larger room for accuracy improvements in the future. Given this, we believe our research is meaningful and would potentially inspire more studies on E2E-based MoRec, for example, developing more powerful recommendation architectures (particular for CTR <ref type="foot" target="#foot_8">12</ref> prediction tasks), more expressive &amp; generalized item encoders, better item &amp; user fusion strategies and more effective optimizations to reduce the compute &amp; memory costs and the longer training time. We also envision that in the long run the prevailing paradigm of RS may have a chance to shift from IDRec to MoRec when raw modality features are available.</p><p>As mentioned above, this study is only a preliminary of MoRec and has several limitations: (1) we considered RS scenarios with only text and vision, whereas MoRec's behaviors with other modalities, e.g., voice and video, remain unknown; (2) we consider only singlemodal item encoders, while the behaviors of multimodal MoRec are unknown; (3) we considered only a very basic approach to fusing ME into recommendation models, thereby MoRec may achieve sub-optimal performance; (4) our observations were made on three medium-sized dataset, and it remains unknown whether the key findings hold if we scale up training data to 100Ã— or 1000Ã— as in real industrial systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the Research Center for Industries of the Future (No.WU2022C030) and the Key Research Project of Zhejiang Lab (No.2022PG0AC02). MoRec is a natural fit for cold item recommendation as their ME is specifically developed to model the raw modality features of an item, whether it is cold or not. To validate this, we evaluate IDRec and MoRec in two scenarios, i.e., COLD item setting and NEW item setting. Specifically, we counted the interactions of all items in the training set and regarded those that appeared less than 10 times as cold items. We found that the number of cold items were very small in our original testing test. So we performed dataset crawling again for one month and then selected user sequences (from this new dataset) that contained these cold items (as cold item setting) and items that did not appear in the training set (as new item setting). We report the results in Table <ref type="table" target="#tab_7">9</ref>. As expected, MoRec consistently and substantially improve IDRec on all three datasets for both text and vision modalities in both cold and new settings. The superiority of MoRec comes from the powerful representations of ME which were pre-trained on large-scale text and image datasets beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 MoRec vs IDRec on cold-start settings</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SASRecFigure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of IDRec vs MoRec. ğ‘‰ ğ‘– andğ‘‡ ğ‘– denote raw features of vision and text modalities. ğ¸ ğ‘– is the item representation vector fed into the recommender model. The only difference between IDRec and MoRec is the item encoder. IDRec uses an itemID embedding matrix as the item encoder, whereas MoRec uses the pre-trained ME (followed by a dense layer for the dimension transformation, denoted by DT-layer) as the item encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Item cases on Bili.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Item cases on datasets. Image ME we used are all pre-trained in the ImageNet1K dataset.</figDesc><graphic coords="4,361.13,147.00,75.21,61.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 :</head><label>1</label><figDesc>Dataset characteristics. ğ‘› and ğ‘š denote the numbers of users and items. |R| ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› , |R ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ | and |R ğ‘¡ğ‘’ğ‘ ğ‘¡ | denote the number of interactions of the training set, validation set and testing set, respectively. |R|/(ğ‘›ğ‘š) represents density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Item popularity distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, MoRec's performance keeps consistent with the findings in NLP and CV, i.e., RoBERTa base -based MoRec &gt; BERT base -based MoRec &gt; OPT 125M -based MoRec &gt; TextCNN-based MoRec &gt; GloVe-based MoRec, and Swin-T based MoRec &gt; ResNet50-based MoRec (Swin-T has a similar model size to ResNet50, the same for RoBERTa base , BERT base and OPT 125M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>R</head><label></label><figDesc>=0.0001 M =0.001 R =0.0001 M =0.0001 R =0.0001 M =5e-05 R =0.001 M =0.0005(b) MoRec with Swin-T on HM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training collapse (on the validation set) with different learning rates ğ›¾. ğ‘€ and ğ‘… denote learning rate for ME and the remaining modules, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) comparison of IDRec and MoRec using DSSM and SASRec for regular setting. MoRec with different ME are directly denoted by their encoder names for clarity. The best results for DSSM and SASRec are bolded. 'Improv.' is the relative improvement of the best MoRec compared with the best IDRec. All results of MoRec are obtained by fine-tuning their whole parameters including both the item encoder and user encoder. Swin-T and Swin-B are Swin Transformer with different model sizes, where T is tiny and B is base. ResNet50 is a 50-layer ResNet variant.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell>DSSM</cell><cell></cell><cell></cell><cell></cell><cell>SASRec</cell><cell></cell><cell>Improv.</cell></row><row><cell></cell><cell cols="7">IDRec BERT base RoBERTa base IDRec BERT small BERT base RoBERTa base</cell><cell></cell></row><row><cell>MIND</cell><cell>HR@10 NDCG@10 1.69 3.58</cell><cell>2.68 1.21</cell><cell>3.07 1.35</cell><cell>17.71 9.52</cell><cell>18.50 9.94</cell><cell>18.23 9.73</cell><cell>18.68 10.02</cell><cell>+5.48% +5.25%</cell></row><row><cell></cell><cell cols="2">IDRec ResNet50</cell><cell>Swin-T</cell><cell cols="2">IDRec ResNet50</cell><cell>Swin-T</cell><cell>Swin-B</cell><cell></cell></row><row><cell>HM</cell><cell>HR@10 NDCG@10 2.93 4.93</cell><cell>1.49 0.75</cell><cell>1.87 0.94</cell><cell>6.84 4.01</cell><cell>6.67 3.56</cell><cell>6.97 3.80</cell><cell>7.24 3.98</cell><cell>+5.85% -0.75%</cell></row><row><cell>Bili</cell><cell>HR@10 NDCG@10 0.56 1.14</cell><cell>0.38 0.18</cell><cell>0.57 0.27</cell><cell>3.03 1.63</cell><cell>2.93 1.45</cell><cell>3.18 1.59</cell><cell>3.28 1.66</cell><cell>+8.25% +1.84%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>MoRec vs IDRec (HR@10) in the warm-start settings with SASRec as user backbone. Warm-20 means removing items with less than 20 interactions in the original dataset.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>MIND</cell><cell></cell><cell>HM</cell><cell>Bili</cell><cell></cell></row><row><cell></cell><cell cols="6">IDRec BERT base IDRec Swin-T IDRec Swin-T</cell></row><row><cell>Warm-20</cell><cell>20.12</cell><cell>20.19</cell><cell>7.89</cell><cell>8.05</cell><cell>3.48</cell><cell>3.57</cell></row><row><cell>Warm-50</cell><cell>20.65</cell><cell>20.89</cell><cell>8.88</cell><cell>8.83</cell><cell>4.04</cell><cell>4.02</cell></row><row><cell cols="2">Warm-200 22.00</cell><cell>21.73</cell><cell>11.15</cell><cell>11.10</cell><cell>10.04</cell><cell>9.98</cell></row></table><note><p>the three text encoders, i.e., BERT small , BERT base and RoBERTa base . For instance, MoRec outperform IDRec by over 5% on the two evaluation metrics with the RoBERTa base text encoder. Meanwhile, MoRec perform comparably to IDRec when using Swin Transformer as ME but perform relatively worse when using ResNet50. The performance disparity of MoRec between DSSM and SASRec potentially implies that a powerful recommendation backbone (SAS-Recvs DSSM ) and training approach (seq2seqvs &lt; ğ‘¢, ğ‘– &gt; pair) is required to fully harness the strengths of the modalitybased item encoder. Given MoRec's poor results with DSSM, we mainly focus on the SASRec architecture in the following.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Pre-trained (PE) ME vs TFS on the testing set regarding HR@10 (%). BERT base are used as text ME, and ResNet50 and Swin-T are used as vision ME. 'Improv.' indicates the relative improvement of PE over TFS.</figDesc><table><row><cell>HR@10. (%)</cell><cell cols="2">15.0 16.5 18.0</cell><cell cols="2">GloVe MIND</cell><cell cols="3">TextCNN OPT125M BERTtiny BERTsmall BERTbase RoBERTabase ID</cell></row><row><cell cols="2">HR@10. (%)</cell><cell>6.0 6.4 6.8 7.2</cell><cell cols="2">ResNet18 HM</cell><cell>ResNet34</cell><cell cols="2">ResNet50</cell><cell>Swin-T</cell><cell>Swin-B ID</cell></row><row><cell cols="2">HR@10. (%)</cell><cell>2.4 2.7 3.0 3.3</cell><cell cols="2">ResNet18 Bili</cell><cell>ResNet34</cell><cell cols="2">ResNet50</cell><cell>Swin-T</cell><cell>Swin-B ID</cell></row><row><cell cols="8">Figure 4: Accuracy with different pre-trained ME in MoRec.</cell></row><row><cell cols="8">Parameters of the pre-trained encoder network are all fine-</cell></row><row><cell cols="8">tuned on the recommendation task.</cell></row><row><cell cols="4">Dataset</cell><cell cols="2">ME</cell><cell>Base</cell><cell>50K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TFS</cell><cell>PE</cell><cell>Improv.</cell><cell>TFS</cell><cell>PE</cell><cell>Improv.</cell></row><row><cell></cell><cell cols="7">MIND BERT base 17.78 18.23 +2.53% 15.04 14.35 -4.59%</cell></row><row><cell></cell><cell></cell><cell cols="2">HM</cell><cell cols="2">ResNet50 5.82 Swin-T 6.27</cell><cell cols="2">6.67 +14.60% 2.74 6.97 +11.16% 2.84</cell><cell>3.26 +18.98% 4.47 +57.39%</cell></row><row><cell></cell><cell></cell><cell cols="2">Bili</cell><cell cols="2">ResNet50 2.67 Swin-T 2.83</cell><cell cols="2">2.93 3.18 +12.37% 1.08 +9.74% 1.07</cell><cell>1.20 +12.05% 1.46 +35.19%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>HR@10 (%) of E2E vs TS with additional MLP layers . 'TS-DNN 6' denotes that TS-based MoRec with 6 learnable MLPs layers on top of these fixed modality representation.</figDesc><table><row><cell cols="2">Dataset IDRec</cell><cell>ME</cell><cell>TS</cell><cell></cell><cell></cell><cell>TS-DNN</cell><cell></cell><cell></cell><cell>E2E</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell cols="10">MIND 17.71 BERT base 13.93 15.20 16.26 16.66 16.32 16.14 18.23</cell></row><row><cell>HM</cell><cell>6.84</cell><cell cols="2">ResNet50 4.03 Swin-T 3.45</cell><cell>4.64 4.46</cell><cell>5.40 5.28</cell><cell>5.39 5.55</cell><cell>5.40 5.40</cell><cell>5.02 5.38</cell><cell>6.67 6.97</cell></row><row><cell>Bili</cell><cell>3.03</cell><cell cols="2">ResNet50 0.72 Swin-T 0.79</cell><cell>1.23 1.40</cell><cell>1.62 1.81</cell><cell>1.47 2.10</cell><cell>1.28 1.95</cell><cell>1.24 1.64</cell><cell>2.93 3.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>The training cost. #Param: number of tunable parameters, FLOPs: computational complexity (we measure FLOPs with batchsize=1), Time/E: averaged training time for one epoch, 'm' means minutes, MU: GPU memory usage, e.g., 'V100-32G(2)' means that we used 2 V100s with 32G memory.</figDesc><table><row><cell cols="2">Dataset Method</cell><cell cols="4">#Param. FLOPs Time/E MU</cell><cell>GPU</cell></row><row><cell></cell><cell>IDRec</cell><cell>47M</cell><cell>0.12G</cell><cell>2.7m</cell><cell>3G</cell><cell>V100-32G(1)</cell></row><row><cell>MIND</cell><cell>BERT tiny</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>HR@10 (%) of co-training ID and modality. 'ADD' and 'CON' are two fusion methods. w/ and w/o denote whether to add extra MLP layers after the fusion layer. We search the layer number from {2, 4, 6, 8}. Adding extra DNN layers for 'ID+E2E' does not improve the accuracy, so we do not report them below for clarity. 'Improv.' means the relative improvement with ID+modality featurs compared to the best result of pure IDRec and pure MoRec. base for both the TS and E2E models. Similarly, we explore whether it holds for the vision encoder. Note that ResNet and Swin Transformer used in previous experiments are pre-trained in a supervised manner, but neither HM nor Bili contains supervised image labels. To this end, we turn to use MAE<ref type="bibr" target="#b17">[18]</ref>, a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM. We find MAE base -MLM clearly improves the standard MAE base on HM with the TS model, but obtains marginal gains with the E2E model. By contrast, no accuracy improvements are observed on Bili. By examining image cases in Figure</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ID+TS</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ID+TS-DNN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ID+E2E</cell><cell></cell></row><row><cell>Dataset</cell><cell>ME</cell><cell>IDRec</cell><cell>TS</cell><cell>w/o</cell><cell></cell><cell>w/</cell><cell></cell><cell cols="2">Improv. TS-DNN</cell><cell>w/o</cell><cell></cell><cell>w/</cell><cell></cell><cell>Improv.</cell><cell>E2E</cell><cell>w/o</cell><cell></cell><cell>Improv.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ADD CON ADD CON</cell><cell></cell><cell></cell><cell cols="4">ADD CON ADD CON</cell><cell></cell><cell></cell><cell cols="2">ADD CON</cell><cell></cell></row><row><cell cols="9">MIND BERT base 17.71 13.93 16.10 17.20 17.66 17.57 -0.28%</cell><cell>16.66</cell><cell cols="9">14.93 16.58 17.29 17.55 -0.90% 18.23 16.25 17.12 -6.09%</cell></row><row><cell>HM</cell><cell>Swin-T</cell><cell>6.84</cell><cell>3.45</cell><cell>5.75</cell><cell>4.89</cell><cell>5.37</cell><cell cols="2">5.40 -15.94%</cell><cell>5.55</cell><cell>5.27</cell><cell>4.00</cell><cell>4.77</cell><cell cols="3">5.11 -22.95% 6.97</cell><cell>5.40</cell><cell cols="2">4.95 -22.53%</cell></row><row><cell>Bili</cell><cell>Swin-T</cell><cell>3.03</cell><cell>0.79</cell><cell>3.01</cell><cell>2.61</cell><cell>3.02</cell><cell>2.86</cell><cell>-0.33%</cell><cell>2.10</cell><cell>2.86</cell><cell>2.35</cell><cell>2.50</cell><cell>2.72</cell><cell>-5.61%</cell><cell>3.18</cell><cell>2.94</cell><cell>2.55</cell><cell>-7.55%</cell></row><row><cell cols="3">accuracy than BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Comparison of HR@10 (%) w/ and w/o extra pretraining. 'Improv.' means the relative improvement of w/ extra pre-training compared to w/o extra pre-training.</figDesc><table><row><cell cols="3">Dataset</cell><cell>ME</cell><cell></cell><cell>TS</cell><cell></cell><cell></cell><cell>E2E</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/o</cell><cell>w/</cell><cell cols="2">Improv. w/o</cell><cell>w/</cell><cell>Improv.</cell></row><row><cell cols="9">MIND BERT base 13.93 14.68 +5.38% 18.23 18.63 +2.19%</cell></row><row><cell cols="3">HM</cell><cell>MAE base</cell><cell>2.50</cell><cell cols="3">2.79 +11.60% 7.03</cell><cell>7.07</cell><cell>+0.57%</cell></row><row><cell cols="3">Bili</cell><cell>MAE base</cell><cell>0.57</cell><cell>0.57</cell><cell>0.00%</cell><cell>3.18</cell><cell>3.17</cell><cell>-0.31%</cell></row><row><cell>HR@10. (%)</cell><cell>0 2 4 6 8 10 12 18 16 14</cell><cell cols="4">Training Epoch 0 10 20 30 40 50 R =0.0001 M =0.0001 R =0.0001 M =5e-05 R =0.0005 M =0.0001 R =0.001 M =0.0001</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>HR@10 (%) of IDRec and MoRec for cold and new item recommendation. ğ‘š ğ‘ğ‘œğ‘™ğ‘‘ and ğ‘š ğ‘›ğ‘’ğ‘¤ denote the number of cold items and new items, respectively. All results are evaluated based on the SASRec architecture.</figDesc><table><row><cell>Dataset</cell><cell>ME</cell><cell cols="4">ğ‘š ğ‘ğ‘œğ‘™ğ‘‘ IDRec MoRec ğ‘š ğ‘›ğ‘’ğ‘¤ IDRec MoRec</cell></row><row><cell cols="2">MIND BERT base</cell><cell>32K</cell><cell>0.0036 3.0637</cell><cell cols="2">13K 0.0125 0.5899</cell></row><row><cell>HM</cell><cell>Swin-B</cell><cell>37K</cell><cell>0.3744 1.0965</cell><cell cols="2">14K 0.0115 0.6846</cell></row><row><cell>Bili</cell><cell>Swin-B</cell><cell>39K</cell><cell>0.3551 0.6400</cell><cell>5K</cell><cell>0.0078 0.0832</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.kaggle.com/competitions/h-and-m-personalized-fashionrecommendations/overview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://www.bilibili.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>To build this dataset, we randomly crawled URLs of short videos (with duration time less than 10 minutes) from 23 different video channels of Bili from October 2021 to March 2022. Then we recorded public comments of these videos as interactions. Finally, we merged all user interactions chronologically and removed duplicate interactions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>It seems that so far there is no publicly available dataset that fully satisfies the above mentioned requirement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Due to space reasons, we do not report results of other sequential models, but we have indeed evaluated them. The conclusions hold when using GRU4Rec, NextItNet, and BERT4Rec as backbones.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>Unfortunately, so far, there is not even any literature showing that an E2E-based MoRec has been successfully deployed in real-world recommender systems.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Note that the inference time of MoRec for online service is as fast as IDRec.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>This entire work has costed us over $140,000. For example, with 8 A100 GPUs, MoRec with Swin-B requires nearly 1 week to converge on HM, and the cost of purchasing the GPU service is about $2,000 (for one set of hyper-parameters).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>In fact, we notice that the NLP/CV communities are formulating most tasks into sequence learning problem with Transformer as the backbone<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, e.g., GPT-3 and pixelGPT<ref type="bibr" target="#b4">[5]</ref>. It will be interesting to see whether complex CTR models with various user/item features can be formulated in a similar fashion (the way MoRec is powerful).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully content-based movie recommender system with feature extraction using neural network</title>
		<author>
			<persName><forename type="first">Hung-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Leh</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maw-Kae</forename><surname>Hor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yuan</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International conference on machine learning and cybernetics (ICMLC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autodebias: Learning to debias for recommendation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guli</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Content-based video recommendation system based on stylistic visual features</title>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franca</forename><surname>Garzotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Piazzolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Quadrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Data Semantics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Zeroshot recommender systems</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Hao Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08318</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Shereen</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Brinkmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.02923</idno>
		<title level="m">End-to-End Image-Based Fashion Recommendation</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Comparison of Transformer-Based Sequential Product Recommendation Models for the Coveo Data Challenge</title>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zoller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply fusing reviews and contents for cold start users in cross-domain recommendation systems</title>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>MarasoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">Don&apos;t stop pretraining: adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Content-aware neural hashing for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Alstrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VBPR: visual bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Universal Sequence Representation Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanlei</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional matrix factorization for document context-aware recommendation</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinoh</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1181</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kula</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08439</idno>
		<title level="m">Metadata embeddings for user and item cold-start recommendations</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale content-only video recommendation</title>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="987" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MINER: Multi-Interest Matching Network for News Recommendation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grounded language-image pre-training</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10965" to="10975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">IntTower: the Next Generation of Two-Tower Model for Pre-Ranking System</title>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxia</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3292" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.02043</idno>
		<title level="m">Could Giant Pretrained Image Models Extract Universal Representations?</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amazon. com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 38th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09820</idno>
		<title level="m">Visually-aware personalized recommendation using interpretable image representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering vs. matrix factorization revisited</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM conference on recommender systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Kyuyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjae</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00573</idno>
		<title level="m">One4all user representation for recommender systems in e-commerce</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Kyuyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><forename type="middle">Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">Nihlen</forename><surname>Ramstrom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11294</idno>
		<title level="m">Scaling law for recommendation models: Towards generalpurpose user representations</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification?</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China national conference on Chinese computational linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A generic network compression framework for sequential recommender systems</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1299" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial training towards robust multimedia recommender system</title>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="867" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">TransRec: Learning Transferable Recommendation from Mixture-of-Modality Feedback</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joemon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyun</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.06190</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Neural news recommendation with multi-head self-attention</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6389" to="6394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Empowering news recommendation with pre-trained language models</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1652" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07407</idno>
		<title level="m">Mm-rec: multimodal news recommendation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">NewsBERT: Distilling pre-trained language model for intelligent news application</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04887</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mind: A large-scale dataset for news recommendation</title>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3597" to="3606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Training large-scale news recommenders with pretrained language models in the loop</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvan</forename><surname>Middha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4215" to="4225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Yoonseok</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seok</forename><surname>Kyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juneyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04179</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sampling-bias-corrected neural modeling for large corpus item recommendations</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="269" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Tiny-NewsRec: Efficient and Effective PLM-based News Recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00944</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Lambdafm: learning optimal ranking with factorization machines using lambda surrogates</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joemon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international on conference on information and knowledge management</title>
		<meeting>the 25th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer from sequential behaviors for user modeling and recommendation</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liguang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1469" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A simple convolutional generative network for next item recommendation</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twelfth ACM international conference on web search and data mining</title>
		<meeting>the twelfth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">One person, one model, one world: Learning continual user representation without forgetting</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="696" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems</title>
		<author>
			<persName><forename type="first">Guanghu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beibei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10629</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks for sequential recommendation</title>
		<author>
			<persName><forename type="first">Mengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">UNBERT: User-News Matching BERT for News Recommendation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno>3356-3362</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bars: Towards open benchmarking for recommender systems</title>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangcai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2912-2923</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
