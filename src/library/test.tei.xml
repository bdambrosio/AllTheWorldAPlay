<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative AI Research O1 Replication Journey: A Strategic Progress Report -Part 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-08">8 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwei</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Zou</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixiu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>MBZUAI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>MBZUAI</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Generative AI Research Lab (GAIR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative AI Research O1 Replication Journey: A Strategic Progress Report -Part 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-08">8 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">9370040EDECC8F8E4C5FB9FBAE3E6BD6</idno>
					<idno type="arXiv">arXiv:2410.18982v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-19T00:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a pioneering approach to artificial intelligence research, embodied in our O1 Replication Journey. In response to the announcement of OpenAI's groundbreaking O1 model, we embark on a transparent, real-time exploration to replicate its capabilities while reimagining the process of conducting and communicating AI research. Our methodology addresses critical challenges in modern AI research, including the insularity of prolonged team-based projects, delayed information sharing, and the lack of recognition for diverse contributions. By providing comprehensive, real-time documentation of our replication efforts, including both successes and failures, we aim to foster open science, accelerate collective advancement, and lay the groundwork for AI-driven scientific discovery. Our research progress report diverges significantly from traditional research papers, offering continuous updates, full process transparency, and active community engagement throughout the research journey. Technologically, we proposed the "journey learning" paradigm, which encourages models to learn not just shortcuts, but the complete exploration process, including trial and error, reflection, and backtracking. With only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded. We share valuable resources including technical hypotheses and insights, cognitive exploration maps, custom-developed tools, etc at <ref type="url" target="https://github.com/GAIR-NLP/O1-Journey">https://github.com/GAIR-NLP/O1-Journey</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Initial Assessment, Multi-path Exploration, Iterative Improvement, and Current Results. The journey culminates in our novel "journey learning" approach, which significantly outperforms traditional "shortcut learning" methods. With only 327 training samples, our journey learning technique surpassed shortcut learning by 8.4% and 8.0% respectively on the MATH500 (Lightman et al., 2024). denotes our Walnut Plan, which aims to revolutionize AI by developing systems capable of deep scientific thinking, ultimately enabling AI-driven breakthroughs in human knowledge and discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction</head><p>The landscape of artificial intelligence research has been dramatically altered by the announcement of OpenAI's O1 model, a purportedly groundbreaking language model capable of complex reasoning tasks. Despite the excitement generated by this announcement, the AI community finds itself in a peculiar position: we know of O1's existence and its claimed capabilities, but the details of its implementation, training data, and even its complete outputs remain shrouded in mystery. This lack of transparency not only hampers technological progress but also raises important questions about the open nature of scientific advancement in the AI field. It is within this context that our team embarked on the O1 Replication Journey. Our primary goal is not to achieve performance parity with OpenAI's O1 -a task we acknowledge as extremely challenging given the limited information and resources available. Instead, our mission is to transparently document and share our exploration process, focusing on the fundamental questions we encounter, uncovering new scientific questions, and sharing our trial-and-error experiences with the broader AI community. By doing so, we aim to reduce the total collective cost of trial-and-error in the world and identify the key factors contributing to O1's reported success. This report's structure marks a significant departure from traditional scientific publications, addressing key challenges in modern AI research. In an era of prolonged, team-based AI projects, we aim to combat information isolation and researcher burnout through enhanced transparency and real-time feedback. Additionally, this report represents a bold reimagining of AI research methodology. It aims not only to provide a valuable reference for current O1 replication efforts but also to establish a new paradigm for future AI research and broader scientific exploration. Through this innovative approach, we strive not only to achieve technological breakthroughs but also to actively shape a more open, collaborative, and responsible scientific culture. Simultaneously, we are accumulating invaluable learning materials for future AI systems capable of scientific discovery, thus laying the groundwork for the next generation of artificial intelligence in scientific research.</p><p>Our initial explorations have already yielded intriguing insights into the potential mechanisms behind O1's reported capabilities. A key breakthrough in our research at the current stage is the proposed "journey learning" paradigm, which represents a fundamental shift in how we approach model training. This innovative method encourages models to learn not just shortcuts to solutions, but the complete exploration process, including trial and error, reflection, and backtracking (see Figure <ref type="figure" target="#fig_1">3</ref>). The power of this approach is evident in its performance: with only 327 training samples and without any additional tricks, journey learning outperformed conventional supervised learning by over 8% on the MATH dataset, demonstrating its extremely powerful potential. We believe this to be the most crucial component of O1 technology that we have successfully decoded so far.</p><p>• Determine the criteria I am looking for the smallest positive integer composed of 0s and 2s that is divisible by 30. First, we need to ensure that this number is divisible by 2, 3, and 5.</p><p>• Reveal the characteristics of the number It is observed that the number ends in 0 or 2, which satisfies the condition for divisibility by 2. Meanwhile, the number must end in 0 and the sum of its digits must be divisible by 3, so we need to find the smallest number that meets these conditions.</p><p>• Construct and filter I am constructing numbers that contain only 0s and 2s, and end in 0. The goal is to find the smallest number among them such that the sum of its digits is divisible by 3.</p><p>• Find the qualifying number I am searching for the smallest number consisting of 0s and 2s. The test is based on whether the sum of the digits is divisible by 3. If it is, the number may be a suitable candidate.</p><p>• Test numbers I am testing numbers like <ref type="bibr">20020,</ref><ref type="bibr">2220,</ref><ref type="bibr">200220,</ref><ref type="bibr">20220,</ref><ref type="bibr">220,</ref><ref type="bibr">2020,</ref><ref type="bibr">2000</ref>, and checking if the sum of their digits is divisible by 3. I am looking for a number smaller than 2220 that satisfies these conditions.</p><p>• Analyze the sum of the digits I am studying the pattern of digits made up of 2s and 0s. Different numbers of digits and their arrangements affect the sum of the digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Find the least common multiple</head><p>To make the given sum a multiple of 5, the sum of the weights of x and y must be a multiple of 3. This suggests that I need to find the smallest x and y such that x + y is a multiple of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O1-preview (long thought from ChatGPT)</head><p>Our model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>selfcorrection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation</head><p>We are looking for the least positive integer multiple of 30 that can be written using only the digits 0 and 2. A number divisible by 30 must satisfy two conditions:</p><p>• Divisibility by 10: The number must end in 0.</p><p>• Divisibility by 3: The sum of the digits must be divisible by 3.</p><p>Since the digits can only be 0 or 2, the sum of the digits will be some multiple of 2. For divisibility by 3, the sum of the digits must also be divisible by 3. Therefore, the sum of the digits must be a multiple of 6 (since it must be divisible by both 2 and 3).</p><p>Let's now check progressively larger numbers formed by only the digits 0 and 2 that end in 0 and whose digit sum is divisible by 3:</p><p>• The number 20 ends in 0, but its digit sum is 2, which is not divisible by 3. • The number 220 ends in 0, and its digit sum is 2+2+0=42 + 2 + 0 = 42+2+0=4, which is not divisible by 3. • The number 20220 ends in 0, and its digit sum is 2+0+2+2+0=62 + 0 + 2 + 2 + 0 = 62+0+2+2+0=6, which is divisible by 3.</p><p>Thus, the least positive integer multiple of 30 that can be written with only the digits 0 and 2 is 20220.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-4o</head><p>Problem: What is the least positive integer multiple of 30 that can be written with only the digits 0 and 2?  As part of our commitment to open science, we will be releasing numerous valuable resources throughout this journey. These include:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>🍓</head><p>(1) Our detailed hypotheses regarding the O1 technical stack, along with a comprehensive map of our cognitive exploration path. This resource provides insight into our strategic thinking and decision-making processes throughout the replication attempt. (2) A collection of insights and positive outcomes derived from our trial-anderror experiences. This compilation offers valuable lessons learned and unexpected discoveries that may benefit the broader AI research community. (3) Extensive documentation of our cognitive processes, including discussion presentations and brainstorming sessions. These materials offer a transparent look into our team's collaborative problem-solving approach and idea generation. ( <ref type="formula">4</ref>) Preliminary results and experimental data from our initial efforts, as well as access to our custom-developed annotation platform. These resources showcase our early progress and provide practical tools for researchers engaged in similar endeavors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Why We Created Progress Report?</head><p>In the rapidly evolving landscape of artificial intelligence research, traditional methodologies and reporting practices are increasingly proving inadequate to address the complexities and scale of modern AI projects. This report represents a pioneering effort to reimagine the process of conducting and communicating AI research. By providing a comprehensive, real-time account of our journey to replicate the groundbreaking O1 model, we aim to address critical challenges in contemporary AI research, foster open science, redefine scientific communication, lay the groundwork for AI-driven scientific discovery, and promote responsible AI development. What follows is not merely a documentation of our findings, but a bold proposition for a new paradigm in scientific exploration and collaboration in the AI era.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Addressing the Challenges of Modern AI Research:</head><p>The rapid evolution of artificial intelligence technologies has ushered in a new era of research paradigms, characterized by prolonged, team-based endeavors that often span six months or more. This shift, while conducive to breakthrough innovations, has inadvertently introduced novel challenges to the scientific process. The inherent insularity of extended team collaborations frequently results in a diminished flow of information to the broader scientific community. Moreover, the protracted nature of these projects often leads to delayed gratification for researchers, potentially fostering anxiety and diminished motivation throughout the research journey. Additionally, the complexity of large-scale team projects complicates the recognition of individual contributions, potentially eroding the traditional academic incentive structures. Our progress report methodology aims to address these emergent challenges by enhancing transparency, facilitating real-time feedback and recognition, and encouraging sustained commitment to long-term research initiatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fostering Open Science and Collective Advancement:</head><p>In the spirit of open science and collective advancement, the primary impetus behind this report is to disseminate the invaluable insights, resources, and lessons gleaned from our endeavor to replicate the O1 model. This approach transcends the mere sharing of a trained model; it encompasses a comprehensive documentation of the tools, datasets, and methodologies employed throughout our exploratory process. By candidly sharing our setbacks and unsuccessful attempts, we aim to provide educational value that often surpasses that of mere success stories. This transparency is intended to assist other researchers in navigating potential pitfalls, thereby accelerating progress across the field. Furthermore, by elucidating our thought processes and innovative approaches, we aspire to catalyze creativity within the community, fostering the generation of novel ideas and methodologies.</p><p>3. Laying the Foundation for AI in Scientific Discovery: The meticulous documentation of our scientific exploration process holds profound significance, particularly in the context of rapidly advancing AI capabilities. By recording our exploration process in its entirety, including both successes and failures, we are cultivating a unique and invaluable dataset. This comprehensive record is crucial for training AI models that genuinely comprehend scientific methodologies, mirroring the approach validated by the O1 model. The success of O1 underscores the importance of AI systems learning not just outcomes, but the complete scientific exploration process, including trial and error. Our report captures not only technical details but also decision rationales, sources of inspiration, and thought processes. These "human factors" are essential for training AI models capable of authentic scientific discovery. Moreover, this approach has interdisciplinary value, offering a template for research documentation and knowledge sharing that can foster innovation across various scientific domains.  We claim that most existing approaches to machine learning or large language model training (e.g., supervised fine-tuning) can be characterized as "shortcut learning." This traditional paradigm, while potentially effective in specific, well-defined tasks, shows significant limitations when faced with complex, dynamic, and open-ended problems. Shortcut learning is defined by several key characteristics: (I) Quick results orientation: It emphasizes achieving specific performance metrics or completing particular tasks in a short time frame. (2) Heavy data dependency: Performance improvements often rely on increasing the volume of training data rather than enhancing the learning algorithms themselves. (3) Limited generalization: Performance can deteriorate dramatically in scenarios outside the distribution of the training data. (4) Lack of self-correction: These systems typically lack the ability to identify and correct their own errors. While shortcut learning has driven many advances in AI, it struggles to produce truly intelligent and reliable AI systems capable of handling the complexities of real-world challenges. As we pursue more advanced forms of artificial intelligence or even superintelligence, the limitations of Journey learning represents a significant advancement over shortcut learning. While shortcut learning often falters in complex, dynamic environments, journey learning is specifically designed to thrive in such scenarios. It aims to create AI systems that are not just narrow, task-specific tools, but adaptable, reasoning entities capable of handling the nuances and complexities of real-world challenges. This new paradigm holds the promise of more capable, adaptable, and human-like AI that can better serve and interact with humans across various domains. As we continue to develop and refine the journey learning paradigm, we expect it to open new possibilities in AI research and applications, potentially revolutionizing our approach to artificial intelligence and its role in our society. ). Implementing PRMs can involve using proprietary models with advanced prompting techniques <ref type="bibr" target="#b10">(Hao et al., 2024)</ref> or training with step-level supervision data <ref type="bibr" target="#b39">(Xia et al., 2024;</ref><ref type="bibr">Wang et al., 2024c)</ref>. The latter approach is challenging because it requires high-quality annotated data <ref type="bibr" target="#b39">(Xia et al., 2024)</ref>. This has led to interest in using reinforcement learning principles, which model the multi-step reasoning process as a Markov Decision Process (MDP) and use techniques like Monte Carlo Tree Search <ref type="bibr" target="#b27">(Silver et al., 2016)</ref> to estimate the value of each step, either online <ref type="bibr" target="#b2">(Chen et al., 2024)</ref> or offline <ref type="bibr">(Wang et al., 2024c)</ref>.</p><p>COT Theory Chain-of-thought (CoT) prompting has significantly advanced the reasoning capabilities of LLMs. Foundational studies demonstrate that providing intermediate reasoning steps enhances performance on complex tasks, such as arithmetic and commonsense reasoning <ref type="bibr" target="#b36">(Wei et al., 2022)</ref>. Additionally, theoretical investigations reveal that CoT empowers decoder-only transformers by enabling inherently serial computation, which is otherwise lacking, particularly in low-depth transformers <ref type="bibr" target="#b33">(Li et al., 2024b)</ref>. Recent studies also reveal CoT prompting enhances LLMs by showing that even constant-sized autoregressive Transformers can solve complex tasks like arithmetic and decision-making through CoT derivations, using circuit complexity theory <ref type="bibr" target="#b8">(Feng et al., 2024)</ref>. Recent work emphasizes the integration of "error-correction" data into the pretraining stage to enhance reasoning accuracy, showing that such data can lead to higher accuracy without the need for multi-round prompting <ref type="bibr" target="#b42">(Ye et al., 2024)</ref>. Overall, these findings underscore the pivotal role of CoT prompting in enhancing LLM performance and accessibility in complex reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Internal Thought</head><p>The exploration of internal thought in AI models has evolved as researchers emphasize the need for models to reflect on their reasoning and refine their outputs. Early work like STaR <ref type="bibr" target="#b45">(Zelikman et al., 2022)</ref> proposed bootstrapping reasoning by having models generate rationales that explain their decisions, allowing them to improve their performance on complex tasks through iterative refinement. Building on this, Quiet-STaR <ref type="bibr">(Zelikman et al., 2024a)</ref> generalizes the approach by training language models to generate rationales after each token, helping them predict and explain future text more effectively. <ref type="bibr" target="#b48">Zhang et al. (2024)</ref> further expanded this line of work by embedding reflection within each training instance, encouraging models to review their decisions and consider alternative reasoning paths. RISE <ref type="bibr" target="#b21">(Qu et al., 2024)</ref> introduced a method for recursive introspection, where models iteratively adjust their responses after detecting errors, aiming for self-improvement over multiple attempts. These developments illustrate the growing focus on enabling AI models to engage in reflective, self-correcting processes, enhancing their ability to handle more complex reasoning tasks.</p><p>Inference Time Scaling Recent studies have demonstrated that scaling inference time can provide more efficient improvements in model performance <ref type="bibr" target="#b24">(Sardana and Frankle, 2023;</ref><ref type="bibr" target="#b29">Snell et al., 2024)</ref> compared to traditional scaling approaches such as increasing model parameters or training data volume. While parameter scaling has been a dominant paradigm in advancing model capabilities <ref type="bibr" target="#b13">(Kaplan et al., 2020;</ref><ref type="bibr">Brown et al., 2020;</ref><ref type="bibr" target="#b4">Chowdhery et al., 2022)</ref>, it often leads to diminishing returns and significant computational overhead. In contrast, allowing models more time to process and refine their outputs during inference has emerged as a promising alternative scaling dimension <ref type="bibr" target="#b18">(Madaan et al., 2023)</ref>. Inference time scaling offers several advantages: 1) Resource efficiency, utilizing existing model capacities more thoroughly; 2) Adaptable computation, allocating more processing time to complex tasks; and 3) Improved reasoning through step-by-step problem solving or iterative refinement <ref type="bibr" target="#b41">(Yao et al., 2023;</ref><ref type="bibr" target="#b5">Cobbe et al., 2021)</ref>. Empirical evidence suggests that doubling inference time often yields performance improvements comparable to those achieved by significantly increasing model size, but at a fraction of the computational cost <ref type="bibr" target="#b29">(Snell et al., 2024)</ref>. Some successful implementations include deliberation mechanisms and iterative refinement protocols <ref type="bibr" target="#b11">(Huang et al., 2022;</ref><ref type="bibr" target="#b19">Miao et al., 2023)</ref>, which have shown particular promise in tasks requiring complex reasoning or creative generation.</p><p>Search-to-thought In recent years, the shift from traditional search-based methods to implicit reasoning approaches has significantly advanced AI research <ref type="bibr" target="#b23">(Ruoss et al., 2024)</ref>. Classic systems like Deep Blue <ref type="bibr" target="#b1">(Campbell et al., 2002)</ref> relied heavily on explicit search algorithms such as alpha-beta pruning and Monte Carlo Tree Search to achieve superhuman performance <ref type="bibr" target="#b28">(Silver et al., 2017)</ref>. However, with the advent of deep learning, Chain of Thought (CoT) <ref type="bibr" target="#b36">(Wei et al., 2022)</ref> reasoning has gained significant attention for its ability to improve model performance by generating intermediate reasoning steps without search. Implicit Chain-of-Thought Reasoning <ref type="bibr" target="#b6">(Deng et al., 2023)</ref> bypass the need for generating explicit reasoning steps by leveraging the internal hidden states of models. This method distills knowledge from a teacher model trained to generate intermediate steps, allowing student models to solve tasks more efficiently by reasoning vertically through their internal layers. Similarly in chess AI, a 270M parameter transformer model can achieve grandmaster-level play without any explicit search by learning action-values through supervised training on a large dataset of games <ref type="bibr" target="#b23">(Ruoss et al., 2024)</ref>. These approaches highlight a trend where models are increasingly able to generalize complex reasoning and decision-making processes internally, thus reducing reliance on computationally expensive search algorithms while maintaining high performance in domains like mathematical reasoning and game playing.</p><p>Self-improvement in LLM Self-improvement methods for LLMs aim to enhance model performance by enabling them to learn from their own outputs with minimal human intervention. These approaches typically involve supervised fine-tuning (SFT) on high-quality outputs generated by the models <ref type="bibr">(</ref> ), highlighting their potential for driving further advancements in LLM capabilities. However, recent findings suggest that LLM-generated texts often exhibit truncated "tails", meaning that the distribution of generated outputs lacks the variability found in human-generated content, particularly in the less common, outlier responses (or "tails" of the distribution) <ref type="bibr" target="#b26">(Shumailov et al., 2024;</ref><ref type="bibr" target="#b7">Dohmatob et al., 2024)</ref>. This reduced variability can lead to a phenomenon known as model collapse, where the model converges toward a narrower range of behaviors, ultimately harming performance <ref type="bibr" target="#b26">(Shumailov et al., 2024)</ref>. This issue has been observed in tasks like language modeling <ref type="bibr" target="#b26">(Shumailov et al., 2024)</ref> and iterative preference optimization for mathematical reasoning <ref type="bibr" target="#b33">(Wu et al., 2024b)</ref>. To mitigate the risk of model collapse, researchers recommend maintaining a balanced mix of clean, human-authored data alongside LLM-generated content during training <ref type="bibr" target="#b26">(Shumailov et al., 2024;</ref><ref type="bibr" target="#b7">Dohmatob et al., 2024;</ref><ref type="bibr" target="#b9">Gerstgrasser et al., 2024)</ref>. This approach helps preserve diversity and prevents the model from degrading in performance over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Exploration Journey</head><p>This section represents the core of our O1 replication endeavor. This section systematically unfolds our exploration process through a series of pivotal questions, mirroring the complex pathway illustrated in our research timeline diagram. From the initial evaluation of O1 using OlympicArena <ref type="bibr" target="#b12">(Huang et al., 2024)</ref> datasets to the intricate "Long Thought Construction" phase, our journey has been marked by multiple attempts, continuous iterations, and deep dives into the essence of O1's capabilities.</p><p>The questions we address in this chapter not only reflect the progression of our research but also embody our profound inquiry into the nature of O1's cognitive processes. We begin by examining the structure of O1's thoughts, then delve into the mechanics and construction of long thoughts -a concept central to our "Long Thought Construction" phase depicted in the diagram. Our exploration extends to the development of reward models, the construction of on-policy reasoning trees, and the integration of these elements into cohesive long thoughts, mirroring the complex web of interconnected processes in our research timeline. Our methodology, as visualized in the diagram, involves multiple iterations and parallel streams of investigation. This approach is reflected in our discussion of evaluation methods and training strategies, showcasing how we validate hypotheses and refine our techniques through cycles of quantitative and qualitative assessments, including human checks and specialized analysis tools.</p><p>By structuring this chapter around these key questions, we not only provide a clear narrative of our technical journey but also demonstrate a systematic approach to exploring unknown AI technologies. This question-driven format aligns with our "journey learning" paradigm, emphasizing the importance of the entire learning and exploration process, not just the final outcomes. As we progress through each question, readers will gain insights into our decision-making process, the challenges we faced, and the innovative solutions we developed. This transparent sharing of our thought processes, attempts, and even failures, as illustrated in our research timeline, aims to contribute valuable insights to the AI community and foster collective advancement in the field.</p><p>Through this section, we invite readers to traverse our exploration journey, understanding not just what we discovered about O1, but how we approached the daunting task of replicating a groundbreaking AI model with limited information. Our journey, marked by curiosity, persistence, and innovation, serves as a testament to the power of open, collaborative AI research in pushing the boundaries of what's possible in artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Q1: What does O1's Thought Look Like?</head><p>The Table <ref type="table">3</ref> is created based on a detailed analysis of O1's thought examples provided by OpenAI, <ref type="foot" target="#foot_0">1</ref> which includes eight instances of reasoning steps, or "thoughts," for solving complex tasks. Each example in this is meticulously examined to extract relevant features such as the number of tokens, lines, and keyword. These examples are categorized into different problem types, each associated with a difficulty level ranging from simple English reading comprehension to complex multi-step math reasoning tasks. Our analysis demonstrates a trend: as the difficulty increases, the response length (both tokens and lines) tends to grow proportionally. This suggests that higher difficulty problems involve more reasoning steps.</p><p>In addition to token and line counts, we conducted a keyword frequency analysis to identify recurring terms that may characterize the reasoning process. In addition to commonly observed connective words like "and" and "so", our analysis highlights several less frequently occurring but highly significant keywords. Keywords such as "consider", "if" and "possible" appear frequently, often signaling branching in the reasoning process where multiple paths are considered. The frequency of these keywords was notably higher in problems with higher complexity, indicating the model's exploration of different solution paths in these scenarios. Keywords like "wait" and "Alternatively" are crucial indicators of the model's ability to engage in reflection and self-correction. This suggests a deeper understanding and a more nuanced approach to reasoning, as the model is not just following a linear path but is capable of reconsidering and refining its approach based on reflection.</p><p>To understand the thought process of OpenAI's O1, we consult two PhD candidates from the mathematics department to carefully review the reasoning process employed by OpenAI's O1 in solving mathematical problems. Through their detailed examination, they extracted the underlying thought chain that reflects how O1 approaches and reasons through complex equations. This structured thought graph is illustrated in Figure <ref type="figure">5</ref>. After these explorations, we determined that the long thought data we need to construct should have the following characteristics:</p><p>• Iterative Problem-Solving: The model starts by defining functions and gradually explores related expressions, breaking down complex equations into simpler components, reflecting a structured and methodical approach.</p><p>• Key Thought Indicators: The use of terms like "Therefore" for conclusions, "Alternatively" for exploring different paths, "Wait" for reflection, and "Let me compute" for transitioning into calculations highlights the model's reasoning stages.</p><p>6.2 Q2: How does Long Thought Work?</p><p>• Recursive and Reflective Approach: The model frequently reassesses and validates intermediate results, using a recursive structure to ensure consistency, which is typical in rigorous mathematical reasoning.</p><p>• Exploration of Hypotheses: The model tests different hypotheses, adjusting its approach as it gathers more information, demonstrating flexibility in its reasoning process.</p><p>• Conclusion and Verification: Finally, the model solves the equations and verifies the results, emphasizing the importance of validating conclusions before finishing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Q2: How does Long Thought Work?</head><p>This is a question we consider important. However, at our current stage of progress, we are merely putting forward our hypotheses. We don't believe we have sufficient empirical evidence to verify their accuracy. The remarkable success of O1's long-thought approach can be attributed to journey learning, which we have introduced in §4. Unlike traditional shortcut learning, journey learning allows the model to explore the entire decision trajectory, mimicking human problem-solving processes. This comprehensive exploration enables O1 to consider multiple solution paths, learn from errors, and understand the complete problem-solving process. By experiencing both correct and incorrect paths, the model develops robust error-handling and self-correction capabilities, enhancing its adaptability to new challenges. This approach fosters a deeper understanding of the problem domain, going beyond merely knowing the correct answer to comprehending why and how to arrive at it. The journey learning process closely simulates human cognitive processes, incorporating trial-and-error, reflection, and adjustment. This results in enhanced explainability, as O1 can provide detailed solution steps and explain its reasoning, including how it recovers from mistakes. Consequently, O1's long thought process, grounded in journey learning, is not simply about extended computation time but represents a thorough, human-like reasoning exploration. This methodology equips O1 to handle more complex problems, offer more reliable and interpretable answers, and demonstrate greater adaptability when faced with novel challenges, thus explaining its exceptional performance across various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Q3: How to Construct Long Thoughts?</head><p>Constructing long thoughts with actions such as reflection and backtracking is the core part of journey learning. To achieve this, we undertook a series of attempts.</p><p>Attempt 1: Tree Search with LLM and Reward Based on our observations of long thought in §6.1, its most prominent feature is the attempt to reflect and backtrack when reasoning leads to an incorrect or unhelpful node. This resembles searching on a reasoning tree for a problem, backtracking at erroneous nodes, until the correct solution path is found. To achieve this, we need to construct a reasoning tree where the root node represents the problem, and each other node represents a reasoning step. The path from the root to any node signifies the reasoning process from the problem to that conclusion. Moreover, backtracking and reflection must be based on incorrect reasoning steps, necessitating a more fine-grained reward model (i.e., process-level) to indicate the correctness of each node in the tree. By executing a search algorithm on a reasoning tree with process-level rewards, we can integrate erroneous steps into a chain of thought, thereby constructing long thought that encompasses actions like backtracking and reflection.</p><p>Attempt 2: Propose-Critique Loop Attempt 1 constructs long thought by executing searches on the tree based on predefined rules, but this limits the freedom of actions like backtracking and reflection. Therefore, we allow the model to choose its current actions. We constructed a Propose-Critique Loop, where we pre-define some possible actions for the model (i.e., continue, backtracking, reflection, terminate) and let the model select actions to build the reasoning tree. If the tree does not reach the final answer, the model can be informed of this negative signal, guiding it to reflect and correct its approach.</p><p>Attempt 3: Multi-Agent Approach Building long thought on the foundation of a reasoning tree presents several challenges, including the presence of numerous ineffective nodes that do not contribute to constructing Long Thought, as well as issues of logical inconsistency caused by reasoning steps that do not depend on the reflection behavior. To address this, we designed an algorithm utilizing multi-agent debate, where one agent acts as the policy model, continuously reasoning, while another agent serves as the critique model, indicating whether the policy model should continue with the current reasoning or perform actions like backtracking. The two agents engage in ongoing dialogue, naturally constructing a long thought dataset when the correct answer is found.</p><p>Attempt 4: Complete Human Thought Process Annotation When humans tackle reasoning problems, they typically do not engage in constant forward reasoning until they either solve the problem or fail; instead, they reflect, backtrack, and rewrite reasoning when they can no longer proceed. This behavior closely aligns with the characteristics of long thought. Thus, we can faithfully and comprehensively document the process by which humans solve reasoning tasks, resulting in high-quality long thought.</p><p>The Thought Structure of OpenAI o1 in Mathematical Reasoning # Problem: Solving the equation p</p><formula xml:id="formula_0">(1/x) = x 2 ## Step 1: Define q(x) = p(1/x) -x 2</formula><p>• Analyze the roots of q(x):</p><p>-Therefore: q(k) = 0 for k = ±1, ±2, . . . , ±n</p><formula xml:id="formula_1">• Let me consider: Construct s(x) = x 2n q(x)</formula><p>-Analyze the properties of s(x):</p><formula xml:id="formula_2">* Therefore: s(x) is a polynomial -Alternatively: Consider s(x) = x 2n p(1/x) -x 2n+2</formula><p>* Let me explain: Expand the expression for p(1/x).</p><formula xml:id="formula_3">-Factorize s(x) = m(x)h(x) * Define m(x) = (x 2 -k 2 )</formula><p>* Hypothesize the form of h(x):</p><p>• Consider: h(x) must be a quadratic polynomial • Try: h(x) = -x 2 + bx + c</p><p>• Wait: We need to determine the values of b and c</p><p>• Let me compute: Determine the coefficients of h(x)</p><formula xml:id="formula_4">## Step 2: Alternatively, construct t(x) = x 2n p(1/x) = p * (x)</formula><p>• Analyze the properties of p * (x):</p><p>-Therefore: p * (x) is the reciprocal polynomial of p(x) </p><formula xml:id="formula_5">• Consider s(x) = t(x) -x 2n+2</formula><formula xml:id="formula_6">• Compute: m(0) = (-1) n (n!) 2 • Consider: s(0) = m(0)h(0) = 1 • Therefore: c = (-1) n (n!) 2</formula><p>-Verify: This is consistent with the previous result</p><formula xml:id="formula_7">## Step 5: So: Solve h(x) = 0 • -x 2 + c = 0 • x 2 = c = 1 (n!) 2</formula><p>• Therefore:Additional solutions are x = ±1/n!</p><p>• Wait: We need to consider the case when n is even -When n is even: c &gt; 0 -Verify: This ensures that the solutions are real First, we need to expand the product (3t 2 + 5t + a)(4t 2 + bt -2).</p><p>When we do this, we should carefully multiply each term:</p><p>• The product of 3t 2 and 4t 2 gives us 12t 4 .</p><p>• The product of 3t 2 and bt gives us 3bt 3 .</p><p>• The product of 3t 2 and -2 gives us -6t 2 .</p><p>• The product of 5t and 4t 2 gives us 20t 3 .</p><p>• The product of 5t and bt gives us 5bt 2 .</p><p>• The product of 5t and -2 gives us -10t.</p><p>• The product of a and 4t 2 gives us 4at 2 .</p><p>• The product of a and bt gives us abt.</p><p>• The product of a and -2 gives us -2a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, let's combine like terms:</head><p>• The t 4 term is 12t 4 , so we have 12t 4 .</p><p>• The t 3 terms are 3bt 3 and 20t 3 , which combine to (3b + 20)t 3 .</p><p>• The t 2 terms are -6t 2 , 5bt 2 , and 4at 2 , which combine to (-6 + 5b + 4a)t 2 .</p><p>• The t terms are -10t and abt, which combine to (-10 + ab)t.</p><p>• The constant term is -2a.</p><p>Let's pause and consider what we know so far. We need this expanded expression to match the given polynomial 12t 4 + 26t 3 -8t 2 -16t + 6. Now, let's equate the coefficients of like terms:</p><p>• For the t 4 term, we have 12 = 12, which checks.</p><p>• For the t 3 term, we get 26 = 3b + 20. Solving gives b = 2.</p><p>• For the t 2 term, we have -8 = -6 + 5b + 4a. Substituting b = 2, we get -8 = -6 + 10 + 4a, which simplifies to 4a = -14 and a = -3.5.</p><p>• For the t term, we have -16 = -10 + ab. Substituting a = -3.5 and b = 2, we get -16 = -10 -7, which checks.</p><p>• For the constant term, we have 6 = -2a. Substituting a = -3.5, we get 6 = 7, which doesn't check.  </p><formula xml:id="formula_8">f (x) = x 10 + 5x 9 -8x 8 + 7x 7 -x 6 -12x 5 + 4x 4 -8x 3 + 12x 2 -5x -5</formula><p>is divided by x 2 -1 without using long division.</p><p># Our Proposed Model Response:</p><p>First, we need to find the remainder when f (x) is divided by x 2 -1. To do this, we can use the Remainder Theorem, which states that if we divide a polynomial f (x) by x -a, the remainder is f (a). Now, let's apply this to our problem. We need to find the remainder when f (x) is divided by x 2 -1. This suggests that we should substitute x = 1 and x = -1 into f (x) to find the remainders. Let's start with x = 1. Substituting this into f (x), we get:</p><formula xml:id="formula_9">f (1) = 1 10 + 5(1) 9 -8(1) 8 + 7(1) 7 -(1) 6 -12(1) 5 + 4(1) 4 -8(1) 3 + 12(1) 2 -5(1) -5</formula><p>Calculating this step-by-step: Adding these values together, we find:</p><formula xml:id="formula_10">1 10 = 1, 5<label>(</label></formula><formula xml:id="formula_11">f (1) = 1 + 5 -8 + 7 -1 -12 + 4 -8 + 12 -5 -5 = -10</formula><p>So, the remainder when f (x) is divided by x -1 is -10. Now, let's try x = -1:</p><p>f (-1) = (-1) 10 + 5(-1) 9 -8(-1) 8 + 7(-1) 7 -(-1) 6 -12(-1) 5 + 4(-1) 4 -8(-1) 3 + 12(-1) 2 -5(-1) -5</p><p>Calculating each term:</p><p>(-1) 10 = 1, 5(-1) Therefore, the remainder when f (x) is divided by x + 1 is 16.</p><p>Let's pause and consider what we know so far. We have found the remainders when f (x) is divided by x -1 and x + 1. Now, we need to use these results to determine the remainder when f (x) is divided by x 2 -1. Since x 2 -1 can be factored as (x -1)(x + 1), the remainder when f (x) is divided by x 2 -1 should be a linear expression ax + b.</p><p>Let's express f (x) as: f (x) = (x 2 -1)q(x) + ax + b, where q(x) is the quotient. At this point, we know:</p><p>• When f (x) is divided by x -1, the remainder is -10.</p><p>• When f (x) is divided by x + 1, the remainder is 16.</p><p>Let's use these to find a and b. First, consider the Remainder Theorem:</p><p>• Substituting x = 1, we have:</p><formula xml:id="formula_12">f (1) = a(1) + b = -10 • Substituting x = -1: f (-1) = a(-1) + b = 16</formula><p>Let's solve these equations simultaneously: From the first equation: a + 3 = -10 ⇒ a = -13 So, the remainder when f (x) is divided by x 2 -1 is:</p><formula xml:id="formula_13">-13x + 3</formula><p>To summarize, the remainder is -13x + 3 .  We need to find the least positive integer multiple of 30 that can be formed using only the digits 0 and 2. To start, we know that a number is divisible by 30 if it is divisible by both 10 and 3. Let's address each condition separately. First, let's consider the divisibility by 10. A number is divisible by 10 if it ends in 0. So, we need to ensure that our number ends in 0. Now, let's focus on the divisibility by 3. A number is divisible by 3 if the sum of its digits is divisible by 3. Let's calculate the sum of the digits using only the digits 0 and 2. The digits available are 0 and 2. The sum of these digits is 0 + 2 = 2. At this point, we have a sum of 2, which is not divisible by 3. Let's pause and consider what this means. Since 2 is not divisible by 3, the number formed using only the digits 0 and 2 will not be divisible by 3 either. Let's rethink this. Perhaps we're missing something. Maybe we can adjust the digits to make the sum divisible by 3. Let's reconsider the sum of the digits. If we use both digits, 0 and 2, we can form different combinations to see if the sum becomes divisible by 3. Let's try different combinations:</p><p>• Using only 0, the sum is 0.</p><p>• Using only 2, the sum is 2.</p><p>• Using both, the sum can be 0 + 2 = 2 or 2 + 0 = 2.</p><p>Clearly, the sum of 2 is not divisible by 3. Let's try combining the digits differently. Wait, maybe I'm missing something again. Let's reconsider the combinations. If we use both digits, we can form larger numbers by arranging them differently. Let's try:</p><p>• 20: Sum is 2 + 0 = 2.</p><p>• 22: Sum is 2 + 2 = 4.</p><p>• 202: Sum is 2 + 0 + 2 = 4.</p><p>Still, the sums of 2 and 4 are not divisible by 3. Let's try something different. Let's try using larger numbers formed by rearranging the digits:</p><p>• 200: Sum is 2 + 0 + 0 = 2.</p><p>• 220: Sum is 2 + 2 + 0 = 4.</p><p>• 2020: Sum is 2 + 0 + 2 + 0 = 4.</p><p>Still, the sums of 2 and 4 are not divisible by 3. Let's try one more approach. Let's consider using more digits:</p><p>• 2220: Sum is 2 + 2 + 2 + 0 = 6. Now, the sum of 6 is divisible by 3. Let's pause and consider what this means. Since 2220 ends in 0, it is divisible by 10. Therefore, 2220 is divisible by both 10 and 3, making it divisible by 30. So, the least positive integer multiple of 30 that can be formed using only the digits 0 and 2 is 2220 . We present the results in Table <ref type="table" target="#tab_2">4</ref>  The construction of a reasoning tree requires a policy model π that can perform single-step reasoning. Given a problem q and its corresponding final answer a, π starts from the problem as the root node and continuously adds new nodes to the tree. It first generates w possible first-step reasoning steps as child nodes of the root node. Then, it iteratively performs forward reasoning, generating w possible subsequent reasoning steps for each current node (e.g., the first-step reasoning) as child nodes of that node. This process is repeated until a preset maximum depth D is reached or all leave nodes reach the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Model and</head><p>Step Segmentation Constructing the reasoning tree requires a clear definition of reasoning steps. To this end, we adopt the data format proposed in Abel <ref type="bibr" target="#b3">(Chern et al., 2023)</ref>, transforming mathematical problem solutions into a form with clear steps, dividing answers into multiple lines, each beginning with a line number and including reasoning within the line. Thus, we fine-tuned DeepSeekMath-7B-Base (Shao et al., 2024) using the dataset from Abel to obtain Abel-DSMath, serving as the policy model π. The model fine-tuned on this specific format data can conveniently control the generation of individual reasoning steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward Model and Pruning</head><p>The tree generation algorithm proposed above is computationally expensive. When setting w to 3 and D to 10, the last iteration requires generating 3 10 reasoning steps. Therefore, we use a reward model to prune erroneous reasoning steps, improving operational efficiency. Specifically, we employ beam search, selecting only a small number of candidates for retention in each iteration for the next round. Depending on the reward model used, the details of pruning implementation vary. We attempted two reward models: mathshepherd (Wang et al., 2024b) and o1-mini. Math-shepherd provides a real number between 0 and 1 for each step, representing the probability of correctness for the current step. In each iteration of tree generation, we score all reasoning steps and select the top K with the highest scores for the next iteration. This reduces the total generation count from n D -1 n-1 to nKD. However, math-shepherd struggles to effectively evaluate reasoning steps for difficult problems, necessitating a more robust reward model that offers high accuracy in correctness indications for each step. Thus, finally, we use o1-mini to provide rewards for each step, directly indicating whether each reasoning step is correct or incorrect. At this point, in each iteration of tree generation, we utilize the rewards from o1-mini and select at most K correct reasoning steps for the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Q6: How to Derive a Long Thought from a Reasoning Tree?</head><p>Once the reasoning tree is constructed, our goal is to derive a long thought from the tree that incorporates trial and error. This approach contrasts with traditional methods that focus solely on a shortcut to the correct answer and valid intermediate steps. In our framework, each node of the reasoning tree is annotated with a rating from a reward model that indicates whether the step is correct or incorrect, along with reasoning that justifies this judgment.</p><p>Constructing the ShortCut from a Reasoning Tree We first construct the shortCut from the reasoning tree, which includes only the correct answer and valid intermediate steps. Starting from the root node, which represents a question, we identify a path that leads to a correct answer leaf node. If there are multiple correct answer nodes, multiple correct paths will be established.</p><p>Traversal Path from a Reasoning Tree To derive a long thought, we employ a Depth First Search (DFS) traversal of the tree. This traversal constructs a path in DFS order, documenting each step from the root question node to a correct answer leaf node while including reasoning for any node marked as incorrect. The challenge with DFS lies in its exploration of a vast search space, resulting in numerous trial-and-error paths that may not yield a correct solution. To simplify this initial exploration, we introduce specific constraints to manage the complexity. 6.7 Q7: How to Evaluate our Trials? Initially, we mark all nodes in the tree based on whether they lie on the correct path (i.e., the shortCut). The traversal adheres to the following rules: (i). Nodes on the correct path: We allow exploration of child nodes that are not on the correct path. This means that when DFS encounters a node on the correct path, it may explore a child node that leads to an incorrect outcome. Once this node reaches a leaf node and is determined to be incorrect, the algorithm backtracks to continue traversing along the correct path. (ii). Nodes not on the correct path: The traversal randomly selects one child node to explore without branching into trial and error. To further streamline the process, we apply an additional constraint: each node on the correct path is permitted a maximum of K trials-one trial on an incorrect path and one on the correct path.</p><p>These constraints ensure that the DFS traversal focuses on a manageable subset of the search space, allowing for meaningful trial-and-error exploration while avoiding excessive exploration of incorrect paths. In future experiments, we plan to remove or adjust these constraints to investigate the relationship between the length of trial paths and the performance of the final model.</p><p>Long Thought from a Traverse Path With the traversal path generated and reasoning attached to the wrong nodes, we construct a draft long thought by concatenating all steps in the path. This draft incorporates the reasoning for each incorrect step. However, initial experiments using this raw draft to train models have demonstrated suboptimal performance. To address this, we employ GPT-4o to modify the draft. GPT-4o enhances the coherence and smoothness of the thought process while preserving all reasoning steps, including incorrect steps, reflections, and corrections. This approach ensures that the final long thought is not only accurate but also flows naturally, simulating the human problem-solving process with both correct and incorrect steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Q7: How to Evaluate our Trials?</head><p>In addition to testing accuracy scores using specific evaluation metrics on benchmarks, manually reviewing actual cases is a crucial step in evaluating data and models. Therefore, to provide a more intuitive way to evaluate the model's performance on specific problems, we build a visual data analysis platform using Streamlit. <ref type="foot" target="#foot_1">2</ref> Specifically, our visualization platform includes the visualization of synthetic trees and their corresponding long thoughts as well as the output of the trained model. Furthermore, when visualizing results, we support detailed conditional filtering, such as filtering for correctly or incorrectly answered questions, or whether the output contains keywords indicating reflection or hesitation (e.g., "wait"). Additionally, we support comparison between different iterations of synthetic data and model outputs, which makes it highly intuitive and helps us easily validate whether the new round of data or models is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Q8: How to Train our Models?</head><p>Our experiments utilize the pre-trained language model deepseek-math-7b-base. <ref type="foot" target="#foot_2">3</ref> The training process is divided into two main phases: Supervised Fine-Tuning (SFT) and Direct Preference Learning (DPO) <ref type="bibr" target="#b22">(Rafailov et al., 2024)</ref>.</p><p>Phase 1: Supervised Fine-Tuning (SFT) The SFT process consists of two stages: 1. ShortCut Learning: In this initial stage, we focus on fine-tuning the model using responses that include only the correct intermediate steps and the final correct answer. We fine-tune Deepseek-math-7b-base <ref type="bibr" target="#b25">(Shao et al., 2024)</ref> on the Abel dataset <ref type="bibr" target="#b3">(Chern et al., 2023)</ref>, which comprises 120k examples, and the PRM800K dataset (Lightman et al., 2024). For each question in PRM800K, we utilize a single correct step-by-step solution, discarding responses that do not lead to the final answer. This results in a total of 6,998 examples for fine-tuning. During this stage, we conduct fine-tuning for one epoch on each dataset, primarily aiming to familiarize the model with the desired response format. 2. Journey Learning: In this second stage, we further fine-tune the initial stage SFT model using the long thoughts we constructed, which comprise 327 examples. This phase is designed to enhance the model's ability to detect errors, incorporate reflections, execute corrections, and perform backtracking. By training on long thoughts that include not only the correct reasoning paths but also erroneous trials, we aim to equip the model with a deeper understanding of the complexities involved in longer reasoning chains. As a comparison, we also fine-tune the model on the corresponding shortCut generated from the same reasoning tree, which also consists of 327 examples. Both the long thought SFT and shortCut SFT settings are trained for 3 epochs on these 327 examples.</p><p>Phase 2: Direct Preference Learning (DPO) In this phase, we generate 20 responses per question from the MATH Train dataset, a re-divided dataset from PRM800k that includes 12,000 examples, using nucleus sampling with top p = 0.95 and temperature T = 0.7. These 20 responses are categorized into positive and negative responses based on the correctness of the final answer. From these, we randomly select 5 positive responses and 5 negative responses to create 5 preference pairs. We then train the model using these preference pairs with DPO loss, allowing it to learn from the comparison of correct and incorrect answers.</p><p>6.9 Q9: What Would be an Effective Annotation Strategy for Human-AI Collaboration?</p><p>The results of our experiments are shown in Table <ref type="table" target="#tab_4">6</ref>. All results are tested on the MATH test set, using a re-divided subset from PRM800K, which includes 500 examples. The results show that Journey Learning led to significant improvements compared to Shortcut Learning, with gains of +8.4 and +8.0 on the deepseek-sft-abel and deepseek-sft-prm800k models, respectively, demonstrating the effectiveness of our proposed Journey Learning method. However, the improvement from DPO was more modest, and we acknowledge that this is an initial exploratory result. In future experiments, we plan to further explore preference learning and Reinforcement Learning (RL) techniques. This will include, but not be limited to, iterative self-improvement, incorporating process-level reward models, and transitioning from outcome-level DPO to process-level DPO/RL approaches. We have developed a human-AI pipeline designed to generate high-quality, long-form reasoning data for problems derived from the MATH dataset. This pipeline enables the expansion of a human-annotated solution of several lines into thousands of tokens, which follows our "journey learning" paradigm. During the pipeline's construction, we identified key techniques for efficient annotation, including:</p><p>Complete Thought Process It is not essential for annotators to record every word that comes to mind in detail, but it is crucial to document each trial, reflection, association, and correction. These diverging cognitive pathways may not always be explicitly expressed or consciously recognized in everyday thinking. Nevertheless, capturing shifts in thought, along with the reasons behind these shifts, is critical. This ability to navigate and understand cognitive transitions is a core skill that large language models must learn from our data.</p><p>Additional Explanation for Common Sense Humans often omit information that can be inferred from context, such as references to previously mentioned formulas or the application of well-known theories. However, this can lead to hallucination when large language models attempt to interpret human annotations. Therefore, high-quality data must include explicit explanations of common-sense knowledge to prevent misinterpretation by LLMs.</p><p>With the essential components outlined previously, the concise yet precise annotated data is fully generated by human effort. The next stage involves AI-driven processes. By designing sophisticated prompts, we implement data augmentation by LLMs in aspects below:</p><p>1. Enhancement of Data Granularity The prompt emphasizes breaking down the problem-solving process into finer, smaller steps. By splitting the process into fine-grained, easily digestible chunks, it becomes easier for LLMs to grasp and internalize each concept before moving on to the next. This ensures deeper comprehension at every stage.</p><p>2. Gradual Reasoning LLMs are required to frequent pause, reflect on known information or to clarify the next step should be added to help guide reasoning. Taking pauses in reasoning mimics how students would naturally think about the problem, helping them stay engaged and connected to the reasoning process rather than passively following instructions.  <ref type="figure">2</ref>, while the explanations and resources represent a detailed elaboration of the purpose of the node and the relevant resources required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Plan</head><p>As our O1 Replication Journey continues to evolve, our future plans are shaped by the insights gained and challenges encountered thus far. Drawing from our research timeline and the progress we've made, we've identified several key areas for future exploration and development:</p><p>1. Scaling Up Long Thought Integration: Building on our successful iterations of long thought integration, we plan to conduct a third round of integration, as indicated in our research diagram. This will involve scaling up our processes to handle more complex and diverse thought patterns, potentially uncovering new dimensions of O1's capabilities.</p><p>2. Experiments on Long Thought Scaling Laws: Our diagram highlights planned experiments on long thought scaling laws. This research stream aims to understand how the performance and capabilities of our model scale with increases in data, model size, and computational resources. These insights will be crucial for optimizing our approach and potentially discovering fundamental principles underlying advanced AI systems.</p><p>3. Fine-Grained, Thought-Centric Evaluation: We plan to develop and implement more sophisticated evaluation methodologies, focusing on fine-grained, thought-centric assessment. This approach, highlighted in our research timeline, will allow us to more accurately measure the quality and coherence of the generated long thoughts, providing deeper insights into our model's reasoning capabilities.</p><p>4. Human-AI Collaboration for Quality Thought: A key component of our future plan, as shown in the diagram, is to explore and enhance human-AI collaboration for producing high-quality thoughts. This involves developing interfaces and methodologies that leverage the strengths of both human intelligence and AI capabilities, potentially leading to breakthroughs in hybrid intelligence systems.</p><p>5. Continued Improvement of Reward and Critique Models: Building on our process-level reward model and critique model setup, we aim to refine these systems further. This ongoing process will involve iterative improvements to better capture the nuances of human-like reasoning and problem-solving strategies.</p><p>6. Advanced Integration of Reasoning Trees: We plan to explore more sophisticated methods of deriving and integrating long thoughts from our reasoning trees. This will involve developing advanced algorithms for traversing and synthesizing information from these complex structures. 9. Exploration of Multi-Agent Approaches: Building on our initial attempts with multi-agent systems, we plan to delve deeper into this area, potentially uncovering new ways to model complex reasoning and decisionmaking processes.</p><p>10. Refinement of Analysis Tools: We aim to further develop and enhance our analysis tools, as indicated in our research timeline. These tools will be crucial for interpreting model outputs, tracking progress, and guiding future research directions.</p><p>By pursuing these avenues, we aim to not only advance our understanding and replication of O1's capabilities but also to push the boundaries of AI research methodologies. Our future plans reflect our commitment to the journey learning paradigm, emphasizing continuous improvement, transparent exploration, and collaborative advancement in the field of artificial intelligence. As we move forward, we remain adaptable to new discoveries and challenges, ready to adjust our plans as our understanding of O1 and advanced AI systems continues to evolve. Through this ongoing journey, we hope to contribute significantly to the development of more capable, interpretable, and ethically aligned AI systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our O1 replication journey from September 12 to October 8, 2024. It depicts four key stages: Initial Assessment, Multi-path Exploration, Iterative Improvement, and Current Results. The journey culminates in our novel "journey learning" approach, which significantly outperforms traditional "shortcut learning" methods. With only 327 training samples, our journey learning technique surpassed shortcut learning by 8.4% and 8.0% respectively on the MATH500 (Lightman et al., 2024). denotes our Walnut Plan, which aims to revolutionize AI by developing systems capable of deep scientific thinking, ultimately enabling AI-driven breakthroughs in human knowledge and discovery.</figDesc><graphic coords="1,72.00,428.56,451.27,186.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A case study comparing our model with OpenAI O1-preview and GPT-4o in solving math problems.</figDesc><graphic coords="3,220.56,438.67,197.50,234.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 .</head><label>4</label><figDesc>Promoting Responsible AI Development: In our pursuit of technological breakthroughs, we remain acutely aware of the potential societal impacts and ethical considerations associated with AI development. Through detailed documentation of our research processes and decision-making, we establish a high standard of transparency, which is crucial for cultivating public trust in AI research. Our report goes beyond technical specifics, incorporating ongoing discussions and reflections on potential societal impacts, thereby demonstrating the integration of ethical considerations throughout the technological development process. This holistic approach contributes to the cultivation of a more responsible and ethically-minded AI research culture.and recognition, and encouraging sustained commitment to long-term research initiatives. 4 Journey Learning: A New Paradigm Shift from "Shortcut Learning" (a) Shortcut learning. (b) Journey learning (c) Performance Comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A paradigm shift from "shortcut learning" to "journey learning". A searching tree for reasoning tasks. For the math problem-solving task, the root node represents the initial problem, while the leaf nodes are final conclusions. Green nodes indicate correct answers, and red nodes incorrect ones. Traditionally, learning focused on supervised training of a direct root-to-leaf shortcut path. This work, however, explores supervised learning of the entire exploration path, encompassing trial-and-error and correction processes. (c) Performance of "shortcut learning" and "journey learning" on MATH500 (Lightman et al., 2024). The base models are deepseek-math-7bbase fine-tuned on Abel training data and PRM800K separately.</figDesc><graphic coords="5,351.01,390.80,157.95,118.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Reward Model Process reward models (PRMs) are used to provide fine-grained evaluations of responses from LLMs (Lightman et al., 2024; Uesato et al., 2022; Xia et al., 2024), especially in the area of mathematical reasoning. By accurately assessing the correctness of each step, PRMs can enhance post-training quality (Wang et al., 2024c; Sun et al., 2024) and improve accuracy during inference through various search methods (Luo et al., 2024; Wang et al., 2024a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Zelikman et al., 2024b; Li et al., 2024a; Wang et al., 2024d) or preference optimization, where the model learns from pairs of good and bad responses it generated to a query (Xu et al., 2024; Yuan et al., 2024; Pang et al., 2024; Wu et al., 2024a). In general instructionfollowing tasks, the quality of model outputs is often determined by an external reward system-this can be a trained reward model (Xu et al., 2024), human evaluators (Ziegler et al., 2019), or the LLMs themselves through techniques like LLM-as-a-Judge prompting (Zheng et al., 2023). In mathematical domains, however, output quality is primarily judged by whether the model reaches the correct answer (Zelikman et al., 2024b; Pang et al., 2024). For more fine-grained evaluation, step-level rewards for mathematical reasoning tasks may be assigned by human annotators or a trained process reward model (Lightman et al., 2024). Iterative self-improvement techniques have shown promise across a range of tasks, from instruction-following (Xu et al., 2024; Yuan et al., 2024) to more complex reasoning-based challenges (Zelikman et al., 2024b; Pang et al., 2024</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-- 1 -## Step 3 :</head><label>13</label><figDesc>Thus: s(x) = p * (x) -x ( 2n + 2) • Let me consider the coefficients of (p * (x)) Analyze the constant term: p * (0) = Analyze the coefficient of the x 2n term: a0 = c Let me compute: Expand m(x)h(x) • Analyze the higher-order terms: -x 2n+2 + cx 2n • Compare with s(x): -x 2n+2 + [lower degree terms] -Thus: Confirm h(x) = -x 2 + c ## Step 4: Wait: Let us reconsider the value of m(x) at x = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Case 1 :</head><label>51</label><figDesc>Figure 5: The Thought Structure of OpenAI O1 in Mathematical Reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Wait a second, let's check our calculations again. The constant term calculation should have been 6 = -2a, which gives a = -3. Let's go back and correct that. With a = -3 and b = 2, we find a + b = -3 + 2 = -1. So, the correct value of a + b is -1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :Case 2 :</head><label>62</label><figDesc>Figure 6: Generated Thought by Our Proposed Model in Mathematical Reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 ) 9 = 5 ,</head><label>195</label><figDesc>-8(1) 8 = -8, 7(1) 7 = 7, -(1) 6 = -1, -12(1) 5 = -12, 4(1) 4 = 4, ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>9 = - 5 , - 8 (- 1 ) 8 =</head><label>95818</label><figDesc>-8, 7(-1) 7 = -7, -(-1) 6 = -1, -12(-1) 5 = 12, ... Adding these values, we get: f (-1) = 1 -5 -8 -7 -1 + 12 + 4 + 8 + 12 + 5 -5 = 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>a + b = - 10</head><label>10</label><figDesc>From the second equation: -a + b = 16 Adding these equations gives: 2b = 6 ⇒ b = 3 Substituting back into a + b = -10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Generated Thought by Our Proposed Model in Mathematical Reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Case 3 :</head><label>3</label><figDesc>Solution to Number Theory Problem with Multiples of 30 # Problem: What is the least positive integer multiple of 30 that can be written with only the digits 0 and 2? # Our Proposed Model Response:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Generated Thought by Our Proposed Model in Mathematical Reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>7 . 8 .</head><label>78</label><figDesc>Expansion of Training Methodologies: Our future plans include further experimentation with and refinement of our training pipeline. This encompasses enhancements to our pre-training, iterative training, reinforcement learning, preference learning, and DPO (Direct Preference Optimization) stages, as outlined in our research diagram. Continued Transparency and Resource Sharing: In line with our commitment to open science, we will continue to share resources, insights, and tools developed throughout our journey. This ongoing practice, represented by the resource-sharing icons in our diagram, aims to foster collaboration and accelerate progress in the wider AI research community.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between "Traditional Research Paper" and "Proposed Progress Report". entire journey, including both successes and failures, we will create an invaluable dataset for training future AI systems in scientific discovery, laying the groundwork for the next generation of AI-driven research methodologies.</figDesc><table><row><cell>Aspects</cell><cell></cell><cell>Traditional Research Paper</cell><cell>Proposed Progress Report</cell></row><row><cell></cell><cell cols="2">Timing of Publication Published after research completion</cell><cell>Real-time updates throughout the research process</cell></row><row><cell>Research Process</cell><cell>Length of Research</cell><cell>Suitable for shorter-term projects</cell><cell>Designed for prolonged, team-based endeavors</cell></row><row><cell></cell><cell cols="2">Handling of Setbacks Typically not reported in detail</cell><cell>Candidly shared as valuable learning experiences</cell></row><row><cell></cell><cell>Information Flow</cell><cell>Limited until publication</cell><cell>Continuous sharing of insights and findings</cell></row><row><cell>Information Sharing</cell><cell>Transparency</cell><cell>Focus on successful outcomes</cell><cell>Full disclosure of process, including failures</cell></row><row><cell></cell><cell>Data Sharing</cell><cell>Often limited to final results</cell><cell>Includes interim data, tools, and methodologies</cell></row><row><cell></cell><cell cols="2">Impact on Motivation Delayed gratification until publication</cell><cell>Ongoing feedback and recognition</cell></row><row><cell>Impact and Value</cell><cell>Reproducibility</cell><cell cols="2">Often challenging due to limited details Enhanced through comprehensive documentation</cell></row><row><cell></cell><cell cols="2">AI Training Potential Limited to final outcomes</cell><cell>Rich dataset including full exploration process</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between Shortcut Learning and Journey Learning.</figDesc><table><row><cell>Characteristic</cell><cell>Shortcut Learning</cell><cell>Journey Learning</cell></row><row><cell>Learning Depth</cell><cell>Surface features and simple correlations</cell><cell>Deep causal relationships and underlying principles</cell></row><row><cell>Reasoning Ability</cell><cell>Limited, struggles with complex reasoning</cell><cell>Powerful, demonstrates human-like reasoning</cell></row><row><cell>Self-Improvement</cell><cell>Lacks self-correction mechanisms</cell><cell>Continuous self-assessment and improvement</cell></row><row><cell>Generalization</cell><cell cols="2">Limited, easily affected by data distribution changes Strong, can handle new situations</cell></row><row><cell>Innovation Capacity</cell><cell>Limited, struggles to solve new problems</cell><cell>High, can generate innovative solutions</cell></row><row><cell>Data Dependency</cell><cell>Highly dependent on large training datasets</cell><cell>More focused on quality and learning strategies</cell></row><row><cell>Interpretability</cell><cell>Poor, often seen as a "black box"</cell><cell>Better, can track internal reasoning processes</cell></row><row><cell cols="2">Ethical Considerations May unintentionally amplify data biases</cell><cell>Easier to implement ethical constraints and adjustments</cell></row><row><cell>Security</cell><cell>Vulnerable to adversarial attacks</cell><cell>More robust, able to identify potential threats</cell></row><row><cell>Long-term Value</cell><cell>Quick results in specific tasks</cell><cell>Paves the way for AGI development</cell></row><row><cell>Human Analogy</cell><cell>Exam-oriented education, crash courses</cell><cell>Comprehensive education, lifelong learning</cell></row><row><cell cols="3">this approach become increasingly apparent. Recognizing these shortcomings, we propose a new paradigm called</cell></row><row><cell cols="3">"journey learning." This innovative approach represents more than just a learning method; it's a new paradigm</cell></row><row><cell>for AI development.</cell><cell></cell><cell></cell></row></table><note><p>Journey learning is designed to enable AI systems to progress continuously through learning, reflection, backtracking, and adaptation, much like humans do, thereby exhibiting higher levels of intelligence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>and Table5. O1-mini performs best across different datasets. Results on the subset of MR-GSM8K</figDesc><table><row><cell>Model</cell><cell>F1 score</cell><cell></cell><cell></cell></row><row><cell>o1-mini</cell><cell>0.855</cell><cell>Model</cell><cell>F1 Score</cell></row><row><cell>GPT-4o-mini</cell><cell>0.722</cell><cell>GPT-4o-mini</cell><cell>0.756</cell></row><row><cell>Math-shepherd</cell><cell>0.734</cell><cell>o1-mini</cell><cell>0.880</cell></row><row><cell>ReasonEval-7B</cell><cell>0.728</cell><cell>o1-preview</cell><cell>0.867</cell></row><row><cell>ReasonEval-34B</cell><cell>0.735</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results on the subset of PRM800K 6.5 Q5: How to Construct an On-policy Reasoning Tree?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Training Results on MATH Test Set 6.9 Q9: What Would be an Effective Annotation Strategy for Human-AI Collaboration?</figDesc><table><row><cell></cell><cell cols="2">deepseek-sft-abel deepseek-sft-prm800k</cell></row><row><cell>SFT-phase1</cell><cell>0.372</cell><cell>0.290</cell></row><row><cell>SFT-phase2-shortcutLearning</cell><cell>0.386</cell><cell>0.348</cell></row><row><cell>SFT-phase2-journeyLearining</cell><cell>0.470</cell><cell>0.428</cell></row><row><cell>DPO</cell><cell>0.472</cell><cell>0.440</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Detailed event explanation of our research exploration. The nodes and short descriptions correspond to node in Figure</figDesc><table><row><cell>Node</cell><cell>Short Description</cell><cell>Explanation</cell><cell>Resource</cell></row><row><cell>1</cell><cell>OpenAI o1 Release</cell><cell>OpenAI released its latest reasoning model, o1</cell><cell></cell></row><row><cell>2</cell><cell>Evaluate o1 (OlympicArena, Gaokao Math)</cell><cell>Evaluate the performance of o1 on high-difficulty competition questions</cell><cell>OlympicArena, Gaokao,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>o1 API</cell></row><row><cell>3</cell><cell>Knowledge Acquisition</cell><cell>Learn about the possible technical routes for OpenAI o1</cell><cell></cell></row><row><cell>4</cell><cell>1st o1 Technical Discussion</cell><cell>Discuss the technical route of o1 and determine research goals</cell><cell></cell></row><row><cell>5</cell><cell>Team Assembled</cell><cell>Gather relevant students to form a team</cell><cell></cell></row><row><cell>6</cell><cell>o1 Thought Analysis &amp; Schema Design</cell><cell>1. Analyze the properties/schema of o1 long thought: long thought structure, func-</cell><cell>OpenAI o1 Examples</cell></row><row><cell></cell><cell></cell><cell>tions of each part</cell><cell></cell></row><row><cell></cell><cell></cell><cell>2. Explore how to construct long thought training data: Use the MATH dataset</cell><cell></cell></row><row><cell>7</cell><cell>Attempt1: Propose-Critique Loop</cell><cell>Multi-agent system where Propose suggests possible reasoning steps and Critique</cell><cell>Proposer, Critic, Loop Algorithm</cell></row><row><cell></cell><cell></cell><cell>points out issues and suggests directions</cell><cell></cell></row><row><cell>8</cell><cell>Attempt2: Tree Search with LLM and Reward</cell><cell>Build a reasoning tree for a reasoning problem using PolicyModel and RewardModel,</cell><cell>Policy Model, Reward Model, long</cell></row><row><cell></cell><cell></cell><cell>where each node represents a step, and use the reasoning tree to construct long</cell><cell>thought Construction Algorithm</cell></row><row><cell></cell><cell></cell><cell>thought</cell><cell></cell></row><row><cell>9</cell><cell>Attempt3: Multi-Agent Approach</cell><cell>Use a multi-agent debate system to solve reasoning problems and integrate reasoning</cell><cell>Policy Model, Reward Model, Algo-</cell></row><row><cell></cell><cell></cell><cell>paths, including reflection and backtracking, into long thought data</cell><cell>rithm</cell></row><row><cell>10</cell><cell>Attempt4: Complete Human Thought Process</cell><cell>Human experts create a small amount of high-quality long thought data</cell><cell>Human</cell></row><row><cell></cell><cell>Annotation</cell><cell></cell><cell></cell></row><row><cell>11</cell><cell>Process-level Reward Model</cell><cell>Used to score each reasoning step in the reasoning tree, providing reasons</cell><cell>Reward Model</cell></row><row><cell>12</cell><cell>Construct of Reasoning Tree</cell><cell>Construct a reasoning tree where each node represents a reasoning step</cell><cell>Policy Model, Search Algorithm</cell></row><row><cell>13</cell><cell>Integrating Reasoning Tree into Long</cell><cell>Use the reasoning tree to construct long thought data that includes backtracking and</cell><cell>Reasoning Tree; Long Thought Con-</cell></row><row><cell></cell><cell>Thought</cell><cell>reflection, rather than a direct forward reasoning chain</cell><cell>struction Algorithm</cell></row><row><cell>14</cell><cell>Teacher-Student Incentive-Driven Data Con-</cell><cell>Student (Policy Model) continuously reasons forward, while Teacher (Critique</cell><cell>Policy Model, Reward Model, Algo-</cell></row><row><cell></cell><cell>struction</cell><cell>Model) provides feedback to Student, points out errors, and helps with backtracking</cell><cell>rithm</cell></row><row><cell></cell><cell></cell><cell>and reflection</cell><cell></cell></row><row><cell>15</cell><cell>Reward &amp; Critique Setup</cell><cell>The selection of Reward Model or Critique Model includes using the open-source</cell><cell>MathShepherd, o1-mini</cell></row><row><cell></cell><cell></cell><cell>model MathShepherd to score steps 0-1, or using powerful closed-source models</cell><cell></cell></row><row><cell></cell><cell></cell><cell>like o1-mini to directly indicate the correctness of steps</cell><cell></cell></row><row><cell>16</cell><cell>On-Policy Sampling &amp; Searching Tree</cell><cell>On-policy setting, using the target model to be improved as the policy model to</cell><cell>Policy Model, Reward Model, Math</cell></row><row><cell></cell><cell></cell><cell>provide reasoning steps. To speed up tree construction, use the reward model and</cell><cell>Training Set</cell></row><row><cell></cell><cell></cell><cell>corresponding algorithms to prune the tree during its construction</cell><cell></cell></row><row><cell>17</cell><cell>Off-Policy PRM800k Tree</cell><cell>OpenAI officially released PRM800k, a dataset that contains both Reasoning Tree</cell><cell>PRM800k Dataset</cell></row><row><cell></cell><cell></cell><cell>and Process-Level Reward, used to construct the corresponding Reasoning Tree with</cell><cell></cell></row><row><cell></cell><cell></cell><cell>reward. Since reasoning steps are provided by humans rather than the target model</cell><cell></cell></row><row><cell></cell><cell></cell><cell>to be improved, it is set as off-policy</cell><cell></cell></row><row><cell>18</cell><cell>1st Construction of Reasoning Tree</cell><cell>Construction of the first-generation reasoning tree. Train the Policy Model M1 using</cell><cell>Policy Model: DeepSeekMath-</cell></row><row><cell></cell><cell></cell><cell>DeepSeekMath-Base-7B on Abel data and use MathShepherd to provide the Process</cell><cell>7B + Abel; Reward Model:</cell></row><row><cell></cell><cell></cell><cell>Level Reward Model. Generate and prune the tree using the Beam Search algorithm</cell><cell>Mathshepherd-Mistral-7B</cell></row><row><cell>19</cell><cell>1st Round of Long Thought Integration</cell><cell>Use PRM800k data to synthesize long thought training data</cell><cell>1st Reasoning Tree; long thought</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Construction Algorithm</cell></row><row><cell>20</cell><cell>Evaluation</cell><cell>Evaluate the synthesized long thought</cell><cell>Evaluation Methods, Long Thought</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Data</cell></row><row><cell>21</cell><cell>Training</cell><cell>Train models using long thought</cell><cell>Long Thought, Model</cell></row><row><cell>22</cell><cell>Pretrain</cell><cell>Pre-train the model using large amounts of data</cell><cell>Massive Long Thought Data, Model</cell></row><row><cell>23</cell><cell>Post-train</cell><cell>Fine-tune the model using long thought on the pre-trained model</cell><cell>Long Thought Data, Pretrained</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model</cell></row><row><cell>24</cell><cell>Iterative Train</cell><cell>Iterative training to improve the model</cell><cell>Fine-tuned Model</cell></row><row><cell>25</cell><cell>Preference Learning</cell><cell>Preference learning, enabling models with reflection and backtracking capabilities to</cell><cell>SFT Model; Data</cell></row><row><cell></cell><cell></cell><cell>automatically select more effective answering strategies</cell><cell></cell></row><row><cell>26</cell><cell>SFT</cell><cell>Supervised learning, directly training the model with long thought data</cell><cell>Pretrained Model; Long Thought</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Data</cell></row><row><cell>27</cell><cell>RL</cell><cell>Reinforcement learning, such as PPO</cell><cell>SFT Model; Reward Model</cell></row><row><cell>28</cell><cell>DPO</cell><cell>Direct preference optimization, a stable preference learning algorithm</cell><cell>Preference data; SFT Model</cell></row><row><cell>29</cell><cell>Analysis Tool</cell><cell>A platform for visualizing long thought data and analyzing model responses</cell><cell>Streamlit</cell></row><row><cell>30</cell><cell>Human Check</cell><cell>Human experts analyze and evaluate long thought data and model responses</cell><cell>Human; Long Thought Data; Model</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Response</cell></row><row><cell>31</cell><cell>2nd Round of Long Thought Integration</cell><cell>Use the constructed second-generation reasoning tree to synthesize long thought</cell><cell>2nd Reasoning Tree; long thought</cell></row><row><cell></cell><cell></cell><cell>training data</cell><cell>Construction Algorithm</cell></row><row><cell>32</cell><cell>2nd Construction of Reasoning Tree</cell><cell>Build the second-generation reasoning tree, replacing the Reward Model with o1-</cell><cell>Policy Model: DeepSeekMath-7B +</cell></row><row><cell></cell><cell></cell><cell>mini, which directly indicates the correctness of steps and performs pruning, provid-</cell><cell>Abel; Reward Model: o1-mini</cell></row><row><cell></cell><cell></cell><cell>ing more accurate rewards</cell><cell></cell></row><row><cell>33</cell><cell>Fine-Grained, Thought-Centric Evaluation</cell><cell>Conduct a more fine-grained evaluation of synthesized long thought data to enhance</cell><cell>Long Thought Data</cell></row><row><cell></cell><cell></cell><cell>the effectiveness of various actions within long thoughtt</cell><cell></cell></row><row><cell>34</cell><cell>Experiments on Long Thought Scaling Law</cell><cell>Experiment with the scaling law of long thought in terms of training time and</cell><cell>Massive Long Thought Data of Var-</cell></row><row><cell></cell><cell></cell><cell>inference time</cell><cell>ious Formation</cell></row><row><cell>35</cell><cell>Human-AI Collaboration for Quality Thought</cell><cell>Use human-generated high-quality long thought data</cell><cell>Human</cell></row><row><cell>36</cell><cell>3rd Round Long Thought Integration</cell><cell>Synthesize the third-generation long thought data, further improving quantity and</cell><cell>Massive Reasoning Trees; Construc-</cell></row><row><cell></cell><cell></cell><cell>quality</cell><cell>tion Algorithm</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://openai.com/index/learning-to-reason-with-llms/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://streamlit.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>More other models have already been in our waiting list.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Student-Explorer Perspective Instead of presenting the solution as if the answer is already known, LLMs are encouraged using a tone of discovery, where the they solving the problem is thinking through it for the first time. This fosters curiosity and encourages students to think critically, making them feel like they are part of the learning process rather than simply receiving information.7 Detailed Event Explanation of Our Research Exploration</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We would like to express our sincere gratitude to the <rs type="institution">Shanghai Innovation Institute</rs> for providing an exceptional environment for our discussion and debate, which has laid a solid foundation for the advancement of our project. We deeply indebted to all co-authors, with special appreciation extended to the students from GAIR. Your dedication in sacrificing your National Day holiday to collaborate on this groundbreaking endeavor is truly commendable. The progress of this project would not have been possible without your unwavering commitment and hard work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Token  <ref type="table">table</ref> presents key metrics including the token count, the line count, the average number of words per line, and the frequency of the highest-occurring words or phrases derived using the n-gram algorithm. These keywords reflect the structure and style of the reasoning process, highlighting how the model introduces logical steps, alternatives, or corrections in different contexts.</p><p>6.4 Q4: How to Construct Reward Models?</p><p>The first step in utilizing the reward model is to define the granularity. Instead of focusing solely on the final results, we aim to enhance the capabilities of LLMs specifically in reflection, backtracking, and related cognitive processes. Therefore, we define the evaluation granularity at the step level. Specifically, we use the fine-tuning data from <ref type="bibr" target="#b3">Chern et al. (2023)</ref> to make the solutions distinct by line numbers. The process of implementing the reward model can involve using either open-source reward models or proprietary models. We compare the performance of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep blue</title>
		<author>
			<persName><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Joseph</forename><surname>Hoane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng Hsiung</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(01)00129-1</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Alphamath almost zero: process supervision without process</title>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minpeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<idno>preprint, abs/2405.03553</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiewen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/GAIR-NLP/abel" />
		<title level="m">Generative ai for math: Abel</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><surname>Fiedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Implicit chain of thought reasoning via knowledge distillation</title>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.01460</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A tale of tails: Model collapse as a change of scaling laws</title>
		<author>
			<persName><forename type="first">Elvis</forename><surname>Dohmatob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Charton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Kempe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards revealing the mystery behind chain of thought: a theoretical perspective</title>
		<author>
			<persName><forename type="first">Guhao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gerstgrasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apratim</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Sleight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajashree</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gromov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithya</forename><surname>Samavedhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyue</forename><surname>Gao</surname></persName>
		</author>
		<idno>preprint, abs/2404.05221</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large language models can self-improve</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><forename type="middle">Shane</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.11610</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Run-Ze</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyumanshan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiadi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-alignment with instruction backtranslation</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05-07">2024. May 7-11, 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Chain of thought empowers transformers to solve inherently serial problems</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno>preprint, abs/2402.12875</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Let&apos;s verify step by step</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Cobbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations, ICLR 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05-07">2024. May 7-11, 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improve mathematical reasoning in language models by automated process supervision</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<idno>preprint, abs/2406.06592</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Selfcheck: Using llms to zero-shot check their own step-by-step reasoning</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<idno>ArXiv, abs/2308.00436</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Iterative reasoning preference optimization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2404.19733</idno>
		<idno>CoRR, abs/2404.19733</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Recursive introspection: Teaching language model agents how to self-improve</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18219</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Grandmaster-level chess without search</title>
		<author>
			<persName><forename type="first">Anian</forename><surname>Ruoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Delétang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Medapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Grau-Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Catt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04494</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<idno>ArXiv, abs/2401.00448</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The curse of recursion: Training on generated data makes models forget</title>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Shumailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakhar</forename><surname>Shumaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Scaling llm test-time compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno>ArXiv, abs/2408.03314</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Easy-to-hard generalization: Scalable alignment beyond human supervision</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno>preprint, abs/2403.09472</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Solving math word problems with process-and outcome-based feedback</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<idno>preprint, abs/2211.14275</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">a. Q*: Improving multi-step reasoning for llms with deliberative planning</title>
		<author>
			<persName><forename type="first">Chaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Bo</surname></persName>
		</author>
		<idno>preprint, abs/2406.14283</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<title level="m">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</title>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</title>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9426" to="9439" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-taught evaluators</title>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Golovneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Fazel-Zarandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge</title>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Golovneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2407.19594</idno>
		<idno>CoRR, abs/2407.19594</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Progress or regress? self-improvement reversal in post-training</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2407.05013</idno>
		<idno>CoRR, abs/2407.05013</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Evaluating mathematical reasoning beyond accuracy</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno>preprint, abs/2404.05692</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-rewarding language models</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning, ICML 2024</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-07-21">2024. July 21-27, 2024</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Quiet-star: Language models can teach themselves to think before speaking</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varuna</forename><surname>Jayasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09629</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Star: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15476" to="15488" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Star: self-taught reasoner bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS &apos;22</title>
		<meeting>the 36th International Conference on Neural Information Processing Systems, NIPS &apos;22<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mr-gsm8k: A meta-reasoning benchmark for large language model evaluation</title>
		<author>
			<persName><forename type="first">Zhongshen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2312.17080</idno>
		<idno>CoRR, abs/2312.17080</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learn beyond the answer: Training language models with reflection for mathematical reasoning</title>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.12050</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Judging llm-asa-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fine-tuning language models from human preferences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
