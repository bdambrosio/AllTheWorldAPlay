<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Violet</forename><surname>Xiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kanishk</forename><surname>Gandhi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anikait</forename><surname>Singh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chase</forename><surname>Blagden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Duy</forename><surname>Phung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Lile</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dakota</forename><surname>Mahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis</forename><surname>Castricato</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SynthLabs.ai</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan-Philipp</forename><surname>Fränken</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBC1A5EC128A8D7B8EBB09F5770E1256</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-01-10T20:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.</p><p>Give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime.</p><p>-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction 1.Motivation</head><p>A key aspect of the current era of Large Language Models has been the foundational principle of next-token prediction <ref type="bibr">(Elman, 1990;</ref><ref type="bibr" target="#b26">Jordan, 1997)</ref>. That is, tokenizing text (or other continuous modalities) into a discrete sequence in the following way: "The quick brown fox jumps over the lazy dog." → y 1 , y 2 , . . . , y 𝑛 , where y 𝑖 are elements of some finite vocabulary and, subsequently, train a large parameterized neural network 𝑝 𝜃 (transformer) model with the following maximum likelihood objective:</p><formula xml:id="formula_0">ℒ 𝜃 = E 𝒟 train [︃ - ∑︁ 𝑡 log 𝑝 𝜃 (y 𝑡+1 |y ≤𝑡 )</formula><p>]︃ .</p><p>Behind this approach is a simple principle often abbreviated as "compression is intelligence", or the model must approximate the distribution of data and perform implicit reasoning in its activations in order to predict the next token (see Solomonoff Induction; Solomonoff 1964). That is, language models learn the implicit meaning in text, as opposed to the early belief some researchers held that sequence-to-sequence models (including transformers) simply fit correlations between sequential words.</p><p>A fundamental question, however, is: What is the relationship between the complexity of the underlying data stream and the model's capability to learn the data-generating algorithm? While natural language has some irreducible entropy, this question holds even in deterministic scenarios (as demonstrated below). Mathematical reasoning is a good domain to demonstrate and evaluate these questions. One can ask an LLM to generate a response to questions like: "What is 1+2?" to which most immediately answer "3". We can increase the complexity of the question by asking, for example:</p><p>"What is the value of (𝑥 2 -1)(𝑥 + 1) 𝑥 3 -𝑥 -1 𝑥 evaluated at 𝑥 = 𝜋?" This is actually a simple question since the expression is canonically equal to 1, yet even powerful LLMs such as GPT-4o and Claude fail to answer it correctly even once. That is, the true conditional generative process 𝑝(y 𝑡+1 |y 𝑡 ) could have arbitrarily high computational complexity even when it is deterministic as in the example above. In contrast, if we instruct models to "think step by step" and produce a chain-of-thought we can significantly improve their capabilities producing traces like the following:</p><p>We are tasked with evaluating (𝑥 2 -1)(𝑥 + 1) 𝑥 3 -𝑥 -1 𝑥 at 𝑥 = 𝜋. Let us simplify the expression before substituting 𝑥 = 𝜋.</p><p>Step 1: Simplify (𝑥 2 -1)(𝑥 + 1)</p><p>The numerator of the first term can be rewritten:</p><p>𝑥 2 -1 = (𝑥 -1)(𝑥 + 1), so (𝑥 2 -1)(𝑥 + 1) = (𝑥 -1)(𝑥 + 1) 2 .</p><p>Step 2: Simplify 𝑥 3 -𝑥 in the denominator Factorize 𝑥 3 -𝑥: 𝑥 3 -𝑥 = 𝑥(𝑥 2 -1) = 𝑥(𝑥 -1)(𝑥 + 1).</p><p>Thus, the first term becomes:</p><p>(𝑥 2 -1)(𝑥 + 1) 𝑥 3 -𝑥 = (𝑥 -1)(𝑥 + 1) 2 𝑥(𝑥 -1)(𝑥 + 1) .</p><p>Cancel (𝑥 -1) and one factor of (𝑥 + 1):</p><p>(𝑥 2 -1)(𝑥 + 1) 𝑥 3 -𝑥 = 𝑥 + 1 𝑥 .</p><p>Step 3: Subtract 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>Now substitute this simplified result into the full expression:</p><formula xml:id="formula_1">𝑥 + 1 𝑥 - 1 𝑥 .</formula><p>Combine the terms over the common denominator 𝑥:</p><formula xml:id="formula_2">𝑥 + 1 𝑥 - 1 𝑥 = 𝑥 + 1 -1 𝑥 = 𝑥 𝑥 = 1.</formula><p>Step 4: Evaluate at 𝑥 = 𝜋 The expression simplifies to 1, which is independent of 𝑥. Thus, the value at 𝑥 = 𝜋 is: 1 .</p><p>There have been multiple explanations for this phenomenon <ref type="bibr" target="#b44">(Merrill &amp; Sabharwal, 2023;</ref><ref type="bibr" target="#b56">Prystawski et al., 2024;</ref><ref type="bibr">Ye et al., 2024a;</ref><ref type="bibr">Dutta et al., 2024)</ref>, however we focus on the complexity hypothesis <ref type="bibr" target="#b44">(Merrill &amp; Sabharwal, 2023)</ref>. Specifically, a CoT expansion allows for (in-theory) arbitrarily large amounts of compute to be deployed towards the prediction of answer tokens. A large theoretical literature has been developed arguing for the representational complexity of transformers with and without CoT <ref type="bibr" target="#b44">(Merrill &amp; Sabharwal, 2023;</ref><ref type="bibr" target="#b50">Nowak et al., 2024;</ref><ref type="bibr" target="#b38">Li et al., 2024)</ref>. The literature says that endowing LLMs with CoT allows them to represent new orders of complexity and (with assumptions, such as infinite memory) may even make them Turing complete. However, in practice, today's LLMs can only reliably solve problems of limited complexity <ref type="bibr" target="#b75">(Snell et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Outline</head><p>In this paper, we investigate the limitations of current LLMs in handling complex reasoning tasks and propose a novel framework, Meta Chain-of-Thought (Meta-CoT), to address these shortcomings. We argue that traditional Chain-of-Thought (CoT) methods, while effective for simpler problems, fail to capture the true data-generating process of complex reasoning which often involves a non-linear, iterative, and latent process of exploration and verification. Meta-CoT extends CoT by explicitly modeling this latent "thinking" process, which we hypothesize is essential for solving problems that require advanced reasoning capabilities.</p><p>We draw inspiration from Cognitive Science's dual-process theory, framing Meta-CoT as a form of System 2 reasoning. We establish the theoretical foundations of Meta-CoT, demonstrating how it can be realized through systematic search processes, and how these processes can be internalized within a single auto-regressive model. We then present empirical evidence supporting our claims, including analyses on state-of-the-art models like OpenAI's o1 <ref type="bibr" target="#b51">(OpenAI, 2024)</ref> and DeepSeek-R1 <ref type="bibr" target="#b10">(DeepSeek, 2024)</ref>, which exhibit behaviors consistent with internalized (in-context) search. We further explore methods for training models on Meta-CoT through process supervision, and synthetic data generation via search algorithms like Monte Carlo Tree Search (MCTS) and A*.</p><p>Finally, we outline a concrete pipeline for achieving Meta-CoT in a single end-to-end system, incorporating instruction tuning with linearized search traces and reinforcement learning (RL) posttraining. We discuss open research questions, including the scaling laws of reasoning and search, the role of verifiers, and the potential for discovering novel reasoning algorithms through meta-RL. We also present the "Big MATH" project, an effort to aggregate over 1,000,000 high-quality, verifiable math problems to facilitate further research in this area. Our work provides both theoretical insights and a practical road map to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Meta Chain-Of-Thought</head><p>In this section, we first formulate the meta chain-of-thought process and discuss how it can describe the problem solving process for complex reasoning problems. Then, we describe and demonstrate why classical chain-of-thought fails under certain circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deriving The Meta-CoT Process</head><p>A question to ask ourselves is: Should language models with Chain-Of-Thought prompting really be able to express any function, and thus solve arbitrarily complex problems, which was the theoretical point of the previous section? We will stick with the mathematical reasoning domain for the purpose of the discussion. Today, the capabilities of frontier models are enough for a large class of mathematical reasoning problems. Current state-of-the art systems such as GPT-4o and Claude largely solve the Hendrycks MATH Levels 1-3 Benchmark <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref>, however, they still struggle with advanced problems such as those in Levels 4 and 5, HARP <ref type="bibr" target="#b93">(Yue et al., 2024)</ref> and Omni-MATH <ref type="bibr" target="#b15">(Gao et al., 2024)</ref> (as well as other advanced reasoning tasks). We put forward the following theory to explain these empirical observations. Reasoning data present in pre-training corpuses does not represent the true data generation process, especially for complex problems, which is a product of extensive latent reasoning. Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion.</p><p>In more details, the CoT reasoning data prevalent in the pre-training corpus and post-training instruction tuning follows the true data-generating process for solutions of simple problems such as algebraic computations, counting, basic geometry etc.. That is, for example, the textbook solutions for solving high-school algebra present the general process of generating those solutions. If we follow some set of steps or approaches present in existing textbooks, we can eventually arrive at the solution. Hence, these are learnable with a constant-depth transformers that can express the complexity of each individual step in the process. In contrast, complex reasoning problems do not follow that pattern. We may have a set of triples (q, S, a) of questions q, solution steps S = (s 1 , . . . , s 𝑛 ) and (optionally) answers a, but the true data generation process is not auto-regressive:</p><formula xml:id="formula_3">q → z 1 → . . . → z 𝐾 → (s 1 , . . . , s 𝑛 , a),<label>(1)</label></formula><p>where z 𝑖 are the latent "thoughts" left out of the solutions steps, which can be represented fully with left-to-right generation, while the dataset solution S = (s 1 , . . . , s 𝑛 ) is generated jointly. Let us illustrate this with an example from the International Mathematics Olympiad 2011. This is the (in)famous "windmill" problem:</p><p>"Let 𝒮 be a finite set of at least two points in the plane. Assume that no three points of 𝒮 are collinear. A windmill is a process that starts with a line ℓ going through a single point 𝑃 ∈ 𝒮.</p><p>The line rotates clockwise about the pivot 𝑃 until the first time that the line meets some other point belonging to 𝒮. This point, 𝑄, takes over as the new pivot, and the line now rotates clockwise about 𝑄, until it next meets a point of 𝒮. This process continues indefinitely. Can we choose a point 𝑃 in 𝒮 and a line ℓ going through 𝑃 such that the resulting windmill uses each point of 𝒮 as a pivot infinitely many times."</p><p>which has the following solution:</p><p>"Let |𝑆| = 𝑛. Now consider an arbitrary point 𝑃 in 𝑆 and choose a line 𝑙 through 𝑃 which splits the points in the plane into roughly equal chunks. Next notice that as the line rotates it will sweep a full 2𝜋 angle against some fixed reference frame. Now take another random point 𝑃 ′ and similarly constructed stationary line 𝑙 ′ . At some point in the windmill process we will have 𝑙||𝑙 ′ . However notice that 𝑙 and 𝑙 ′ split the points into the same two sets and are parallel. Therefore we must have that 𝑙 ≡ 𝑙 ′ and thus 𝑙 passes through 𝑃 ′ . This of course holds for any 𝑃 ′ ∈ 𝑆. Applying the same argument recursively yields the final proof that it is in fact possible to make such a construction for any set 𝑆 with these properties."</p><p>The solution above does not use any prior knowledge and fits within a few sentences. Yet, this problem was considered among the most difficult in the competition (there were only a handful of solutions among the 600+ participants). What makes the problem difficult is the highly non-linear structure of the solution. Most participants would follow the standard "generative" solution process and investigate approaches based on the convex hull construction or use tools from Hamiltonian graph theory, none of these leading to a solution. Instead, participants who solved the problem followed an experimental approach with a lot of geometric exploration and inductive reasoning. Moreover, the solution itself is not linear. It's hard to see the utility of the proposed construction in the beginning without the analysis of the dynamics of 𝑙. Essentially, to start generating the solution requires that we already know the full approach. The underlying generative process of the solution is not auto-regressive from left-to-right.</p><p>We can formalize this argument through the interpretation of reasoning as a latent variable process <ref type="bibr" target="#b55">(Phan et al., 2023)</ref>. In particular, classical CoT can be viewed as Figure <ref type="figure">1</ref>: Top: Performance of current frontier models by size on the HARP mathematics benchmark <ref type="bibr" target="#b93">(Yue et al., 2024)</ref> by difficulty level and topic. The OpenAI O1 series significantly out-performs prior generation models across the board. Source: Figure <ref type="figure" target="#fig_1">3</ref> in <ref type="bibr" target="#b93">(Yue et al., 2024)</ref>. Bottom Average number of tokens generated by each model grouped by difficulty level, as well as average number of tokens in human-generated solutions (using GPT4 tokenizer). Source: Figure <ref type="figure">4</ref> in <ref type="bibr" target="#b93">(Yue et al., 2024)</ref>.</p><p>𝑝 data (a|q) ∝</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∫︁</head><p>𝑝 data (a|s 1 , . . . , s 𝑛 , q) ⏟ ⏞</p><formula xml:id="formula_4">Answer Generation 𝑛 ∏︁ 𝑡=1 𝑝 data (s 𝑡 |s &lt;𝑡 , q) ⏟ ⏞ CoT 𝑑S,</formula><p>i.e., the probability of the final answer being produced by a marginalization over latent reasoning chains. We claim that for complex problems, the true solution generating process should be viewed as 𝑝 data (a, s 1 , . . . , s 𝑛 |q) ∝</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∫︁</head><p>𝑝 data (a, s 1 , . . . , s 𝑛 |z 1 , . . . , z 𝑘 , q) ⏟ ⏞</p><formula xml:id="formula_5">Joint Answer+CoT 𝐾 ∏︁ 𝑡=1 𝑝 data (z 𝑡 |z &lt;𝑡 , q) ⏟ ⏞ Meta-CoT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑑Z,</head><p>i.e., the joint probability distribution of the solution (a, s 1 , . . . , s 𝑛 ) is conditioned on the latent generative process. Notice that this argument is a meta-generalization of the prior CoT argument, hence why we will refer to the process q → z 1 → . . . → z 𝐾 as Meta-CoT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Why Does (Classical) CoT Fail?</head><p>Based on the previous discussion, a natural question follows: Why do LLMs fail at these advanced reasoning tasks? Above we proposed that the pre-training and instruction-tuning corpora consist of data of the type (q, s 1 , . . . , s n , a), which do not contain the true data generating process as shown in Equation <ref type="formula" target="#formula_3">1</ref>. Indeed, the solution to the windmill problem above is widely available on the internet, but there is little to no discussion about the ways in which commonly used convex hull or planar graph arguments fail. This is true in generaltextbooks contain advanced proofs but not the full thought process of deriving these proofs. We can then apply the same general meta-argument of why CoT is necessary to the Meta-CoT case: simply because the conditional solution-level distribution 𝑝 data (a, s 1 , . . . , s 𝑛 |q) (without the intermediate Meta-CoT) on hard reasoning questions can have arbitrarily high complexity in the same way that 𝑝 data (a|q) can have arbitrarily high complexity in the standard CoT setting. We will examine some empirical evidence for our stance in the following sections.</p><p>We will argue in the following chapters that the OpenAI o1 model series performs full Meta-CoT reasoning in an auto-regressive fashion at inference time. A useful analysis is presented in a new mathematics benchmark with challenging high-school Olympiad-level problems <ref type="bibr" target="#b93">(Yue et al., 2024)</ref>. Figure <ref type="figure">1</ref> sourced from that work shows the relevant results. First, we see that the o1 family of models significantly outperforms "standard" reasoning models across the board. However, the gap between o1 and other models' performance increases on higher difficulty problems (with the interesting exception of the LLaMa 3.1 models), that is, problems which have higher solution complexity.</p><p>Furthermore, the bottom half of Figure <ref type="figure">1</ref> shows the average number of tokens generated grouped by problem difficulty level. First, we see that outside of the o1 series of models, LLMs generate solutions of comparable lengths to humans. While this may initially appear quite intriguing, suggesting that models are learning to approximate or replicate human reasoning, the simple explanation is that models are learning solutions to match the training data -i.e. 𝑝 data (a, s 1 , . . . , s 𝑛 |q). Much more intriguingly, the o1 series of models exhibits significantly different token behavior. We see that:</p><p>1. On level 1 problems the o1 series generates a comparable number of tokens to human-written solutions. These are the types of problems where the training solutions likely match the true data generation process and each individual logical step can be internalized in a constant-depth transformer. 2. At higher difficulty, the o1 series of models generates significantly more tokens per problem and also widens the performance gap over the classical reasoning models. In fact the gap between the inference compute used by the o1 model and prior series of models seems to scale with the complexity of the problems. We hypothesize that in those more challenging problems the solutions do NOT in fact represent the true data generative process, which is instead better approximated by the more extensive Meta-CoT generated by the o1 family of models.</p><p>Of course, in practice the distinction between these two is not so clear cut, and in fact the constantdepth transformer can likely internalize part of the Meta-CoT generative process as evidenced by the gradation of (Meta-)CoT lengths from Levels 2-6 in Figure <ref type="figure">1</ref>. In the next chapter we will discuss in greater detail what the Meta-CoT process actually represents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Towards Deliberate Reasoning With Language Models -Search</head><p>In the previous section we introduced the Meta-CoT process and argued that LLMs fail on advanced reasoning tasks because the training data does not adequately represent the true data generation of training on on the original data (x) and base filtered data (o) and evaluated using either greedy or majority vote decoding. For all settings, the filtered dataset shows improved scaling, and has not plateaued. Right: A comparison of pass@k for increasing 𝑘 with a model trained on the base filtered dataset. The chart shows how increasing 𝑘 leads to a much greater probability of at least 1 correct solution, even for a relatively small model. Additionally, the performance improvement from 𝑘 = 32 to 64 does not show plateauing, suggesting that 𝑘 can further be increased for improved performance.</p><p>process, i.e. text corpora do not include (or only include limited amounts of) Meta-CoT data. So the remaining question is: what does the true data generating process look like?</p><p>1. First, we argue that for many advanced reasoning or goal-oriented problems there exist meaningful gaps between the complexity of generation and verification. This is of course one of the fundamental open problems of theoretical computer science and any attempt to prove this is significantly beyond the scope of the current writing, but we will review what we believe to be compelling empirical evidence from the literature. 2. Second, assuming a non-trivial generator-verifier gap, we argue that the solutions to challenging problems present in text corpora are the outcomes of an extended search process, which itself is not represented in the data.</p><p>We will dive deeper into these two points in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inference-Time Compute: Search</head><p>The first point above (generation-verification gap) has recently become a popular research and discussion direction under the framework of "deploying inference-time compute" and we explore this in our first experiment. We start with a LLaMa 3.1 8B base model <ref type="bibr">(Dubey et al., 2024)</ref> and carry out extensive supervised fine-tuning on the Numina MATH dataset <ref type="bibr" target="#b38">(LI et al., 2024)</ref>. Refer to Figure <ref type="figure" target="#fig_0">2</ref> for results and Section 8.1 for dataset details. For each intermediate checkpoint we evaluate performance on the Hendrycks MATH <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref> 500 problems evaluation dataset <ref type="bibr" target="#b39">(Lightman et al., 2023)</ref>. Based on the results, we make a few observations here:</p><p>1. We evaluate pass@𝑘 (i.e. using an oracle verifier) on intermediate checkpoints and see a significant jump in performance for increasing 𝑘. While zero-shot performance with greedy decoding improves from about 20% to 40% (see the base filter on the left side of Figure <ref type="figure" target="#fig_0">2</ref>), even the first model checkpoint outperforms these results at pass@4 (right side of Figure <ref type="figure" target="#fig_0">2</ref>). Moreover, the pass@64 for the final checkpoint of an 8B model achieves accuracy close to 85%, outperforming the zero-shot performance of many current frontier models. 2. We also evaluate performance under majority voting with 𝑘 = 8 and 𝑘 = 64. There is continuous improvement for both increased training and samples, with maj@64 outperforming the greedy model performance with only 15% of the training compute without access to a ground-truth verifier.</p><p>These results demonstrate that even as we directly optimize for answer generation ability by finetuning on increasing amounts of SFT data, there remains a consistent verifier-generator gap, as evidenced by the improved performance in botht eh pass@k and majority voting settings. Recent literature has observed similar results on post-training sampling <ref type="bibr" target="#b39">(Lightman et al., 2023;</ref><ref type="bibr" target="#b6">Brown et al., 2024;</ref><ref type="bibr" target="#b75">Snell et al., 2024)</ref>. However, most of these studies do not systematically evaluate the effects of varying amounts of training data, compute, and model size which we believe is a fruitful direction for additional empirical work. These questions are important as the observed gains from additional inference might disappear at larger scales and training -i.e. the model may be able to fully internalize the reasoning process. This definitely seems to be the case for advanced models and simpler benchmarks like GSM8k <ref type="bibr" target="#b9">(Cobbe et al., 2021)</ref>. While we observe the opposite result in our experiments, we admit that our results are the outcomes of preliminary study and additional work is required, but we will argue from a theoretical point in Section 6 that a persistent search gap remains in domains with high enough epistemic uncertainty. Besides this point, the question remains whether the improvement from increased inference can be effectively achieved without oracle verifiers or environment feedback. In theory, it is possible to generate correct solutions under an increased inference budget, but we may not be able to verify them effectively, as verification complexity may be just as high as, or even higher than, generation complexity. We will address this issue next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference-Time Compute: Verification</head><p>Several works focus on training verifier models, which explicitly evaluate the correctness of reasoning steps and solutions. Verifiers can be trained either using explicit binary classification <ref type="bibr" target="#b9">(Cobbe et al., 2021;</ref><ref type="bibr" target="#b39">Lightman et al., 2023;</ref><ref type="bibr" target="#b75">Snell et al., 2024;</ref><ref type="bibr" target="#b4">Anonymous, 2024;</ref><ref type="bibr">Setlur et al., 2024b)</ref> or modeling Figure <ref type="figure">4</ref>: Reasoning via Planning (RAP) demonstrates the search procedure described here. If we have access to a state evaluator, we can truncate branches with low values and backtrack to promising nodes, without resampling the same steps again. Source: Figure <ref type="figure" target="#fig_0">2</ref> in <ref type="bibr" target="#b20">(Hao et al., 2023)</ref>. evaluation directly in natural language, using the LLM-as-a-judge prior <ref type="bibr">(Zhang et al., 2024a;</ref><ref type="bibr" target="#b42">Mahan et al., 2024)</ref>. The unifying formulation of these approaches is the model 𝑣 𝜃 which evaluates a reasoning process 𝑣 𝜃 (q, S) → [0, 1]. Under this framework, 𝐾 candidate solutions (S 1 , . . . , S 𝐾 ) can be generated from a fixed generator 𝜋 𝜃 (•|q) and ranked based on their evaluation score.</p><p>S * = arg max{𝑣 𝜃 (q, S 1 ), . . . , 𝑣 𝜃 (q, S 𝐾 )} For empirical results, we refer the reader to Figure <ref type="figure" target="#fig_1">3</ref> sourced from <ref type="bibr">(Zhang et al., 2024a)</ref> which evaluates a number of verifier models 𝑣 𝜃 . Regardless of the efficiency of the verifier, there is a significant improvement in performance with additional online sampling. Moreover using explicitly trained verifier models outperforms naive inference-compute scaling strategies such as self-consistency or majority voting.</p><p>A question remains regarding the effect of using a fixed generation model (policy): Could this model be under-trained, and if it were further trained, could its zero-shot performance improve to the point where additional online search no longer provides meaningful improvement? We will address this in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">From Best-of-N To General Search</head><p>So far, we empirically explored best-of-N approaches, generating multiple full solutions independently and selecting the most promising one based on scores. However, this approach is inefficient because it requires exploring full solution paths, even if a mistake occurs early on, and may repeatedly sample the same correct steps. Instead, we can model reasoning as a Markov Decision Process (MDP), defined by the tuple ℳ = (𝒮, 𝒜, 𝑃, 𝑅, 𝛾), where:</p><p>• 𝒮: the set of states, where each state S ∈ 𝒮, consists of the prompt and generations so far, i.e.</p><p>S 𝑡 = (q, s 1 , . . . , s 𝑡 ). • 𝒜: the set of actions, where each action a ∈ 𝒜 will be represented as the next reasoning step a 𝑡+1 = s 𝑡+1 . • 𝑃 (s ′ | s, a): the transition probability function, representing the probability of transitioning to state s ′ when taking action a in state s. For simplicity, we will mostly consider the deter-ministic transition function 𝑃 (•|s 𝑡+1 , (q, s 1 , . . . , s 𝑡 )) → (q, s 1 , . . . , s 𝑡 , s 𝑡+1 ) that appends the next reasoning step to the context. In general, the environment dynamics can be more complex. For example, models with tool access have to call the actual tool and receive the environment feedback in context or even modify their environment such as the cases of SWE and Web agents. • 𝑅(s, a): the reward function, which provides a scalar reward for taking action a in state s.</p><p>We will assume zero intermediate rewards and final reward of 1 for a correct solution and zero otherwise, although this is not strictly necessary in the presence of a good process reward model <ref type="bibr">(Setlur et al., 2024c</ref>). • 𝛾 ∈ [0, 1]: the discount factor, balancing the trade-off between further computation and rewards.</p><p>We refer to the LLM generating the reasoning steps as the policy s 𝑡+1 ∼ 𝜋 𝜃 (•|S 𝑡 ). In addition we refer to a solution starting from s 0 = q as an episode or a trajectory. We will also use the notation z 𝑡 to represent individual reasoning steps that are part of the Meta-CoT and correspondingly denote Z 𝑡 = (q, z 1 , . . . , z 𝑡 ).</p><p>In the prior section we considered generating and ranking full solutions, which may be inefficient. We can extend the concept of a solution verifier from the prior section, to estimating the probability that a particular intermediate state will lead to a solution: 𝑣 𝜃 (q, S 𝑡 ) → [0, 1]. These models have become more widely known as Process Reward Models (PRMs) <ref type="bibr" target="#b39">(Lightman et al., 2023)</ref>. If we have access to such a model, we can improve the efficiency of the search process with the following steps:</p><p>1. Terminate a solution attempt that is not making progress, or is incorrect prior to reaching the final answer. 2. Reset the agent to any intermediate, previously visited, state that has a high likelihood of success.</p><p>Notice that with these two operations, and the general structure of language, we can implement any tree search procedure. This is the premise of several approaches <ref type="bibr" target="#b87">(Yao et al., 2023;</ref><ref type="bibr" target="#b20">Hao et al., 2023;</ref><ref type="bibr">Zhou et al., 2024a)</ref> with the RAP method <ref type="bibr" target="#b20">(Hao et al., 2023)</ref> illustrated in Figure <ref type="figure">4</ref>. These approaches use differing search strategies (DF-S/BFS vs. MCTS) and process guidance evaluation (generative self-evaluation vs. Monte-Carlo rollouts), but they all share the same core idea: formulate the reasoning problem as tree search guided by an intermediate heuristic function. As noted above, in theory, tree search does not induce a fundamental capability shift over parallel sampling, however, it may induce significant efficiency gains as demonstrated by <ref type="bibr" target="#b87">Yao et al. (2023)</ref>. In particular, Figure <ref type="figure" target="#fig_2">5</ref> (sourced from <ref type="bibr" target="#b87">Yao et al. (2023)</ref>) shows nearly 4 times increased efficiency, in terms of inference budget, on a toy reasoning problem (Game of 24) when using a tree-structured search approach compared to parallel sampling. While these earlier works focus on zero-shot (or close to zero-shot) performance on simple reasoning tasks, it is important to note that tree-search methods have been successfully scaled and deployed to a number of realistic agentic applications <ref type="bibr" target="#b32">(Koh et al., 2024;</ref><ref type="bibr" target="#b57">Putta et al., 2024;</ref><ref type="bibr" target="#b6">Brown et al., 2024;</ref><ref type="bibr" target="#b91">Yu et al., 2024)</ref>.  <ref type="bibr" target="#b25">(Jones, 2021)</ref>. Right: The trade-off between train-time and test-time compute, with progressively improving Elo (from bottom-left to top-right). Source: Figure <ref type="figure" target="#fig_6">9</ref> in <ref type="bibr" target="#b25">(Jones, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Is Search (Inference Time Compute) A Fundamental Capability Shift?</head><p>As pointed out earlier, the question remains whether inference-time search is a fundamental new capability or whether it is accessible with additional training. Results from classical RLHF tuning <ref type="bibr">(Dubois et al., 2024)</ref> suggest that this is a learnable capability, where zero-shot performance of post-trained models matches or outperforms the best-of-N paradigm.</p><p>We stipulate that performance on complex reasoning tasks is governed by a scaling law, which involves model size, training data (compute) and inference time compute. This is indeed consistent with the theoretical results of <ref type="bibr" target="#b38">Li et al. (2024)</ref> and the intuition presented in Section 2. Larger models are more capable of internalizing the Meta-CoT process in their activations, and are also capable of using longer inference-time Meta-CoT to approximate solutions with significantly higher computational complexity. Empirically, we have limited (but promising) evidence towards this hypothesis. A major prior work to study these questions is <ref type="bibr" target="#b25">Jones (2021)</ref>   <ref type="bibr" target="#b12">(Feng et al., 2024)</ref>.</p><p>out studies using the AlphaZero algorithm <ref type="bibr" target="#b72">(Silver et al., 2018)</ref>   <ref type="formula">2024</ref>) which carries out two iterations of MCTS fine-tuning using a LLaMa 7B on the GSM8k dataset <ref type="bibr" target="#b9">(Cobbe et al., 2021)</ref>. They show improved performance in zero-shot evaluations of the policy, as well as significant gains from using additional inference-time search, at both iterations 1 and 2 (full results are shown in Table <ref type="table" target="#tab_0">1</ref>). However, their work does not ablate the model size, data scaling, or inference-time search scaling, which remain under-explored in the literature for LLM reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Towards Meta-CoT Reasoning</head><p>In prior sections we: introduced the concept of Meta-CoT and argued that it is necessary for advanced reasoning, discussed the generator-verifier gap as a fundamental limitation, argued for search as a fundamental building block of the Meta-CoT, and discussed the utility of approaches integrating generator, verifier, and search components. However, the question remains on how to integrate these into a model to perform Meta-CoT or "System 2" reasoning. The first question we need to answer is: why do we actually need to internalize deliberate reasoning inside a single model? We propose two main reasons:</p><p>1. Efficiency: By incorporating search within the context of an auto-regressive model, exploration can be done efficiently since the model has access to all previously visited nodes, in context. Unique to the case of reasoning in natural language, many branches may contain semantically similar content, unlike other domains (e.g., board games), motivating the need for improved efficiency. In fact, even advanced reasoning models carry out many repeated steps of semantically identical reasoning as we show in Figure <ref type="figure" target="#fig_11">14</ref> and 15. In the remainder of this section, we explore how to train a model to internalize such a reasoning system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bootstrapping Meta-CoT</head><p>In this subsection, we overview the core idea behind the Self-Taught Reasoner (STaR) approach <ref type="bibr" target="#b94">(Zelikman et al., 2022;</ref><ref type="bibr" target="#b73">Singh et al., 2024;</ref><ref type="bibr" target="#b92">Yuan et al., 2023)</ref> to bootstrapping intermediate CoT steps and how to use a similar concept to generalize to meta-reasoning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Self-Taught Reasoner</head><p>The STaR method introduces an iterative bootstrapping approach designed to improve the reasoning capability of LLMs <ref type="bibr" target="#b94">(Zelikman et al., 2022)</ref>. STaR focuses on training models to generate and refine rationales, particularly for tasks requiring complex reasoning in a reinforcement learning-based manner. In this formulation we assume we have access to a dataset 𝒟 = {q (𝑖) , a (𝑖) } 𝑁 𝑖=1 of questions q that require reasoning along with corresponding answers a. Notice that we do not require access to ground-truth rationales for these problems. We begin by prompting a model â(𝑖) , Ŝ(𝑖) ∼ 𝜋(a, S|q (𝑖) ) to provide CoT rationale Ŝ(𝑖) = s</p><formula xml:id="formula_6">(𝑖) 1 , . . . , s (𝑖)</formula><p>𝑁 𝑖 and final answer â(𝑖) . We then filter the generated data, keeping only rationales that lead to a correct final answer (i.e., â(𝑖) = a (𝑖) ) to create a dataset of questions, (bootstrapped) rationales and answers 𝒟 STaR = {q (𝑖) , Ŝ(𝑖) , a (𝑖) } 𝑁 𝑖=1 . 𝒟 STaR is then used to train a model with the standard supervised fine-tuning objective:</p><formula xml:id="formula_7">ℒ STaR (𝜋 𝜑 ) = -E (q, Ŝ,a)∼𝒟 STaR [︁ -log 𝜋 𝜑 (a, Ŝ|q) ]︁ . (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>The above procedure is repeated over several iterations. The core idea behind STaR is to generate a training dataset of synthetic rationales through sampling and verification. We will extend that idea to the the concept of Meta-CoT below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Meta-STaR</head><p>We can generalize the above idea to Meta-CoT in a straightforward way. Consider a base policy 𝜋 𝜃 combined with some general search procedure over intermediate steps. Given a question q we perform the search procedure repeatedly to generate search traces ẑ1 , . . . , ẑ𝐾 until we find a final solution (s 1 , . . . , s 𝑛 ). If we can verify the final produced solution 𝑣(S) → {0, 1}, for example by using a formalization and verification approach (as in AlphaProof<ref type="foot" target="#foot_0">foot_0</ref> ) or some other outcome verification, we can then apply a similar approach to STaR. For example, we can construct a dataset 𝒟 STaR = {q (𝑖) , Ẑ(𝑖) , Ŝ(𝑖) } 𝑁 𝑖=1 and use a similar training objective as before:</p><formula xml:id="formula_9">ℒ Meta-STaR (𝜋 𝜑 ) = -E (q, Ẑ, Ŝ)∼𝒟 STaR [︁ -log 𝜋 𝜑 ( Ŝ, Ẑ|q) ]︁ .<label>(3)</label></formula><p>Essentially, we can use a base policy and search procedure to generate synthetic search data and then train the model to implement these in-context through the Meta-CoT concept. We are effectively proposing to linearize the search approaches described in Section 3 and teach an auto-regressive model to run them sequentially. So far we have deliberately been vague about how these search procedures and datasets look. We will now provide examples and proof of concept from the literature on practical approaches to this problem as well as synthetic examples of realistic training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Empirical Examples Of Internalizing Search</head><p>When we formulate search in a sequential fashion we can explicitly parameterize each component in language, or choose leave it implicit <ref type="bibr" target="#b14">(Gandhi et al., 2024)</ref>. Note that models trained with standard next token prediction still need to implicitly internalize all of these components anyway in order to accurately model the search sequence, even if they are not explicitly verbalized. However, allowing the model to vocalize it's certainty or estimated progress could allow for additional modeling capacity or be useful for interpretability purposes. We will present some examples of auto-regressive search procedures from the literature in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Small-Scale Empirical Results on Internalizing Search</head><p>Two particular prior works that explore the idea of in-context search are <ref type="bibr" target="#b86">Yang et al. (2022)</ref> and <ref type="bibr" target="#b35">Lehnert et al. (2024)</ref> which focus on mazes and other classical RL environments. The formulation from Lehnert    <ref type="formula">2024</ref>) is shown in Figure <ref type="figure" target="#fig_4">7</ref>, which illustrates linearizing A* search. In our framework the "Trace" corresponds to the Meta-CoT Z, and the "Plan" is the CoT output S. In this setting the search procedure is stated explicitly as it shows node states, actions, costs and heuristic values.</p><p>In this "stream" format we can then use standard auto-regressive language models with a next token-prediction objective to train a model to internalize the search process. Evaluation results are shown in Figure <ref type="figure" target="#fig_5">8</ref> sourced from the same paper. We observe empirical effects consistent with the scaling law hypothesis presented in 3.4; there is consistent improvement with additional training data and model size (train-time compute) across the board. A particularly interesting observation is the complexity scaling relationship in part (c) of the figure. At smaller mazes (lower complexity) the model directly producing the Plan (CoT) and performs comparably to smaller search (Meta-CoT) augmented models, however as maze size (complexity) increases we see a widening gap in performance between the search-augmented and zero-shot models. This is essentially identical to the results shown in Figure <ref type="figure">1</ref> on the challenging HARP benchmark <ref type="bibr" target="#b93">(Yue et al., 2024)</ref> between the prior frontier models and the o1 series. These empirical observations are well aligned with the intuition we presented in Section 2. For small mazes (low complexity problems) models are capable of internalizing the reasoning process, but as problem complexity (maze size) increases this becomes more challenging and model performance falls off compared to models which explicitly carries out a search procedure. Unfortunately, <ref type="bibr" target="#b35">Lehnert et al. (2024)</ref> did not publish inference compute scaling laws, but given the algorithmic structure of the training data we can presume that inference-time tokens scale with the same complexity as the A* search algorithm, which can be exponential in the branching factor, while the plan length is linear in 𝑛. These results would also be consistent with the inference costs on advanced math reasoning tasks reported in Figure <ref type="figure">1</ref>. <ref type="bibr" target="#b14">Gandhi et al. (2024)</ref> extend the linearized search idea to a more realistic reasoning task -the Countdown game -which requires the model to predict a sequence of mathematical operations on a given set of numbers to match a target value. While <ref type="bibr" target="#b14">Gandhi et al. (2024)</ref> use a fixed 250M parameter transformer model and do not explore or discuss the role of model size, training data, and complexity in terms of scaling performance, we obtain additional results in terms of inference-time scaling, shown in Figure <ref type="figure" target="#fig_6">9</ref>. Our findings demonstrate a consistent log-linear relationship between tokens spent and success rate. Similar results were also observed in recent work by <ref type="bibr" target="#b65">Schultz et al. (2024)</ref>, who train language models on linearized search traces obtained from MCTS on board game environments. Similar to the work of <ref type="bibr" target="#b14">Gandhi et al. (2024)</ref>, they find consistent improvements in performance as the model is given additional search budget at test-time (Figure <ref type="figure" target="#fig_6">9</ref> right). Note that these models demonstrate an inference-time scaling law with the same functional form as the o1 model on difficult mathematics problems <ref type="bibr" target="#b51">(OpenAI, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">In-context Exploration For LLMs</head><p>While the prior section showed promise in teaching auto-regressive language models to internalize complex search strategies involving exploration and backtracking, it remains unclear whether these results can generalize to realistic language domains. In this section we will overview several recent works, which show promise in internalizing episode-level search. Both <ref type="bibr" target="#b58">Qu et al. (2024)</ref> and <ref type="bibr" target="#b75">Snell et al. (2024)</ref> evaluate results using open-source LLMs in the 7B and larger range on problems from the MATH dataset <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref>. They pose the problem as sequential sampling -i.e. given a problem q, generating full solutions from the same model auto-regressively as</p><formula xml:id="formula_10">S 𝑗 ∼ 𝜋 𝜃 (•|S 𝑗-1 , . . . , S 1 , q) (4)</formula><p>where S 𝑖 are full solutions to the problem q. Both works formulate the problem as self-correction, or revisions, during training. The approach generates training data by concatenating a number of incorrect solutions with the correct revision and training on a linearized sequence (although the exact training objective use a particular weighting grounded in RL <ref type="bibr" target="#b54">(Peng et al., 2019)</ref>). The general objective follows the form</p><formula xml:id="formula_11">min 𝜃 E S 𝑖 ∼𝜋 ref (•|q),q∼𝒟 train [︀ -log 𝜋 𝜃 (S * |S 𝑗-1 , . . . , S 1 , q) ]︀ (5)</formula><p>where 𝑗 is a fixed number of in-context exploration episodes sampled from a fixed distribution 𝜋 ref (i.e. 𝜋 0 ) and S * is some optimal solution. Essentially, this can be considered a linearization of the Best-Of-N search strategy presented in Section 3.1 with rejection sampling. In this setting, the Meta-CoT represents search in full episodes Z = S 1 , . . . , S 𝑗-1 and S = S 𝑗 . At test time we can further control the quantity of compute by iteratively sampling from</p><formula xml:id="formula_12">S 𝑖 ∼ 𝜋 𝜃 (•|S 𝑖-1 , . . . , S 𝑖-𝑗 , q). (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>Representative results for this approach are are shown in Figure <ref type="figure">10</ref>, sourced from <ref type="bibr" target="#b75">(Snell et al., 2024)</ref>.</p><p>We see clear improvement in the pass@1 metric with additional amounts of in-context exploration episodes with nearly 6-7% gain from zero-shot to the level of saturation. At the same time, autoregressive generation shows clearly better scaling properties than independent parallel sampling (Figure <ref type="figure">10</ref> right). These results indicate that the model learns some degree of in-context exploration and self-correction.   <ref type="bibr" target="#b75">(Snell et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Using variable Compute</head><formula xml:id="formula_14">min 𝜃 E S 𝑖 ∼𝜋 ref (•|q),q∼𝒟 train [︀ -log 𝜋 𝜃 (S * , EOS|S 𝑗-1 , . . . , S 1 , q) ]︀ , 𝑗 ∼ Unif(1, 8)<label>(7)</label></formula><p>This formulation encourages the model to continue revising it's solution until it reaches a solution with high confidence of correctness. Interestingly, our model generates an increasing number of solutions based on question difficulty. Summary statistics by problem difficulty are shown in Figure <ref type="figure" target="#fig_7">11</ref> (right), where the model generates an average of 2.45 solutions for Level 1 problems and an average of 5.84 for Level 5 problems, consistent with the behavior shown in Figures <ref type="figure">1</ref> and<ref type="figure" target="#fig_5">8 (c</ref>). Specifically, this demonstrates that the model has internalized the need for extended exploration on complex reasoning tasks.</p><p>Our best performing run achieved an improvement of 2% over the LLaMa 3.1 8B Instruct model that we initialize our models from. We attribute this to a mismatch between the instruct model's RL post-training, the off-policy SFT fine-tuning we utilize, and the limited amount of training data in the MATH train dataset. Indeed, any regular SFT training we performed on the RL post-trained model actually worsened performance. We are currently exploring post-training on pre-RL base models with extended datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Backtracking in LLMs</head><p>In the prior sections, we reviewed evidence that auto-regressive models can internalize complex search strategies in simple domains. We also showed that LLMs can learn in-context exploration at the episode-level. However, whether models can implement complex search strategies (e.g. those outlined in Section 3) auto-regressively remains an open question in public research. Specifically, we refer to the ability to terminate a reasoning chain prior to completion, and the ability to reset (semantically) to an arbitrary previously visited state in-context. These two steps can be unified under the concept of backtracking. Here we will review some recent works demonstrating that LLMs can learn to backtrack.</p><p>Recent works have demonstrated that training on data with backtracking can improve language models on simple reasoning tasks <ref type="bibr">(Ye et al., 2024b;</ref><ref type="bibr" target="#b4">Anonymous, 2024)</ref> find that language models can sometimes "recognize" their errors internally, but do not have the required mechanisms to self-correct. Similar to our motivation with Meta-CoT, their aim is for a single model to both recognize errors and self-correct in-context. In their approach they create training data with incorrect steps directly followed by the correction. The backtracking is signified by a special token, <ref type="bibr">[BACK]</ref>, at the end of an incorrect step to allow the model to explicitly state it's belief that an error has occurred. That is: given a dataset 𝒟 train of questions q and correct reasoning CoT solutions S = s 1 , . . . , s 𝑛 the training objective becomes</p><formula xml:id="formula_15">ℒ backtrack (𝜃) = -E s 1 ,...,s𝑛∼𝒟 train ,𝑡∼Unif(1,𝑛) [︀ log 𝜋 𝜃 (s 1 , . . . , s - 𝑡 , [BACK], s 𝑡 , . . . , s 𝑛 |q) ]︀ (8)</formula><p>where 𝑡 is a randomly sampled time step in the solution and s - 𝑡 is a single incorrect reasoning step. This is in contrast to the standard approach, which only trains on the correct solution chains:</p><formula xml:id="formula_16">ℒ standard (𝜃) = -E S∼𝒟 train [log 𝜋 𝜃 (S|q)] .<label>(9)</label></formula><p>Ye et al. (2024b) explore inserting incorrect steps at varying rates (between 1% and 50%) and find that high rates of incorrect steps actually leads to improved downstream performance. In particular, they find that a 50% rate of incorrect steps (objective in Equation <ref type="formula">8</ref>) leads to an increase from 78% to 94% accuracy on hard math problems as compared to training on only correct solutions (Equation 9, CoT). While promising, these results are only verified on small models (124M parameters).</p><p>In contrast, <ref type="bibr">Zhang et al. (2024b)</ref> teach LLMs to backtrack based on safety considerations using the larger Gemma 2B and LLaMa 3 8B models. In particular, following the above notation, given a prompt q and two possible answers -a safe option S + = s + 1 , . . . , s + 𝑛 and an unsafe option S -= s - 1 , . . . , s - 𝑛 ′ , where s here represent individual tokens (unlike before where they stood for logical steps), they optimize the objective:</p><formula xml:id="formula_17">ℒ(𝜃) = -E (q,S + ,S -)∼𝒟 train ,𝑡∼Unif(1,𝑛 ′ ) [ log 𝜋 𝜃 ([BACK], S + |S - 𝑡 , q) + log 𝜋 𝜃 (S + |q)].<label>(10)</label></formula><p>That is a combination of the Meta-CoT and regular CoT objectives as outlined above. Additionally, notice that this objective masks out the unsafe completion, while the prior work trains on all tokens including the incorrect logical steps. While the approach of <ref type="bibr">Ye et al. (2024b)</ref> backtracks for a single logical step (correction) this work always resets the agent to the initial state. SFT training is successful in teaching the model to backtrack and improves the safety characteristics over supervised fine-tuning on just the safe answer (only the second term of Equation <ref type="formula" target="#formula_17">10</ref>). However, these effects appear weak in regular SFT models, but are significantly improved through further downstream RL training, which we will discuss later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Synthetic Meta-CoT Via Search</head><p>In the prior sections we argued for an approach to reasoning that teaches an LLM to internalize an auto-regressive search procedure in-context. We also reviewed several recent works showing that small auto-regressive models can carry out in-context exploration at the episode level, and larger models can learn individual step backtracking. In this section, we explore how to construct synthetic data for realistic Meta-CoT that involves full-scale in-context tree search.</p><p>Setup. For demonstrative purposes, we use the math problem presented by OpenAI (2024) as our benchmark task, where Gemini 1.5 Pro <ref type="bibr" target="#b61">(Reid et al., 2024)</ref>   <ref type="bibr" target="#b72">Silver et al. (2018)</ref>. Specifically, we estimate the value of a partial solution trajectory as</p><formula xml:id="formula_18">𝑣(S 𝑡 , q) = E S 𝑗 ≥𝑡+1 ∼𝜋 𝜃 (S ≥𝑡+1 |S𝑡,q) 1 𝐾 𝐾 ∑︁ 𝑗=1 𝑟 * ([S 𝑗 ≥𝑡+1 , S 𝑡 ], q) (11)</formula><p>where 𝑟 * is the verifiable ground-truth outcome reward. In our experiments, we sample 128 completions from the partial solution and evaluate the mean success rate under ground-truth outcome supervision. In Appendix E, the numerical values of the states are listed after each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Monte-Carlo Tree Search</head><p>Figure <ref type="figure" target="#fig_0">12</ref>: MCTS tree for the math problem presented by OpenAI (2024). The red node indicates the solution.</p><p>We conduct an example based on Monte-Carlo Tree Search (MCTS), which seeks to balance exploration and exploitation. The MCTS implementation of <ref type="bibr" target="#b72">Silver et al. (2018)</ref> has been widely applied to the reasoning domain <ref type="bibr" target="#b81">(Tian et al., 2024;</ref><ref type="bibr" target="#b12">Feng et al., 2024)</ref>, and we mostly follow their implementation with some modifications to account for the structure of our search problem (see Appendix D).</p><p>We present the search trace for our example problem -all the actions taken during the search (i.e., the Meta-CoT in a linear format) -in Appendix E. The numbers following each reasoning step represent the value estimates. In our initial MCTS attempt we obtained a trace with an excessive number of backtracks and repetitions, including from high-value states (as high as 1.0) with the resulting exploration tree is shown in Figure <ref type="figure" target="#fig_0">12</ref>. We believe these effects are due to the exploration bonus in MCTS search. We did not carry out extensive ablations on the search parameters due to speed and costs. Since we use pure MC rollouts ("simulations") for state value estimation, a single tree uses up to 20 million tokens inference (a cost of ∼$100 per tree). Moreover the process can take up to half an hour due to API limits. Because of these issues we also evaluate a more efficient best-first exploration strategy, which we present below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">A* search</head><p>We begin with an exploration of a type of best-first search based on the work by <ref type="bibr" target="#b32">Koh et al. (2024)</ref>, which itself loosely follows an A* approach. The search procedure maintains a frontier ℱ of states, which is implemented as a max priority queue. Similarly to the MCTS approach, each state S 𝑡 consists of the question q and a partial solution consisting of generated reasoning steps (s 1 , . . . , s 𝑡 ). At each iteration, the state S 𝑝 ← pop(ℱ) with the highest value 𝑣 𝑝 = 𝑣(S 𝑝 , q) is selected, where 𝑣 𝑝 ∈ [0, 1] is the value of the partial solution S 𝑝 including current and previous reasoning steps. At each node the policy 𝜋 𝜑 proposes 𝑏 candidate next steps, each of which is evaluated by 𝑣 and added to ℱ if the depth of the tree |(s 0 , . . . , s 𝑝 )| has not reached the maximum depth search limit 𝑑. For the purpose of generating synthetic data, we run the search until we find a solution that is correct using the ground-truth verifier. The resulting tree is shown in Figure <ref type="figure" target="#fig_8">13</ref>. It shows more consistent flow of the  <ref type="formula">2024</ref>). This trace presents more of a best-first approach with fewer backtracks, concentrated around key steps, as compared to the one produced by MCTS in Figure <ref type="figure" target="#fig_0">12</ref>.</p><p>reasoning steps, with less backtracking concentrated around a few key steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Do Advanced Reasoning Systems Implement In-Context Search?</head><p>In this section we will investigate whether advanced reasoning systems, such as OpenAI's O1 (OpenAI, 2024), DeepSeek R1 (DeepSeek, 2024) and Gemini 2.0 Flash Thinking Mode<ref type="foot" target="#foot_1">foot_1</ref> and the Qwen QwQ Team (2024) implement in-context search. We provide successful reasoning traces for the same math problem in Appendix E.</p><p>Starting with OpenAI's o1 model, by carefully examining the provided mathematical reasoning trace, we observe:</p><p>1. Inconsistent flow of thought -consecutive steps do not logically continue the prior state. 2. Backtracking -the model carries out "semantic backtracking" -frequently returning to the same logical points. 3. Repetition -the model often repeats logical steps.</p><p>The qualitative behaviors observed in o1 (Figure <ref type="figure" target="#fig_11">14</ref> left) are similar to those in the example synthetic trace (Figure <ref type="figure" target="#fig_13">15</ref>) generated by Gemini 1.5 with and MCTS-like search processes. In particular, there are abrupt changes in logical flow of the (Meta) CoT, which is natural as the model backtracks between branches of the tree. Moreover, the model may explore multiple child nodes of the same parent which are different strings, but can also be very semantically similar leading to repetitive logic. This is clear in the provided trace, as the model repeats logical statement and goes over the same derivations multiple times. Note also that we do not claim the model is implementing tree search at test time, but rather that as much as the model's output are expected to resemble it's training data, we hypothesize that examples of search were used during training (likely model initialization). We will specifically address the need and effects of RL training in Section 6.</p><p>The DeepSeek R1 model DeepSeek (2024) also exhibits similar behaviors, as shown in Figure <ref type="figure" target="#fig_11">14</ref>, however, it also carries out a significant amount of self-evaluation steps. This could be achieved by integrating a form of self-criticism <ref type="bibr" target="#b41">(Madaan et al., 2023;</ref><ref type="bibr" target="#b70">Shinn et al., 2023)</ref> or a generative verifier <ref type="bibr">(Zhang et al., 2024a)</ref> in the search trace. The LATS framework <ref type="bibr">(Zhou et al., 2024a</ref>) uses a similar approach, combining MCTS search with self-criticism and shows empirical improvements from self-reflection. Another alternative for synthetic data generation is the "Iteration-Of-Thought" approach Radha et al. ( <ref type="formula">2024</ref>) which also interleaves generation with inner dialogue. This would explain the rather smooth logical flow of the R1 model, which does not exhibit as much abrupt back-tracking, as compared to O1. As mentioned earlier, in order to adequately model the search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verification Steps</head><p>Let n be an even positive integer. Let p be a monic, real polynomial of degree 2n; that is to say, 𝑝 𝑥 = 𝑥 !" + 𝑎 !"#$ 𝑥 !"#$ + ⋯ + 𝑎 $ 𝑥 + 𝑎 % for some real coefficients 𝑎 % , … , 𝑎 !"#$ . Suppose that 𝑝 </p><formula xml:id="formula_19">𝑝 𝑥 = 𝑥 !" + 𝑎!"#$𝑥 !"#$ + ⋯ + 𝑎$𝑥 + 𝑎% for some real coefficients 𝑎%, 𝑎$, … , 𝑎!"#$. ⋯ therefore, 𝑥 !" 𝑝( $ &amp;</formula><p>) is a polynomial of degree 2𝑛, and 𝑥 !"'! is a monomial of degree 2𝑛 + 2. so, their difference</p><formula xml:id="formula_20">𝑥 !" 𝑞 𝑥 = 𝑥 !" 𝑝 $ &amp; -𝑥 !"'! is a polynomial of degree 2𝑛 + 2. wait, but 𝑝(𝑥) is of degree 2𝑛, so 𝑝 $ &amp; is $ &amp;!" + 𝑎!"#$ $ &amp;!"#$ + ⋯ + 𝑎$ $ &amp;</formula><p>+ 𝑎%, and multiplying by 𝑥 !" gives 1 + 𝑎!"#$𝑥 + ⋯ + 𝑎$𝑥 !"#$ + 𝑎%𝑥 !" , which is indeed a degree 2𝑛 polynomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>this is a polynomial of degree 2𝑛 + 2, assuming 𝑎% is not zero, which it might be, but since 𝑝(𝑥) is monic, 𝑎% could be anything. wait, actually, since 𝑝(𝑥) is monic of degree 2𝑛, the leading term</p><formula xml:id="formula_21">is 𝑥 !" , so 𝑝 $ &amp; = 𝑥 #!" + ⋯ + 𝑎%, and 𝑥 !"'! 𝑝 $ &amp; = 1 + 𝑎!"#$𝑥 + ⋯ + 𝑎%𝑥 !" , which is indeed a degree 2𝑛 polynomial. so, 𝑥 !" 𝑞 𝑥 = 𝑥 !" 𝑝 $ &amp; -𝑥 !"'! = 1 + 𝑎!"#$𝑥 + ⋯ + 𝑎%𝑥 !" - 𝑥 !"'! , which is a degree 2𝑛 + 2 polynomial if 𝑥 !"'! is the highest degree term. ⋯ we know that 𝑞 𝑘 = 0 for 𝑘 = ±1, ±2, ⋯ , ±𝑛, because 𝑝 $ ( = 𝑘 ! , so 𝑞 𝑘 = 𝑘 !" 𝑝 $ ( -𝑘 !"'! = 𝑘 !" 5 𝑘 ! -𝑘 !"'! = 𝑘 !"'! - 𝑘 !"'! = 0. therefore, 𝑞 𝑥 has 2𝑛 roots at 𝑥 = ±1, ±2, ⋯ , +𝑛.</formula><p>since 𝑞 𝑥 is a degree 2𝑛 + 2 polynomial, there are two more roots beyond 2𝑛 roots.</p><p>for some constant 𝑎, because the roots come in pairs 𝑥 = 𝑘 and 𝑥 = -𝑘, and possibly another pair 𝑥 = 𝑎 and 𝑥 = -𝑎.</p><p>wait, but 𝑞 𝑥 is degree 2𝑛 + 2, and (𝑥 ! -𝑘 ! ) for 𝑘 = 1 to 𝑛 gives 𝑛 quadratic factors, which is degree 2𝑛. so, there are two more roots to account for. alternatively, perhaps 𝑞 𝑥 = (𝑥 ! -1)(𝑥 ! -4) ⋯ (𝑥 ! -𝑛 ! ) 5 (𝑥 ! -𝑎)(𝑥 ! -𝑏), where 𝑎 and 𝑏 are the remaining roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>wait, but in the factorization, I have</p><formula xml:id="formula_22">𝑥 !" -$ ("!)! , which gives 𝑥 = ± $ "!</formula><p>, assuming that's correct. but i need to verify this conclusion.</p><p>given that 𝑎 = $ ("!)!</p><p>, and the factorization is</p><formula xml:id="formula_23">𝑟 𝑥 -𝑥 !"'! = -𝑥 ! -1 𝑥 ! -4 ⋯ 𝑥 ! -𝑛 ! 𝑥 ! -$ "! ! , that indeed, the roots are 𝑥 = ±1, ±2, … , ±𝑛, ± $ "!</formula><p>. therefore, the additional solutions beyond 𝑥 = ±1, ±2, … , ±𝑛 are</p><formula xml:id="formula_24">± $ "!</formula><p>. but wait, is 𝑛! correct here? let me double-check the calculation of 𝑎.</p><p>earlier, i had:</p><formula xml:id="formula_25">𝑎 = 1 -1 " (1 5 4 5 9 ⋯ 𝑛 ! ) = 1 (𝑛!) ! since 𝑛 is even, (-1) " = 1, so indeed, 𝑎 = $ ("!)! . therefore, the additional roots are 𝑥 = ± $ "! hence, the other real numbers 𝑥 that satisfy 𝑝 $ &amp; = 𝑥 ! are 𝑥 = ± $ "!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O1</head><p>We are told that for all integer values of 𝑘 satisfying 1 ≤ 𝑘 ≤ 𝑛 𝑝 $ ( = 𝑘 ! Our problem is: Given that 𝑝 is monic of degree 2𝑛, and that</p><formula xml:id="formula_26">𝑝 $ ( = 𝑘 ! for 𝑘 = ±1, ±2, … , ±𝑛, find all other real numbers 𝑥 such that 𝑝 $ &amp; = 𝑥 ! .</formula><p>In other words, we are to find all real numbers 𝑥 such that</p><formula xml:id="formula_27">𝑝 $ &amp; = 𝑥 ! . ⋯ Our equation is 𝑝 $ &amp; -𝑥 ! = 0. Let me define q(x) = 𝑝 $ &amp; -𝑥 ! . ⋯ Moreover, since 𝑝 is a polynomial of degree 2𝑛, then𝑝 $ &amp; is a rational function of 𝑥 of degree 2𝑛 when considered as a function of 𝑥. But we can consider q(x) = 𝑝 $ &amp; -𝑥 ! . Alternatively, let's consider s x = 𝑥 !" 𝑝 $ &amp; -𝑥 !"'! . ⋯ Let me consider that 𝑠 𝑥 = 𝑥 !" 𝑝 $ &amp; -𝑥 ! = 𝑥 !" 𝑞(𝑥).</formula><p>So 𝑠 𝑥 = 𝑥 !" 𝑞(𝑥).</p><p>Since 𝑞 𝑘 = 0 for 𝑘 = ±1, ±2, … , ±𝑛, we have 𝑞 𝑘 = 0. ⋯ But 𝑡 𝑥 = 𝑝 * (𝑥), and 𝑠 𝑥 = 𝑡 𝑥 -𝑥 !"'! .</p><p>But our key is that 𝑠 𝑥 = 𝑥 !" 𝑞(𝑥).</p><p>Recall that 𝑞 𝑘 = 0 for 𝑘 = ±1, ±2, …, ±𝑛. ⋯ Thus, 𝑠 𝑥 must be divisible by the monic polynomial whose roots are ±1, ±2, … , ±𝑛.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus, 𝑠 𝑥 = 𝑚 𝑥 ℎ(𝑥).</head><p>Where</p><formula xml:id="formula_28">𝑚 𝑥 = 𝑥 --𝑛 𝑥 --𝑛 + 1 … 𝑥 --1 D𝑥 - 1 E … (𝑥 -𝑛). ⋯ Therefore, ℎ(𝑥) is a quadratic polynomial.</formula><p>Our plan is to write 𝑠 𝑥 = 𝑚 𝑥 ℎ(𝑥).</p><p>Given that 𝑠 𝑥 is degree 2𝑛 + 2 with leading coefficient -1, ⋯ Similarly, 𝑚 𝑥 is a degree 2𝑛 monic polynomial.</p><p>Therefore, ℎ(𝑥) must be a degree 2 polynomial with leading coefficient -1, Alternatively, since 𝑠 𝑥 = 𝑚 𝑥 ℎ(𝑥),</p><p>We can write 𝑠 𝑥 = -1 𝑥 !"'! + 𝑙𝑜𝑤𝑒𝑟 𝑑𝑒𝑔𝑟𝑒𝑒 𝑡𝑒𝑟𝑚𝑠 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>So we have:</p><formula xml:id="formula_29">𝑠 𝑥 = 𝑚 𝑥 ℎ 𝑥 = [∏ (𝑥 ! -𝑘 ! )](-𝑥 ! + 𝑏𝑥 + 𝑐) " (-$ and 𝑠 𝑥 = 𝑥 !" 𝑝 $ &amp; -𝑥 !"'! . Alternative, let's consider that 𝑡 𝑥 = 𝑥 !" 𝑝 $ &amp; = 𝑝 * (𝑥). Then 𝑠 𝑥 = 𝑡 𝑥 -𝑥 !"'! = 𝑝 * 𝑥 -𝑥 !"'! . ⋯ So, the coefficient of 𝑥 !" in 𝑠(𝑥) is 𝑠!" = 𝑐. But 𝑠 𝑥 = 𝑝 * 𝑥 -𝑥 !"'! . So 𝑠 𝑥 = [𝑝 * (𝑥)] -𝑥 !"'! . But 𝑝 * 𝑥 has degree ≤ 2𝑛. ⋯ Thus, 𝑐 = $ ("!)! Thus 𝑥 ! = 𝑐 = 0 ⟹ 𝑥 = ± $ "! Thus, the other real numbers are 𝑥 = ± $ "! Answer: The other real numbers 𝑥 satisfying 𝑝 1 𝑥 ⁄ = 𝑥 ! are 𝑥 = ± $ "! Gemini 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">Flash Thinking</head><p>The problem asks for the real numbers 𝑥 other than</p><formula xml:id="formula_30">± 1, ± $ ! , … , ± $ " such that 𝑝 $ &amp; = 𝑥 ! .</formula><p>We are given that 𝑝(𝑥) is a monic polynomial of degree 2𝑛.</p><p>The condition 𝑝 $ ( = 𝑘 ! holds for integers 𝑘 such that 1 ≤ 𝑘 ≤ 𝑛.</p><p>This means the condition holds for 𝑘 = 1,2, … , 𝑛 and 𝑘 = -1, -2, … , -𝑛.</p><formula xml:id="formula_31">The value of $ ( are ±1, ± $ ! , … , ± $ " . ⋯ So, 𝑔 𝑥 = -𝑥 ! -1 𝑥 ! -4 … 𝑥 ! -𝑛 ! 𝑄(𝑥).</formula><p>The degree of</p><formula xml:id="formula_32">𝑥 ! -1 𝑥 ! -4 … 𝑥 ! -𝑛 ! is 𝑛. The degree of 𝑔 𝑥 is 2𝑛 + 2.</formula><p>Therefore, the degree of</p><formula xml:id="formula_33">𝑄(𝑥) is 2𝑛 + 2 -𝑛 = 𝑛 + 2.</formula><p>Let's consider the expression of 𝑔 𝑥 in more detail.</p><formula xml:id="formula_34">𝑔 𝑥 = -𝑥 !"'! + 𝑎%𝑥 !" + 𝑎$𝑥 !"#$ + ⋯ + 𝑎!"#$𝑥 + 1.</formula><p>We know that 𝑔 𝑘 = 0 for 𝑘 = ±1, ±2, … , ±𝑛.</p><formula xml:id="formula_35">Consider the polynomial ℎ 𝑥 = 1 + 𝑎!"#$𝑥 + ⋯ + 𝑎%𝑥 !" -𝑥 !"'! .</formula><p>The roots of ℎ(𝑥) are ±1, ±2, … , ±𝑛.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>Let's recheck the degree of the factors.</p><p>𝑥 ! -1 has degree 2.</p><p>The product has degree 2𝑛.</p><p>The leading term of ℎ(𝑥) is a form -𝑥 !"'! .</p><p>The leading term of 𝐶 𝑅(𝑥) is 𝐶 𝑥 !" .</p><p>This doesn't seem right.</p><p>Let's go back to the definition of 𝑔 𝑥 .</p><formula xml:id="formula_36">𝑔 𝑥 = -𝑥 !"'! + 𝑎%𝑥 !" + 𝑎$𝑥 !"#$ + ⋯ + 𝑎!"#$𝑥 + 1.</formula><p>We have 𝑔 𝑘 = 0 for 𝑘 = ±1, … , ±𝑛.</p><p>Consider the polynomial 𝑠 𝑥 =</p><formula xml:id="formula_37">.(&amp;) ∏ &amp;!#(! " %&amp;$ . ⋯ The values ± $ "! are distinct from ±1, ±2, … , ±𝑛.</formula><p>The real numbers 𝑥 for which</p><formula xml:id="formula_38">𝑝 $ &amp; = 𝑥 ! are ±1, ±2, … , ±𝑛, $ "! , -$ "!</formula><p>.</p><p>The question asks for the other real numbers, which are $ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>The roots of 𝑞(𝑦) are ±1, ±2, … , ±𝑛.</p><formula xml:id="formula_39">So, 𝑞 𝑦 = C y -1 y + 1 y -2 y + 2 … (y -n)(y + n) 𝑞 𝑦 = 𝐶 𝑦 ! -1 𝑦 ! -4 … (𝑦 ! -𝑛 ! ).</formula><p>The leading term of 𝑞 𝑦 is 𝑦 !"'! .</p><p>The leading term of</p><formula xml:id="formula_40">𝐶 𝑦 ! -1 … (𝑦 ! -𝑛 ! ) is 𝐶 𝑦 !" .</formula><p>This does not match.</p><p>Let's re-read the problem carefully.</p><p>We are given conditions on 𝑝 $ &amp; .</p><p>We are asked for real numbers 𝑥 such that</p><formula xml:id="formula_41">𝑝 $ &amp; = 𝑥 ! . Consider the polynomial 𝑟 𝑥 = 𝑥 ! 𝑝 $ &amp; -1.</formula><p>The roots of 𝑟(𝑥) are the values of 𝑥 for which</p><formula xml:id="formula_42">𝑝 $ &amp; = $ &amp;! . The given condition is 𝑝 $ ( = 𝑘 ! .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>The values for which the condition is given are $ ( where 1 ≪ 𝑘 ≪ 𝑛.</p><formula xml:id="formula_43">These are ±1, ± $ ! , … , ± $ " .</formula><p>The question asks for "all other real numbers".</p><p>The roots of 𝑔(𝑥) are the values of 𝑥 for which the equality holds.</p><p>The values ±1, ±2, … , ±𝑛 correspond to the given conditions.</p><p>The other real numbers are $ "! and -$ "!</p><p>. Let n be an even positive integer. Let p be a monic, real polynomial of degree 2n; that is to say, 𝑝 𝑥 = 𝑥 !" + 𝑎 !"#$ 𝑥 !"#$ + ⋯ + 𝑎 $ 𝑥 + 𝑎 % for some real coefficients 𝑎 % , … , 𝑎 !"#$ . Suppose that 𝑝 $ &amp; = 𝑘 ! for all integers k such that 1 ≤ 𝑘 ≤ 𝑛. Find all other real numbers xx for which 𝑝 $ '</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= 𝑥 !</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backtracking Steps</head><p>A* (Gemini 1.5)</p><formula xml:id="formula_44">Let 𝑞 𝑥 = 𝑥 ! 𝑝 " # = 𝑎$𝑥 !% + 𝑎"𝑥 !%&amp;" + ⋯ + 𝑎!%&amp;"𝑥 + 1.</formula><p>The given condition 𝑝</p><formula xml:id="formula_45">" ' = 𝑘 ! implies 𝑞 𝑘 = 𝑘 !% 𝑘 ! = 𝑘 !%(! for 1 ≤ |𝑘| ≤ 𝑛</formula><p>Let 𝑝 𝑥 = 𝑥 !% + 𝑎!%&amp;"𝑥 !%&amp;" + ⋯ + 𝑎"𝑥 + 𝑎$ be a monic polynomial of degree 2𝑛, where 𝑛 is an even positive integer.</p><p>We are given that 𝑝 " ' = 𝑘 ! for all integers 𝑘 such that 1 ≤ 𝑘 ≤ 𝑛.</p><p>We are given that 𝑝</p><formula xml:id="formula_46">" ' = 𝑘 ! for integers 𝑘 such that 1 ≤ 𝑘 ≤ 𝑛.</formula><p>The condition 𝑝</p><formula xml:id="formula_47">" ' = 𝑘 ! for 1 ≤ 𝑘 ≤ 𝑛 is equivalent to 𝑞 𝑘 = 𝑘 !% 𝑘 ! = 𝑘 !%(! for 1 ≤ 𝑘 ≤ 𝑛.</formula><p>Consider the polynomial 𝑟 𝑥 = 𝑥 !%(! -𝑞(𝑥). We are given that 𝑟 𝑘 = 0 for 1 ≤ 𝑘 ≤ 𝑛.</p><p>Since 𝑛 is even, let 𝑛 = 2𝑚. Then we have 2𝑛 = 4𝑚. Thus, 𝑟 𝑘 has the roots ±1, ±2, … , ±2𝑚.</p><formula xml:id="formula_48">𝑟 𝑥 = 𝑥 -1 𝑥 + 1 𝑥 -2 𝑥 + 2 … 𝑥 -2𝑚 𝑥 + 2𝑚 = ∏ 𝑥 -𝑘 𝑥 + 𝑘 = !) '*" ∏ 𝑥 ! -𝑘 ! !) '*" . Since 𝑞 𝑥 is a polynomial of degree 2𝑛, 𝑟 𝑥 is a monic polynomial of degree 2𝑛 + 2 = 4𝑚 + 2.</formula><p>Therefore, we can write</p><formula xml:id="formula_49">𝑟 𝑥 = 𝑥 ! -1 𝑥 ! -2 ! … 𝑥 ! -2𝑚 ! = 𝑥 ! -1 (𝑥 ! - 4) … 𝑥 ! -4𝑚 ! Therefore, we can write 𝑟 𝑥 = 𝑥 !%(! -𝑞 𝑥 = 𝑥 ! -1 (𝑥 ! - 4) … 𝑥 ! -𝑛 ! 𝑅(𝑥), where 𝑅(𝑥) is a polynomial of degree 2𝑛 + 2 -2𝑛 = 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>We are looking for values of 𝑥 such that 𝑝 " # = 𝑥 ! , which is equivalent to 𝑞 𝑥 = 𝑥 !%(! . This means we want to find the roots of 𝑟 𝑥 = 0. We know that 𝑟 𝑥 = 0 for 𝑥 = ±𝑘 for 1 ≤ 𝑘 ≤ 𝑛. We are looking for the roots of 𝑥 ! + 𝑏𝑥 + 𝑐 = 0.</p><formula xml:id="formula_50">𝑟 𝑥 = (𝑥 ! -1)(𝑥 ! -4) ⋯ (𝑥 ! -𝑛 ! )(𝑥 ! + 𝑏𝑥 - " %! ! ).</formula><p>Since the coefficients of 𝑝(𝑥) are real, the coefficients of 𝑞(𝑥) are real, and so the coefficients of r(𝑥) are real. Therefore, the coefficients of R(𝑥) are real. In particular, 𝑏 is real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The roots of 𝑥</head><formula xml:id="formula_51">! + 𝑏𝑥 - " %! ! = 0 are 𝑥 = &amp;,± , ! ( " #!! ! . Thus the roots are real if 𝑏 ! + . %! ! ≥ 0, which is always true because 𝑏 is real.</formula><p>Consider 𝑟 𝑥 = 𝑥 !%(! -𝑞(𝑥).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The equation 𝑝</head><formula xml:id="formula_52">" # = 𝑥 ! implies 𝑥 !% 𝑝 " # = 𝑥 !%(! , so q 𝑥 = 𝑥 !%(! , thus 𝑥 !%(! -𝑞(𝑥) = 0. We already know that 𝑥 = ± 𝑘, 1 ≤ 𝑘 ≤ 𝑛, are roots. We have 𝑟 𝑥 = (𝑥 ! -1)(𝑥 ! -4) ⋯ (𝑥 ! -𝑛 ! )(𝑥 ! + 𝑏𝑥 - " %! ! ). Since 𝑟(𝑥) is an even function, 𝑏 = 0.</formula><p>The other solutions are roots of 𝑥 ! -</p><formula xml:id="formula_53">" %! ! = 0, so 𝑥 = ± " %! . Final Answer: The final answer is ± " %! MCTS (Gemini 1.5) Let 𝑞 𝑥 = 𝑝 " # -𝑥 ! . Let 𝑞 𝑥 = 𝑝 " # -𝑥 ! . Then 𝑞(𝑥) is a rational function of the form 𝑞 𝑥 = "(/!#%&amp;#(⋯(/&amp;# !#%&amp; (/'# !# &amp;# !#(! # !# # !# .</formula><p>We are given that 𝑝(𝑥) is a monic real polynomial of degree 2𝑛.</p><formula xml:id="formula_54">𝑞 𝑥 = "(/!#%&amp;#(⋯(/&amp;# !#%&amp; (/'# !# &amp;# !#(! # !# # !# . 𝑞 𝑥 = "(/!#%&amp;#(⋯(/&amp;# !#%&amp; (/'# !# &amp;# !#(! # !# = 1(#) # !# . ⋯ Let 𝑞 𝑥 = 𝑥 !% 𝑝( " # ). Then 𝑞 𝑥 = 1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !% .</formula><p>Thus, 𝑟(𝑥) is a polynomial of degree 2𝑛 + 2 with roots 𝑘 for 1 ≤ 𝑘 ≤ 𝑛. Since there are 2𝑛 such values of 𝑘, we can write We are given that 𝑝(𝑥) is a monic real polynomial of degree 2𝑛.</p><p>Since 𝑝 " ' = 𝑘 ! for all integers 𝑘 such that 1 ≤ 𝑘 ≤ 𝑛, we have</p><formula xml:id="formula_55">𝑞 𝑘 = 𝑝 " ' -𝑘 ! = 0 for 1 ≤ 𝑘 ≤ 𝑛. ⋯ We also know that the coefficient of 𝑥 !%(" is 𝑟(𝑥) is 0. The coefficient of 𝑥 !%(" in ∏ (𝑥 ! -𝑘 ! )(-𝑥 ! + 𝐵𝑥 + 𝐶) % '*" is 𝐵. Thus 𝐵 = 0. So, 𝐵( 2𝑚 !) ! = 0, which implies 𝐵 = 0. We have 𝑟 𝑥 = ∏ (𝑥 ! -𝑘 ! )(-𝑥 ! + 𝐵𝑥 + 𝐶) % '*" . So we have 𝑟 𝑥 = ∏ (𝑥 ! -𝑘 ! )(-𝑥 ! + &amp;" # %! ! ) % '*" . Thus, 𝐵( 2𝑚 !) ! = 0, which means 𝐵 = 0. Therefore, Since 𝑛 is even, 𝐶 = " (%!) ! . Also, 𝐴 = -1. So,</formula><p>The constant term of 𝑟 𝑥 is 1. Also, the constant term is given by ∏ -𝑘 𝑘 … 𝐶 = -1 % 𝑛! ! 𝐶.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% '*"</head><p>Thus, (-1) % (𝑛!) ! 𝐶 = 1,</p><formula xml:id="formula_56">which implies 𝐶 = " (&amp;") # (%!) ! . So, we have 𝑟 𝑥 = ∏ 𝑥 ! -𝑘 ! -𝑥 ! + &amp;" # %! ! . % '*" ⋯ Thus, we have 𝑟 𝑥 = ∏ (𝑥 ! -𝑘 ! )(-𝑥 ! + % '*" (&amp;") # (%!) ! ). Since 𝑛 is even, let 𝑛 = 2𝑚.</formula><p>We have</p><formula xml:id="formula_57">𝑟 𝑥 = ∏ (𝑛 ! -𝑘 ! )(-𝑥 ! + (&amp;") # (%!) ! % '*"</formula><p>. The roots of 𝑟(𝑥)</p><formula xml:id="formula_58">other than ±1, ±2, … , ±𝑛 satisfy -𝑥 ! + &amp;" # %! ! = 0, so 𝑥 ! = (&amp;") # (%!) ! . 𝑟 𝑥 = ∏ 𝑥 ! -𝑘 ! -𝑥 ! + " %! ! = -𝑥 ! % '*" ∏ (𝑥 ! - % '*" 𝑘 ! ) + " (%!) ! ∏ 𝑥 ! -𝑘 ! % '*" . So, 𝑟 𝑥 = ∏ (𝑥 ! -𝑘 ! )(-𝑥 ! + (&amp;") # (%!) ! % '*"</formula><p>. We are looking for the roots of 𝑟 𝑥 = 0, other than ±1, ±2, … , ±𝑛. These must be the</p><formula xml:id="formula_59">roots of -𝑥 ! + &amp;" # %! ! = 0, which gives us 𝑥 ! = (&amp;") # (%!) ! . Since 𝑛 is an even integer, we have 𝑥 ! = " (%!) ! , so 𝑥 = ± " %!</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qwen QwQ</head><p>Let's tackle this problem step by step. So, we have an even positive integer 𝑛, and a monic real polynomial 𝑝 of degree 2𝑛. That means</p><formula xml:id="formula_60">𝑝 𝑥 = 𝑥 !% + 𝑎!%&amp;"𝑥 !%&amp;"</formula><p>, where all the coefficients are real numbers.</p><p>The problem states that 𝑝 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>But 𝑝 " # is a rational function, not a polynomial. To make it easier to handle, let's multiply both sides by 𝑥 !% to eliminate the denominator:</p><formula xml:id="formula_61">𝑥 !% 𝑝 1 𝑥 -𝑥 !% 𝑥 ! Now, 𝑝 " # = ( " # ) !% +𝑎!%&amp;"( " # ) !%&amp;" + ⋯ + 𝑎" " #</formula><p>+ 𝑎$, so multiplying by 𝑥 !% gives:</p><formula xml:id="formula_62">1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !% -𝑥 !%(! = 0 Let's denote this polynomial as 𝑞 𝑥 = 1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !% -𝑥 !%(! .</formula><p>So, the roots of q 𝑥 = 0 corresponds to the values of 𝑥 where</p><formula xml:id="formula_63">𝑝 " # = 𝑥 ! .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>As we did earlier, multiplying both sides by 𝑥 !% gives us</p><formula xml:id="formula_64">𝑥 !% 𝑝 " # - 𝑥 !%(! = 0. But 𝑝 " # = ( " # ) !% +𝑎!%&amp;"( " # ) !%&amp;" + ⋯ + 𝑎" " #</formula><p>+ 𝑎$, so:</p><formula xml:id="formula_65">𝑥 !% 𝑝 1 𝑥 = ( 1 𝑥 ) !% +𝑎!%&amp;"( 1 𝑥 ) !%&amp;" + ⋯ + 𝑎" 1 𝑥 + 𝑎$ = 1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !%</formula><p>Therefore, the equation becomes:</p><formula xml:id="formula_66">1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !% = 0</formula><p>As we defined earlier, 𝑞 𝑥 = 1 + 𝑎!%&amp;"𝑥 + 𝑎!%&amp;!𝑥 ! + ⋯ + 𝑎"𝑥 !%&amp;" + 𝑎$𝑥 !% -𝑥 !%(!</p><p>We know that 𝑞(𝑥) has roots at 𝑥 = ±1, ±2, … , ±𝑛, which are 2𝑛 roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯</head><p>However, the problem specifies to find all other real numbers 𝑥 for which 𝑝 process the model must internalize an evaluation mechanism. However, providing an explicit CoT verification may be able to expand the model computational capacity and improve self-verification. This is an empirical question, which is currently unclear in open research.</p><p>Gemini 2.0 Flash Thinking Mode appears to implement a somewhat different structure. Specifically, the flow of reasoning qualitatively appears smoother with fewer logically inconsistent steps. Moreover, it backtracks less frequently and often returns to the initial state. In fact in the provided example the model solves the problem correctly and then fully re-generates a new solution from scratch (backtracks from the final state to the initial one). It's behavior seems to be to attempt a full solution, which may be terminated early based on some search heuristic. In cases where the solution attempt is unsuccessful, the model attempts a different solution approach, rather than branch at the step-level in a tree search structure. This seems more consistent with a revision-based strategy as reflected in past works <ref type="bibr" target="#b58">(Qu et al., 2024;</ref><ref type="bibr" target="#b4">Anonymous, 2024;</ref><ref type="bibr" target="#b33">Kumar et al., 2024)</ref>. The Qwen QwQ model <ref type="bibr" target="#b80">Team (2024)</ref> shows similar behavior, generating multiple solutions in-context, as also pointed out by <ref type="bibr" target="#b8">Chen et al. (2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Process Supervision</head><p>A key component of the search approaches presented in prior sections is the evaluation function 𝑣(q, S 𝑡 ), which scores intermediate states in a reasoning chain. These evaluation functions have become widely known as Process Reward Models (PRM). By incorporating process supervision, the search mechanism gains the flexibility to backtrack to earlier promising states when suboptimal paths are encountered, thereby enabling more effective exploration. However, the question of how to efficiently access such capabilities remains an open question. In Section 4.3 we showed examples of using outcome-based verification with MCTS in combination with Monte-Carlo rollouts. However, this approach can only be used during training due to the necessity for ground-truth answers, and moreover it is extremely computationally inefficient. As mentioned earlier, a single training example requires up to 20 million inference tokens, costing up to hundreds of dollars. It is significantly more efficient to amortize the evaluation procedure into a single parameterized model, and we will outline strategies for building such process guidance models below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learning Process Reward Models</head><p>Parameterized PRMs are built on top of pre-trained models, either using a linear head or the logits of specific tokens. The model takes the question q and a partial solution S 𝑡 as input and outputs a single scalar value 𝑣 𝜃 (q, S 𝑡 ) → [0, 1]. Given a dataset 𝒟 train of partial solutions S 𝑡 and corresponding value targets 𝑦 S𝑡 the model is generally optimized with a standard cross-entropy classification loss. A central question for training PRMs is: where do the supervision labels 𝑦 S𝑡 come from? One approach is to have human annotators provide step-by-step level evaluation of reasoning problems, as done by <ref type="bibr" target="#b39">Lightman et al. (2023)</ref>. While their work showed promise in terms of empirical results, this method is challenging to scale due to the high annotation time and cost, especially as evaluating hard reasoning problems requires high-caliber experts. An alternative approach presented by <ref type="bibr" target="#b83">Wang et al. (2024)</ref> only relies on access to outcome verification -i.e. problems with a ground truth answer. The proposed approach is to amortize the Monte Carlo state-value estimation into a parameterized function. Essentially, this method fits an empirical value function of the reference rollout policy where the targets 𝑦 S𝑡 are represented by Equation <ref type="formula">11</ref>. This idea has been widely adopted in follow-up works <ref type="bibr" target="#b75">(Snell et al., 2024;</ref><ref type="bibr" target="#b4">Anonymous, 2024)</ref> and further extended <ref type="bibr">(Setlur et al., 2024c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">PRM Quality And Its Effect On Search</head><p>The performance and efficiency of search at test-time depends on the quality of the PRM <ref type="bibr">(Setlur et al., 2024b;</ref><ref type="bibr" target="#b4">Anonymous, 2024)</ref>. <ref type="bibr">Setlur et al. (2024b)</ref> demonstrate effective scaling (in both training data size and label quality) of a specific variant of PRMs that estimate values based on the improvement in likelihood of the correct answer after a step. The accuracy of test-time search improves log-linearly with training data size, and the quality of learned value labels improve with more Monte Carlo estimates. <ref type="bibr" target="#b4">Anonymous (2024)</ref> show that oracle verifier-enabled search is orders of magnitude more efficient than a learned PRM with noisy value estimates.</p><p>In this section we conduct an experiment demonstrating the scaling characteristics of a PRM. To train our PRM, we first need to generate diverse solution trajectories where each solution step is annotated with a ground truth value. To do so, we use the method from <ref type="bibr" target="#b83">Wang et al. (2024)</ref> to obtain ground truth values, performing 16 Monte Carlo (MC) rollouts for each step of a seed solution. We generate the seed solutions and step-level MC rollouts from a supervised finetuned (SFT) Llama3.1-8B using the PRM800K <ref type="bibr" target="#b39">(Lightman et al., 2023)</ref> dataset. The PRM training data uses 7,086 unique questions -each with seed solutions -and after removing duplicate seed solutions results in 97,000 trajectories in the training data. To evaluate the scaling performance with increasing data, we split the small set of data into three subsets: one with 500 unique questions, one with 3,000 unique questions, and one with all 7,086 unique questions. We create an evaluation set using the MATH-500 dataset <ref type="bibr" target="#b22">(Hendrycks et al., 2021;</ref><ref type="bibr" target="#b39">Lightman et al., 2023</ref>) by generating step-by-step solutions from the SFTed model and step-level ground truth values from 128 MC rollouts.</p><p>With this trained PRM, we find a reduction in the absolute error of predicted values when comparing PRMs that are trained across datasets of different sizes, as well as a selection of intermediate checkpoints in Figure <ref type="figure" target="#fig_14">16</ref>. We observe that: 1) the prediction error decreases as the size of the training data increases, and 2) when the size of the dataset is small, improvement converges early during training (around 30% of an epoch for Qs=500 and Qs=3000). Although these findings are based on small-scale experiments, we anticipate continued improvement in prediction errors with larger datasets and more extensive training, suggesting significant potential in further refining and scaling PRMs. Additionally, we evaluate the performance of the three fully-trained PRMs as outcome verifiers when performing a Best-of-N search during inference time. Figure <ref type="figure" target="#fig_15">17</ref> left shows that the PRM's ability to verify full solutions improves as they are trained with more data, yet there exists a remarkable gap between the trained PRMs and an oracle PRM. Additionally, we observe that the PRM's ability to guide the search process towards the correct answer with a more efficient path also improves as the increased accuracy and reduced number of tokens used in the search process are both observed in Figure <ref type="figure" target="#fig_15">17</ref> right. One interesting remaining question is: what is the scaling law for these process supervision models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Verifiable Versus Open-Ended Problems</head><p>Training a value function with MC rollouts is scalable with infrastructure and inference, but is fundamentally limited to problems with verifiable solutions. This excludes proof problems and scientific derivations which are often more important than the numerical answer itself. While automated proof assistance is an established area of research in mathematics (mathlib Community, 2020), this is rather limiting. First of all, these methods are largely limited to math and do not transfer to other domains such as science or more general problem-solving scenarios. In those domains, training a PRM based on human evaluations of valid reasoning steps could yield a general verifier, which can be used for assuring the validity of the proof/solution chain. This would explain the need for human annotators and verification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Meta Reinforcement Learning -Learning How To Think</head><p>In this section we will build out an interpretation of the reasoning problem and Meta-CoT from the perspectives of meta-learning and meta-RL. In Section 4.2.1 we motivated the need for in-context search through the paradigm of computational complexity and the generator-verifier gap. In this section, we build an alternative formulation which will help us formalize empirical results of RL training. In particular, we consider the search problem in the deterministic MDP formulation from Section 3.3, however, in this section we assume the reward function 𝑟(S, q) → {0, 1} is a deterministic (but a-priori unknown) function of the prompt q, which accepts only a particular set of solutions. At test time under a new prompt, this creates epistemic uncertainty of the reward function i.e. a-priori we do not know the full set of accepted or rejected solutions for this task (prompt question). This process turns the MDP formulation we previously outlined in 3.3 into a Partially Observable MDP (POMDP), a view formalized in classical RL by <ref type="bibr" target="#b17">Ghosh et al. (2021)</ref>. Their work proves the following general remark:  In other words, regular RL-trained policies can, in theory, have arbitrarily bad performance on new reasoning domains <ref type="bibr" target="#b67">Setlur et al. (2025)</ref>. This statement is expanded on and made mathematically precise by <ref type="bibr" target="#b17">Ghosh et al. (2021)</ref> and follow-up works, however that formalization is beyond the scope of the current report. In this work, we stipulate that the reasoning problem, as a POMDP, is better suited to a meta-RL framework <ref type="bibr" target="#b24">(Humplik et al., 2019;</ref><ref type="bibr" target="#b60">Rakelly et al., 2019)</ref> rather than the regular RL setting, as described above. In the meta-learning setting we are given a distribution of tasks, which in our case would be individual reasoning problems q ∼ 𝒟 train . The meta-learning objective seeks a policy 𝜋 𝜃 and an adaptation procedure 𝑈 such that, for a sampled task q, the adapted policy 𝜋 𝑈 (𝜃) performs well with minimal additional samples. The overall optimization objective is</p><formula xml:id="formula_67">min 𝜃 E q∼𝒟 train E 𝜋 𝑈 (𝜃) [𝐿 q (𝜃)] ,</formula><p>where 𝐿 q represents the loss associated with task q. For example, in the revision formulation of Section 4.2.2 the objective is</p><formula xml:id="formula_68">min 𝜃 E q∼𝒟 train E S 𝑖 ∼𝜋 ref (•|q) [︀ -log 𝜋 𝜃 (S * |S 𝑗 , . . . , S 1 , q) ]︀ , (<label>12</label></formula><formula xml:id="formula_69">)</formula><p>where S * is the optimal solution to the problem q and S 𝑖 , 𝑖 = 1, . . . , 𝑗 are solutions provided by some reference policy (usually 𝜋 𝜃 0 ). Here the adaptation procedure is represented by the operator  If we modify the meta-learning objective in Equation <ref type="formula" target="#formula_68">12</ref>with on-policy sampling, we can essentially recover a formulation of the RL 2 (Duan et al., 2016) approach, which has strong synergies with LLMs due to their in-context learning capabilities. In this setting the agent, represented as a recurrent policy, encounters a series of tasks q and interacts with them for several episodes with persistent intra-episode memory for the particular task. Here, the goal of the agent is to maximize the accumulated reward over 𝐾 episodes:</p><formula xml:id="formula_70">𝜋 𝑈 (𝜃) (•|q) → 𝜋 𝜃 (•|S 𝑗 , . . . , S 1 , q),</formula><formula xml:id="formula_71">max 𝜋 𝜃 E q∼𝒟 train E S 𝑗 ∼𝜋 𝜃 (•|S 𝑗-1 ,...,S 1 ,q) ⎡ ⎣ 𝐾 ∑︁ 𝑗=1 𝑟(S 𝑗 , q) ⎤ ⎦ (13)</formula><p>across the distribution of tasks, also notice that here the expectation is taken over the current policy iterate 𝜋 𝜃 , removing the issue of distribution shift. We will draw some additional connections between Equation 12 and Equation 13 in the next section. This objective be optimized through standard reinforcement learning algorithms such as REINFORCE <ref type="bibr" target="#b84">(Williams, 1992)</ref> and PPO <ref type="bibr" target="#b64">(Schulman et al., 2017)</ref>. While successful in classical meta-RL tasks, this approach can lead to policy collapse on locally greedy behavior -i.e. it may not be able to fully explore new environments but instead collapse to common behaviors. Addressing this issue, <ref type="bibr" target="#b77">Stadie et al. (2019)</ref> propose a simple modification, E-RL 2 , which considers the objective</p><formula xml:id="formula_72">max 𝜋 𝜃 E q∼𝒟 train E S 𝑗 ∼𝜋 𝜃 (•|S 𝑗-1 ,...,S 1 ,q) [︀ 𝑟(S 𝐾 , q) ]︀<label>(14)</label></formula><p>aiming to maximize the return over the final episode only. This allows the policy to explore without reward supervision for 𝐾 -1 episodes, providing wider coverage of the environment which allows the policy to maximize rewards in the final episode. This slight modification mostly prevents the collapse to greedy behaviors seen in the standard RL 2 algorithm. Since these early works there has been significant follow-up literature and we refer the reader to the survey of <ref type="bibr" target="#b5">Beck et al. (2024)</ref> for an overview. The above discussion uses the standard RL 2 episodic formulation as it was studied in classical MDPs, however, this is not strictly necessary in the language setting outlined in Section 4. If we equip the agent with the capability to 1. terminate an episode early -i.e. achieve the information objective before it reaches the final solution 2. reset to an arbitrary state in context rather than restart the solution from the first step then the meta-RL formulation remains valid for any in-context exploration strategy, including general tree search approaches. In purely language domains, such as mathematical reasoning, this is feasible and we can directly apply the E-RL 2 algorithm outlined above to models such as those by <ref type="bibr" target="#b86">Yang et al. (2022)</ref> , <ref type="bibr" target="#b35">Lehnert et al. (2024)</ref>, or <ref type="bibr" target="#b14">Gandhi et al. (2024)</ref>. However, in domains where the policy does not have full control of the environment, such as code or agentic formulations, different search structures may be required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Meta-RL In Small Domains</head><p>In the prior section we argued for on-policy RL in reasoning systems to handle train-test distribution shifts, but whether pure instruction-tuning (without RL) is enough to induce capabilities in the model is still debated. In general, the use of RL in post-training improves model performance. Indeed, in Section 4 we outlined the capability to discover improved exploration (adaptation 𝑈 ) algorithms through RL as a major advantage, but the degree to which this occurs remains unclear. Specifically, we want to answer the question: can an in-context search algorithm, post-trained with reinforcement learning, outperform a modular search system? One such example exists where, as shown in Figure <ref type="figure" target="#fig_18">20</ref>, RL post-training improves overall performance in terms of accuracy, reduces the number of logical mistakes, and makes the search more efficient. However, the overall performance is still only comparable to the modular (symbolic) search paradigm -i.e. while RL significantly improves performance over the pure SFT (instruction-tuned) model, RL is not currently able to discover new </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Meta-RL In Language Model Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Training  <ref type="bibr" target="#b16">(Gehring et al., 2024)</ref>.</p><p>Two works that have scaled the multi-turn formulation of meta-RL for reasoning tasks are <ref type="bibr" target="#b33">Kumar et al. (2024)</ref> and <ref type="bibr" target="#b16">Gehring et al. (2024)</ref>. While Reinforcement Learning with Execution Feedback (RLEF) <ref type="bibr" target="#b16">(Gehring et al., 2024)</ref> does not use the meta-RL or search formulations, they do frame their approach as "learning from feedback", and it fits the Meta-CoT framework. Specifically, RLEF implements the E-RL 2 objective from equation 14 with an additional distributional constraint following standard RLHF practices <ref type="bibr" target="#b78">(Stiennon et al., 2022;</ref><ref type="bibr">Ouyang et al., 2022)</ref>. First, the model runs several iterations of exploration (episodes, which represent a full code solution), receiving compiler feedback from public test cases until it passes or reaches an exploration limit. Next, the model proposes a final solution (evaluation episode) and receives a reward based on hidden private test cases, which is used for RL training. See Figure <ref type="figure" target="#fig_19">21</ref> for an overview of the process, along with an example of model outputs. <ref type="bibr" target="#b16">(Gehring et al., 2024)</ref> shows a number of interesting empirical findings. The first finding is the relative effect of SFT versus RL training (results shown in Table <ref type="table" target="#tab_6">2</ref>). The clear trend is that SFT training does not induce any in-context exploration capability and the does not lead to improved performance. This finding is consistent with the results from <ref type="bibr" target="#b33">Kumar et al. (2024)</ref>, which carry out a similar analysis over multi-turn revisions of math problem solutions.</p><p>Next, as shown in Figure <ref type="figure" target="#fig_20">22</ref>, it is clear from the pass@1 results that the model is able to explore and internalize environment feedback to refine the code over multiple turns. However, more interestingly, it is able to do so without reliable feedback. In particular the "random feedback" results replaces the compiler output with output from an unrelated problem. This likely creates significant issues for the model, since the feedback could throw off the LLM's grounding <ref type="bibr" target="#b45">(Mirzadeh et al., 2024)</ref>. However, we see that with RL tuning performance continues to improve with additional test-time revisions, although a gap with ground-truth feedback still exists. This is consistent with the findings from <ref type="bibr" target="#b33">Kumar et al. (2024)</ref>, which demonstrate the capability to self-correct without external feedback in the mathematical reasoning domain. Furthermore, these results are also consistent with our formulation of exploration in the epistemic POMDP from Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Efficiency Or Super-Intelligence?</head><p>As outlined in the beginning of Section 4 two main reasons to internalize a search procedure within a single auto-regressive model are: (1) improved efficiency of the search procedure, and (2) the emergence of "super"-intelligence.</p><p>Results in Figure <ref type="figure" target="#fig_18">20</ref> show significant improvement from RL post-training in the limited domain of the Countdown game, specifically, success rates improve while also using smaller search budgets. However, even after RL post-training the SoS model does not out-perform the success rate of the symbolic approach which generated the instruction-tuning data. Similar findings have also been reported by <ref type="bibr" target="#b35">Lehnert et al. (2024)</ref>, where the model achieves significant improvement in token efficiency, but not in success rates based on some simple post-training procedure. Currently, it is unclear whether continued RL training can lead to a stronger model that substantially out-performs the modular search approach. An interesting analysis by <ref type="bibr" target="#b14">Gandhi et al. (2024)</ref> (Figure <ref type="figure" target="#fig_21">23</ref>) shows the performance of various methods on difficult problems (i.e., problems that no modular symbolic search We see that the model uses inference budget more efficiently than parallel sampling with majority vote, but does not outperform large-scale parallel sampling at higher token budgets. Source: (DeepSeek, 2024). Right: The percentage of problems which the SoS model solves on Countdown, but symbolic strategies do not. The biggest gain comes from RL tuning via STaR. Source: Figure <ref type="figure" target="#fig_2">5c</ref> in <ref type="bibr" target="#b14">(Gandhi et al., 2024)</ref>. approach solves). The base SFT-tuned SoS model solves about 1% of these problems, with RL-tuned models reaching up to 4% (STaR in particular).</p><p>In Section 6 we outlined the Meta-RL formulation, which stipulates that through RL post-training, we're essentially searching over algorithms, i.e. in-context adaptation procedures 𝑈 (𝜃), rather than pure policies. In-theory, this process could discover novel reasoning strategies that unlock capabilities beyond the training data or manually designed reasoning approaches. Within the simple domain of Countdown, this does allow us to solve some complex problems that standard search methods do not, but this effect appears weak, even in this limited setting. As outlined earlier, it is unclear whether further RL training will allow us to discover novel reasoning algorithms that allow us to solve new classes of problems. At larger scales, similar results seem to hold in the math <ref type="bibr" target="#b75">(Snell et al., 2024)</ref> and code-generation domains (with ground-truth environment feedback) <ref type="bibr" target="#b16">(Gehring et al., 2024)</ref>. As shown in Figures <ref type="figure">10</ref> and<ref type="figure" target="#fig_20">22</ref> the E-RL 2 training clearly improves the search performance in terms of pass rate versus search budgets. However, it is unclear whether the RL post-trained model actually solves classes of problems that are unsolved by the base model under increased search budget. Indeed, for code generation, at the 70B scale, with 𝑘 = 100 the performance of the RL-tuned model is only about 5% above the base model. At the same time, performance of the RL model appears to be saturating, while the base model demonstrates continued scaling. One potential counterpoint is the scaling law presented by DeepSeek (2024), as shown in Figure <ref type="figure" target="#fig_21">23</ref> (left). In particular, we see that at low token limits the model demonstrates significantly better scaling with additional in-context search rather than parallel majority voting. On the other hand, at the higher end of token limits, majority voting can out-perform the model with in-context search, but at a higher inference cost. We should note that this approach uses solution-level majority voting, rather than a separate verifier, which has proven to be a much stronger search strategy <ref type="bibr" target="#b9">(Cobbe et al., 2021;</ref><ref type="bibr" target="#b39">Lightman et al., 2023)</ref>.</p><p>In conclusion, based on public evidence, the proposed framework of in-context search can significantly improve the efficiency of the search procedure over base models and "symbolic" approaches or search strategies. However, there is only weak evidence that these models can discover novel reasoning methods that can solve classes of problems that were not solvable previously under some search budget.</p><p>That is, under current empirical evidence the benefits outlined in point (1) above appear clear. However, evidence of emergent "super"-reasoning under point (2) appears weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Can System 2 Reasoning Emerge From Pure RL?</head><p>There is an opinion in the open-research community that the current generation of advanced reasoning models are artifacts of continual reinforcement learning<ref type="foot" target="#foot_2">foot_2</ref> . For both theoretical and practical reasons, we believe this is unlikely with the current generation of language models. First, outside of the OpenAI o1, DeepSeek R1, and Qwen QwQ models, which have undisclosed training routines, "standard" models do not exhibit such strong reasoning behaviors, despite the fact that they have already been extensively post-trained with instruction-tuning and reinforcement learning on reasoning tasks. From a more theoretical point-of-view, meta-RL does not arise from standard RL. Indeed, this is the main point of <ref type="bibr" target="#b17">Ghosh et al. (2021)</ref> and the remark in Section 6 -models trained with RL on standard CoT formulation can exhibit arbitrarily bad performance on new problems.</p><p>We see this empirically in Figure <ref type="figure" target="#fig_20">22</ref> (right). Note that setting the turn limit to 1 (solid blue line) corresponds to standard RL training (no in-context exploration episodes). In this setting the RL post-trained model performs worse than the base model (dotted blue line) at higher levels of inference compute (Sample Budget) on a held-out test set. Furthermore, models trained with even a turn limit of 3 show significantly better inference scaling over the base model, continuing well beyond their training budget. Similar results were obtained by <ref type="bibr" target="#b33">Kumar et al. (2024)</ref>, showing that a naive application of the RL 2 objective in Equation 13 leads the model to collapse onto a greedy policy which does not perform in-context exploration (consistent with the findings of the original work <ref type="bibr" target="#b77">(Stadie et al., 2019)</ref>). On the first step, the two-episode formulation of SCoRe (method proposed by <ref type="bibr" target="#b33">Kumar et al. (2024)</ref>) is identical to standard RL, which prevents the model from meaningful in-context exploration. To alleviate this issue they include an additional training stage using the E-RL 2 objective, similar to <ref type="bibr" target="#b16">Gehring et al. (2024)</ref>, before annealing into an RL 2 approach. These empirical observations are in line with prior results from meta-RL theory. However, in the pure language setting, such as mathematical reasoning, the model uses an auto-regressive architecture with memory. Moreover, it essentially has full control of the environment and, in theory, can induce meta-behaviors such as backtracking and branching. In practical terms, there is no theoretical reason that the LLM cannot produce a complex sequence of tokens, such as the Meta-CoTs described earlier. We will investigate the presence of such artifacts next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.">Inducing Meta-Reasoning In LLMs</head><p>Prior works have shown that complex exploration and reasoning behaviors can be induced in LLMs through in-context demonstrations <ref type="bibr" target="#b66">(Sel et al., 2024;</ref><ref type="bibr" target="#b13">Gandhi et al., 2023;</ref><ref type="bibr" target="#b48">Nie et al., 2024)</ref>. However, the degree to which such prompting can induce genuine meta-reasoning capabilities, particularly for complex reasoning, remains an open question. In this section, we investigate meta-reasoning induction through carefully constructed prompting strategies. We evaluate model performance on the MATH test set <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref>. Our analysis examines three key dimensions: token  generation patterns, expressions of meta-cognitive behaviors (such as explicit error recognition), and mathematical problem-solving accuracy. We evaluate five prompting strategies with progressively increasing complexity:</p><p>1. The Baseline strategy implements minimal instruction, directing the model to assume mathematical expertise while providing basic formatting requirements. 2. The CoT strategy extends the Baseline prompt by requesting explicit step-by-step reasoning and chain of thought verbalization. 3. The Think strategy implements a distinctive approach to mathematical reasoning by requesting stream-of-consciousness meta-cognition within explicit structural constraints. This strategy aims to elicit authentic search by placing an inner monologue inside of a private "thinking" header, using natural language patterns ("Hmm", "Let me see...", "Because of this...") that request self-verification and can demand explicit backtracking on identified errors. These design choices mimic the non-linear reasoning patterns of mathematicians approaching new problems. The three-shot variant (Think 3-shot) augments this with examples, using a static set of in-context demonstrations to illustrate desired behavior patterns. 4. The Think &amp; Verify prompt introduces structured iteration bounds and verification requirements through dual constraints. The prompt mandates a second-pass verification for confident solutions while maintaining a soft maximum of six distinct solution attempts. Upon reaching a high-confidence solution, the protocol demands one additional verification attempt. Successful verification permits termination, though the model retains autonomy to continue exploration within the maximum bound should it self-report uncertainty. This adaptive termination criterion functions as an in-context best-of-N approach with self-consistency based early-stopping, optimistically balancing comprehensive solution space exploration with self-verification.</p><p>We analyze expressions of regret and self-correction across models. We define regret as explicit acknowledgments of errors or uncertainty through phrases like "I made a mistake", "oops", "let me reconsider", or similar language. A detailed breakdown of regret statistics across different models and prompting strategies is provided in Table <ref type="table">5</ref> in Appendix B.</p><p>The regret analysis reveals a clear distinction between basic prompting approaches and those explicitly designed to encourage meta-cognitive behaviors. Under baseline and standard CoT prompting, which provide no explicit instruction for self-verification, models rarely express regret or acknowledge errors (&lt;0.5% of solutions across all models), showing that behaviors like self-verification and backtracking rarely emerge natively with standard prompting. In contrast, the Think strategy, which explicitly instructs models that they can "think, reflect, revise, backtrack, and verify during responding when it considers doing so would lead to a better solution," produces substantially higher rates of error recognition and correction, particularly in larger models like Llama 3.1 70B (12.65%). This effect is further amplified when combining such instruction with structured examples (Think 3-shot) and explicit verification requirements (Think &amp; Verify), leading to significantly increased rates of regret expression across most models, peaking at 25.67% for Llama 3.1 70B under the Think &amp; Verify strategy.</p><p>Interestingly, model scale correlates with willingness to express regret, but not monotonically. While larger Llama models show high rates of regret expression (15-25% under Think &amp; Verify), more advanced models like GPT-4o and Claude 3.5 Sonnet exhibit notably lower rates (1-4%).</p><p>Our main set of results is shown in Figure <ref type="figure" target="#fig_23">25</ref>. First, to further understand how model behavior varies across difficulty levels and prompting strategies, we analyze the relationship between token usage and accuracy. The box plots show token distributions while overlaid dashed lines track accuracy across difficulty levels. Notably, more complex prompting strategies incur higher token overhead but do not consistently translate this additional computation into proportional accuracy gains, particularly in higher difficulty regimes. For smaller models like Llama 3.1 8B, we observe significantly increased token usage in higher difficulty levels that corresponds with a marked decline in accuracy. In contrast, larger models maintain more consistent token usage patterns across difficulty levels. The Think &amp; Verify approach consistently produces longer solutions across all models, reflecting the inherent overhead of verification steps. However, this increased verbosity does not translate to improved accuracy-particularly for smaller models where longer solutions may indicate joint struggles with problem-solving and verification. Moreover, we see significant increases in verbosity on all problem difficulties. Of particular interest, we find that the Think &amp; Verify strategy leads to equivalent accuracies on the lowest difficulty problems (for all models), while often requiring more than double the token budget. This observation indicates that models seek to match the reasoning "style" rather than substance and may be even faking mistakes to match the desired in-context behaviors <ref type="bibr" target="#b19">(Gudibande et al., 2023)</ref>.</p><p>In conclusion, while sophisticated prompts successfully elicit reasoning-like behaviors absent from baseline approaches, these behaviors -including recognizing mistakes and backtrackingdo not consistently yield performance benefits. In fact, when models engage in self-correction and backtracking, the final answers are more likely to be incorrect. This pattern, combined with the observation that token generation increases with MATH difficulty level across all strategies while accuracy trajectories remain similar, suggests fundamental limitations in using explicit meta-cognitive instruction to induce robust reasoning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Putting It All Together -A Pipeline for System 2 Reasoning</head><p>So far, we have presented a theory of advanced reasoning capabilities based around search, as well as some early empirical findings. In this section we suggest an overall approach to training advanced reasoning models. Our proposal follows the overall structure of modern post-training, consisting of instruction-tuning and RL training <ref type="bibr" target="#b78">(Stiennon et al., 2022;</ref><ref type="bibr">Ouyang et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Instruction Tuning</head><p>In Section 6.4.1, we presented evidence that the current generation of models cannot induce effective meta-reasoning through in-context prompting, and may even exhibit misleading behaviors. Instead, we propose to begin the process through instruction-tuning with synthetic in-context search data, in the manner outlined in Section 4.3. We showed in Section 6.4.1 that even advanced models rarely demonstrate meta-reasoning capabilities, such as expressing regret or backtracking. Hence, we believe an instruction-tuning stage is critical to endow the model with such backtracking and branching capabilities, which are not frequently present in the pre-training corpus or general purpose instructiontuning data. Following the synthetic data approach in Section 4.3, we construct a training dataset 𝒟 train = {q (𝑖) , Z (𝑖) , S (𝑖) } 𝑁 𝑖=1 which represents the Meta-CoT Z = z 1 , . . . , z 𝐾 , followed by verifiable solution S = s, . . . , s 𝑛 . For this stage of training, multiple training objectives can be considered, which we outline in Appendix C. To what degree each of these different objectives (and combinations thereof) yield qualitatively different behaviors is an open empirical question. However, based on previous results at the scale of modern LLMs, we hypothesize that the exact format of the pre-training stage is not crucial, rather the key performance contributor is RL post-training <ref type="bibr">(Ye et al., 2024b;</ref><ref type="bibr" target="#b33">Kumar et al., 2024;</ref><ref type="bibr" target="#b16">Gehring et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Post-Training With RL</head><p>Prior works have shown strong empirical results with multi-turn RL training <ref type="bibr" target="#b33">(Kumar et al., 2024;</ref><ref type="bibr" target="#b16">Gehring et al., 2024)</ref>. We propose using a similar objective based on the E-RL 2 approach outlined in Equation 14 with the standard additional distributional constraints:</p><formula xml:id="formula_73">max 𝜃 E S,Z∼𝜋 𝜃 (•|q),q∼𝒟 train [︃ 𝑟 * (S, q) -𝛽 ∑︁ 𝑡 D 𝐾𝐿 [𝜋 𝜃 (z 𝑡+1 |Z 𝑡 , q)||𝜋 ref (z 𝑡+1 |Z 𝑡 , q)] ]︃<label>(15)</label></formula><p>where 𝑟 * is the verifiable reward from the solution and 𝜋 ref is a reference policy (usually the instructiontuned model). Similar objectives have been considered in agentic applications where the "Meta-CoT" represents an actual search over a web interface <ref type="bibr" target="#b47">(Nakano et al., 2022;</ref><ref type="bibr" target="#b57">Putta et al., 2024)</ref>. Note that the reference constraint here is not strictly necessary, but likely required to keep the chain stable and interpretable. Keeping the RL process stable over long horizons and learning robust credit assignment is likely a significant challenge. There are a number of possible options for dealing with these challenges:</p><p>1. The step-wise branching structure of MCTS training presents one possible resolution. For example, by using an objective similar to that proposed by <ref type="bibr" target="#b12">Feng et al. (2024)</ref>, who perform an SFT policy distillation approach using MCTS. Although in general SFT-based policy optimization tends to be inefficient in language model settings <ref type="bibr" target="#b79">(Tajwar et al., 2024</ref>). 2. A number of step-level DPO objectives have been proposed, specifically for reasoning applications, which can be combined with tree-search based exploration <ref type="bibr" target="#b85">(Xie et al., 2024;</ref><ref type="bibr">Setlur et al., 2024a;</ref><ref type="bibr" target="#b34">Lai et al., 2024)</ref>. We believe this might be a promising direction, as DPO-based approaches also tend to be more robust to off-policy data <ref type="bibr" target="#b49">(Noukhovitch et al., 2024)</ref> than comparable policy-gradient based approaches <ref type="bibr" target="#b69">(Shao et al., 2024)</ref>, which allows for massive asynchronous RL scaling. 3. A branching version of on-policy methods could also be used, such as a step-level version of RLOO <ref type="bibr" target="#b1">(Ahmadian et al., 2024)</ref> or VinePPO <ref type="bibr" target="#b21">(Havrilla et al., 2024;</ref><ref type="bibr" target="#b29">Kazemnejad et al., 2024)</ref>, although these methods might not be as robust to off-policy data as prior options and hence less scalable <ref type="bibr" target="#b49">(Noukhovitch et al., 2024)</ref>.</p><p>The exact correlations between instruction-tuning, pre-training, and RL are unknown in the public literature. Smaller scale studies have found SFT tuning to be effective in endowing transformers with meta-reasoning behaviors <ref type="bibr" target="#b86">(Yang et al., 2022;</ref><ref type="bibr" target="#b35">Lehnert et al., 2024;</ref><ref type="bibr" target="#b14">Gandhi et al., 2024;</ref><ref type="bibr">Ye et al., 2024b)</ref>, while larger scale studies indicate that purely training with next-token prediction on synthetic traces might not be enough due to train-test distribution shifts and find that on-policy RL is the key component for improving performance <ref type="bibr">(Zhang et al., 2024b;</ref><ref type="bibr" target="#b33">Kumar et al., 2024;</ref><ref type="bibr" target="#b16">Gehring et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Q* or q-STaR?</head><p>In the prior section, we introduced the RL training objective in Equation 15 using a verifier reward 𝑟 * (S, q). Here we will derive another RL formulation with a self-training type of RL objective without the need for an external verifier.</p><p>Under the (Meta-)CoT formulation of Section 2, we can consider the Meta-CoT process Z as a general latent variable model for the whole problem solution S. We can then apply a widely known </p><p>where 𝑞(Z|q) is a variational inference function Kingma &amp; Welling (2013), or in our case a latent reasoner. Since all components here are auto-regressive transformers (LLMs), we can amortize them inside a single model. Now, if we set 𝜋(Z|q) to be the prior model 𝜋 ref , which is initialized as the instruction-tuned base model (𝜋 𝜃 0 ) from Section 7.1 and amortize the reasoning inference model 𝑞(Z|q) and the decoder model 𝜋(S|Z, q) into a single LLM 𝜋 𝜃 we get the objective:</p><formula xml:id="formula_75">max 𝜃 E Z∼𝜋 𝜃 (•|q),S,q∼𝒟 train [log 𝜋 𝜃 (S|Z, q) -𝛽D 𝐾𝐿 [𝜋 𝜃 (Z, |q)||𝜋 ref (Z|q)]]<label>(18)</label></formula><p>where we used the 𝛽-VAE formulation <ref type="bibr" target="#b23">Higgins et al. (2017)</ref>. Unlike standard VAEs though, the parametric models here are auto-regressive transformers, which sample discrete tokens, hence we cannot use the reparameterization trick to compute gradients of the above distribution and must result to RL optimization. This objective indeed looks similar to the main RL objective from the previous section, as formulated in Equation <ref type="formula" target="#formula_73">15</ref>, but with a few key differences. First, the reward function is represented as 𝑟(S, q) = log 𝜋 𝜃 (S|Z, q)</p><p>where the solution S is no longer sampled on-policy from the model, but from a pre-existing questionsolution training dataset. Hence the sampling expectation from the policy is only over the latent Meta-CoT. Notice also that the reward itself is a function of the model parameters 𝜃, which then requires a modification to the standard policy gradient approach. With some simple differentiationby-parts calculus, the above objective can be represented as:</p><formula xml:id="formula_77">max 𝜃 E Z∼𝜋 𝜃 (•|q),S,q∼𝒟 train [sg(log 𝜋 𝜃 (S|Z, q)) -𝛽D 𝐾𝐿 [𝜋 𝜃 (Z, |q)||𝜋 ref (Z|q)]] + (20) max 𝜃 E Z∼sg(𝜋 𝜃 (•|q)),S,q∼𝒟 train [log 𝜋 𝜃 (S|Z, q)]</formula><p>Here the first equation is now a standard RL problem with reward as defined in Equation 19 which can be optimized with the standard methods, outlined in Section 7.2. The second part is a maximum likelihood training of the Meta-CoT "summarization" model on the ground-truth solution. Essentially the goal of this training objective is to make the conditional model 𝜋 𝜃 (S|Z, q) more expressive than the standard training objective 𝜋 𝜃 (S|q). Some empirical evidence from <ref type="bibr" target="#b58">Qu et al. (2024)</ref> supports this hypothesis as shown in Figure <ref type="figure" target="#fig_24">26</ref> (left) which shows training curves for the revision objective in Equation <ref type="formula">5</ref>. Indeed, even with standard SFT using in-context exploration the model achieves significantly lower loss than directly predicting the correct answer. This is similar in spirit to the goal of Quiet-STaR <ref type="bibr" target="#b95">Zelikman et al. (2024)</ref>, with the difference being that we optimize and marginalize over the entire latent meta-reasoning process and final solution with arbitrary complexity. In comparison Quiet-STaR applies the above objective at a token-level with fixed token budget for the latent process, thus essentially making the transformer computation graph deeper, similarly to Universal Transformer <ref type="bibr" target="#b11">Dehghani et al. (2019)</ref>, rather than optimizing meaningful semantic reasoning. This line of reasoning is also consistent with the complexity arguments outlined in Section 2.</p><p>Finally, we should note that while this approach does not utilize verifiers for RL training, it still needs to bootstrap the latent reasoning processes Z, which still requires verifiable outcomes to generate high-quality synthetic training data.</p><p>The objective in Equation 20 has one major advantage that we no longer require verification of the final solution, since the training objective only requires a dataset of question-solution pairs. This allows us to train on open-ended and hard to verify problems, such as proofs and general reasoning. The downside is now that the model 𝜋 𝜃 (S|Z, q) is only essentially trained with supervised fine-tuning and does not get on-policy reward feedback, which may be sub-optimal. If the Meta-CoT process can find a solution with high certainty, then perhaps an SFT training objective for the summarization model is sufficient, which is an empirical question. Anecdotally, we have observed instances of the DeepSeek R1 model DeepSeek (2024) finds the correct answer in its Meta-CoT but actually outputs the wrong final solution as shown in Figure <ref type="figure" target="#fig_24">26</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">Discount Rates</head><p>While standard RLHF pipelines have not used discount rates in the past, they may be required in reasoning applications. Recent works have discovered that small biases in preferences for longer answers are routinely exploited by reward models in RL pipelines, which has yielded significantly more verbose models <ref type="bibr" target="#b74">(Singhal et al., 2023;</ref><ref type="bibr" target="#b53">Park et al., 2024)</ref>. These issues are present even in strong systems, such as GPT-4 (OpenAI, 2023) and require explicit regularization. As demonstrated in prior chapters, when increased sampling correlates with higher accuracy then, without explicit regularization, the model can choose to continue generating/searching for solutions or collapse on some majority voting approach with potentially unlimited inference targets. Indeed, recent work <ref type="bibr" target="#b8">Chen et al. (2024)</ref> found that advanced reasoning models can generate significantly longer sequences even for simple problems ("What is 2+3=") as shown in Figure <ref type="figure" target="#fig_25">27</ref>. The Qwen QwQ model <ref type="bibr" target="#b80">(Team, 2024)</ref> generates up to 13 solutions in context before providing a final answer. To mitigate this issue, we might require a modification of the RL objective from Equation <ref type="formula" target="#formula_73">15</ref>, to use a discounted objective fiwhere 𝛾 &lt; 1. This objective would (in theory) force the model to trade-off compute versus certainty in its response. This effect is demonstrated in Figure <ref type="figure" target="#fig_25">27</ref> (left), which shows a discounted version of the best-of-N objective. Without a particular penalty on the generation budget, performance increases steadily with additional compute, which may push the model to continuously increase the inference budget (as compared to base models) as shown on the right. If we consider the discounted reward (accuracy) instead, we see a hump-shaped objective as there is a stronger trade-off between verifier confidence and the generation budget.</p><formula xml:id="formula_78">max 𝜃 E S,Z∼𝜋 𝜃 (•|q),q∼𝒟 train [︃ 𝛾 |Z| 𝑟 * (S, q) -𝛽 ∑︁ 𝑡 D 𝐾𝐿 [𝜋 𝜃 (z 𝑡+1 |Z 𝑡 , q)||𝜋 ref (z 𝑡+1 |Z 𝑡 , q)] ]︃<label>(21)</label></formula><p>This also raises an interesting trade-off on controlling model behaviors in terms of inference/accuracy. We might be interested in forcing a particular trade-off depending on problem difficulty. Consider then a distribution over discount rates 𝛾 1 &lt; . . . &lt; 𝛾 𝑚 . We can also associated an individual language prompts p 𝛾 with each discount rate such as "Work as quickly as possible" for high discount, to "Take as much time as needed" for a low discount rate. Then, at train time we can optimize the objective</p><formula xml:id="formula_79">max 𝜃 E S,Z∼𝜋 𝜃 (•|q,p𝛾 𝑖 ),q∼𝒟 train ,𝑖∼1:𝑚 [︃ 𝛾 |Z| 𝑖 𝑟 * (S, q) -𝛽 ∑︁ 𝑡 D 𝐾𝐿 [𝜋 𝜃 (z 𝑡+1 |Z 𝑡 , q, p 𝛾 𝑖 )||𝜋 ref (z 𝑡+1 |Z 𝑡 , q)] ]︃</formula><p>Then, at inference time we can control the qualitative model behavior through conditioning on the prompt to trade-off inference compute versus accuracy. <ref type="bibr" target="#b65">Schultz et al. (2024)</ref> showed that a if we train a model on MCTS search traces with different parameters, at inference time we can condition the model on a particular parameter configuration and recreate the qualitative behaviors(as shown in Figure <ref type="figure" target="#fig_6">9</ref>). As discussed earlier, simple supervised fine-tuning may not be sufficient to induce advanced reasoning capabilities (and likely even more difficult to do so in a controllable way), however, these results demonstrate promise in inducing controllable behaviors into the model through the conditional prompting objective above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Going Forward</head><p>Public open-research/source progress on reasoning models is currently bottle-necked by three main issues:</p><p>1. Access to resources in terms of both data and compute as these algorithms require significant amount of both. 2. Open-source infrastructure for large scale inference and training is currently lackluster. 3. Algorithmic exploration -we have a lot of avenues to explore and only limited people and resources actively working on the right directions.</p><p>We expand on these issues in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">The "Big MATH" Project</head><p>While compute bottlenecks are a persistent issue in open research, we found the lack of open datasets with verifiable reasoning problems to be an even bigger challenge. Prior works have used the GSM8k <ref type="bibr" target="#b9">(Cobbe et al., 2021)</ref> and MATH <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref> datasets, but the first has largely been saturated by the current generation of models and the second is quite limited, with only 12,000 problems. To overcome these challenges, we have put significant work into the "Big MATH" project -an effort to aggregate over 1,000,000 high-quality, high-confidence, and diverse verifiable math problems. We combine existing datasets with significant post-processing, as well as efforts to acquire additional data from novel sources. We outline this below.</p><p>Our proposed training pipeline requires a large-scale corpus of challenging prompts with verifiable answers, but developing such datasets presents fundamental constraints in automated verification and assessment of reasoning capabilities. In many domains, solutions are too unstructured or nuanced for automated verification: a single objective answer may not exist, correct solutions can appear in equally valid but textually distinct forms, and certain tasks (e.g. complex proofs) resist reduction to deterministically verifiable outputs with a single canonical representation. These issues make automated grading infeasible. Even in contexts where problem-answer pairs can be constructed and easily verified, these formats may reduce to factual retrieval rather than exercises that require reasoning capabilities.</p><p>As evidenced by the wide array of literature discussed in previous sections, mathematics offers a stable ground truth for correctness, naturally programmatic answer grading, and existing public datasets in problem-solution-answer format. While we recognize significant potential in many domains (e.g., programming, diagnostics, finance, analysis) where structured reasoning paths can be validated, the substantial resources required to construct and verify such datasets, particularly without access to proprietary data, makes them impractical for our immediate objectives. While the availability of existing datasets ultimately drove our domain selection, even these openly available resources are constrained in both scale and distribution. We face the critical challenge of distinguishing between computational and pattern proficiency and conceptual understanding when assessing model capabilities -a distinction that fundamentally shapes our dataset requirements and evaluation metrics.</p><p>To guide our data construction, we define three core criteria:</p><p>1. the existence of uniquely verifiable solutions, meaning that problems must admit a single correct answer that can be reliably verified; 2. open-ended problem formulations, ensuring that tasks cannot be easily solved by guessing (as might occur in multiple-choice formats) and instead require nontrivial reasoning steps; and 3. closed-form solutions, such that the final answer must be expressible in a closed form (e.g., a scalar or formula, not a proof), thereby enabling automated evaluation.</p><p>These criteria reveal significant limitations in existing datasets. Those that meet our structural requirements remain severely limited in scale, containing orders of magnitude fewer examples than we estimate necessary for model training. Within the structured collections, we observe a concerning scarcity of non-trivial problems that effectively challenge reasoning capabilities, with many examples simply testing computational abilities or following predictable patterns. Perhaps most problematic is the non-negligible proportion of incorrect problem-solution labels, even in widely-used datasets, introducing substantial complexity to data cleaning.</p><p>Preliminary analysis of one of the largest available datasets, NuminaMath <ref type="bibr" target="#b38">(LI et al., 2024)</ref>, exemplifies these issues. Basic verification shows that out of roughly 860,000 entries, more than 42,500 are duplicates (∼5% of the dataset). Further, while 89.7% of entries contain exactly one clearly boxed solution suitable for automated verification, 2.6% contain no boxed solution, and 7.7% include multiple boxed solutions. Problems like this underscore the state of open and publicly available data and suggest deeper quality concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1.">Data Sourcing</head><p>We consider multiple established mathematical problem datasets that are commonly used in the literature (Table <ref type="table" target="#tab_7">3</ref>). First, we include the Human Annotated Reasoning Problems (HARP) dataset <ref type="bibr" target="#b93">(Yue et al., 2024)</ref>, containing nearly 4,800 competition-level short answer problems with programmaticallycheckable answers. Next, we use the NuminaMath <ref type="bibr" target="#b38">(LI et al., 2024)</ref> dataset, which is composed of roughly 860,000 problems from a variety of benchmarks and sources: Chinese high school math exercises, math olympiad-style competition problems, the art of problem solving forum, MATH <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref>, and GSM8k <ref type="bibr" target="#b9">(Cobbe et al., 2021)</ref>. NuminaMath further incorporates synthetic data from the synthetically generated dataset Orca-Math <ref type="bibr" target="#b46">(Mitra et al., 2024)</ref> as well as further generating a significant amount of synthetic data based on the MATH dataset and a subset of the math competition problems. Additionally, while NuminaMath uses the original split of the MATH dataset (7,000 training problems, 5,500 test problems), we choose to use version with 12,000 training problems and 500 test problems, as originally proposed by <ref type="bibr" target="#b39">(Lightman et al., 2023)</ref>. We incorporate the Omni-MATH dataset <ref type="bibr" target="#b15">(Gao et al., 2024)</ref>, contributing almost 4,500 olympiad-level problems curated from 39 different competition websites. Notably, this dataset employs professional annotators and verifiers to maintain solution-answer quality. Finally, we include OpenMathInstruct-2 <ref type="bibr" target="#b82">(Toshniwal et al., 2024)</ref>, composed of about 607,000 synthetically generated problems. <ref type="bibr" target="#b82">Toshniwal et al. (2024</ref><ref type="bibr">) use Llama3.1-405B (Dubey et al., 2024)</ref> in a multi-step data augmentation pipeline which they seed using the GSM8k <ref type="bibr" target="#b9">(Cobbe et al., 2021)</ref> and MATH <ref type="bibr" target="#b22">(Hendrycks et al., 2021)</ref> datasets.</p><p>Beyond drawing from these established datasets, we maintain an active data collection initiative that extends our corpus through manual curation of competition-level problems. This effort has already contributed thousands of additional olympiad-style problems and continues to grow. Our manual collection process focuses on acquiring mathematically rigorous content from competition archives, olympiad materials, and historical sources not present in existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2.">Data Filtering</head><p>To create datasets with which we can train a model in our training pipeline, we next clean and filter the data from each source using a combination of bespoke and common strategies <ref type="bibr" target="#b2">(Albalak et al., 2024)</ref>. Specifically for our two-stage pipeline, we create two versions of the dataset. First, we have the base filter, where the data will be used during SFT for the model to learn (1) an approximation of the distribution of math problems and (2) to follow the desired chain-of-thought format. Next, the strict filter is intended to be used for the Meta-RL training stage. Thus, the strictly filtered data should contain only problems that closely follow our three desired properties: open-ended, verifiable, closed-form problem-solution pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Source</head><p>Original Base Filter Strict Filter HARP <ref type="bibr" target="#b93">(Yue et al., 2024)</ref> 4,780 3,691 2,996 NuminaMath <ref type="bibr" target="#b38">(LI et al., 2024)</ref> 859,608 452,820 231,887 Omni-MATH <ref type="bibr" target="#b15">(Gao et al., 2024)</ref> 4,428 3,660 2,478 OpenMathInstruct-2 <ref type="bibr" target="#b82">(Toshniwal et al., 2024)</ref> 607 The Base Filter performs some operations that are unique to each subset, and some operations that are shared across the full collection of data. First, in the HARP dataset <ref type="bibr" target="#b93">(Yue et al., 2024)</ref>, we find many problems that contain figures in the Asymptote<ref type="foot" target="#foot_3">foot_3</ref> vector graphics language, which we filter out of the dataset. Next, we filter the NuminaMath dataset <ref type="bibr" target="#b38">(LI et al., 2024)</ref>. This dataset contains a significant amount of synthetic data, which is often difficult to verify for accuracy and correctness. However, the Orca-Math <ref type="bibr" target="#b46">(Mitra et al., 2024)</ref> subset has empirically been proven to improve performance on supervised fine-tuning, and for this reason, we elect to maintain only the Orca-Math data, while discarding the other synthetic subsets. Next, NuminaMath does not explicitly contain answers to each problem, so we extract answers to problems by searching for boxed solutions ("\boxed{}" in LaTeX). Any problems whose solution does not contain exactly 1 boxed answer is filtered out. When exploring Omni-MATH <ref type="bibr" target="#b15">(Gao et al., 2024)</ref>, we found many problems containing author attributions (e.g. a person's name in parenthesis) and removed the attributions from the problem. OpenMathInstruct-2 originally contains multiple solutions and final answers to each problem <ref type="bibr" target="#b82">(Toshniwal et al., 2024)</ref>. The first step we take is to group all matching problems together, removing those which have mismatched expected answers. Next, we found that the data still contains evidence of the synthetic data generation process, such as asking "do you want to solve it?" or "here's the new problem" and remove any problems with similar phrases.</p><p>Finally, after running each of the described filters over the individual subsets, we perform four filtering operations across the full collection. First, we use exact matching to find and remove duplicate problems. Then, we use a FastText language identifier <ref type="bibr">(Joulin et al., 2016b,a;</ref><ref type="bibr" target="#b18">Grave et al., 2018)</ref> and remove any problems where English is not the primary language. Next, we remove problems containing a hyperlink, as this suggests that a model may not have the full resources required to solve the problem. Lastly, we decontaminate the dataset by removing any examples of the MATH500 test set <ref type="bibr" target="#b39">(Lightman et al., 2023)</ref>.</p><p>The Strict Filter further reduces the base filtered data to problem-answer pairs which are more likely to be open-ended, verifiable, and closed-form. The strict filters are applied equally to all subsets of the dataset through a mix of rule-based and model-based filtering. First, we remove questions containing multiple parts as these can be challenging to evaluate. For a similar reason, we also remove questions that ask for a proof as these problems are difficult to evaluate. Next, we choose to remove multiple choice problems as models have a high probability of selecting the correct answer without producing an accurate reasoning chain. For the same reason, we also remove Yes/No and True/False problems, as these may give a poor learning signal during Meta-RL training. Finally, we use the SemDeDup algorithm <ref type="bibr" target="#b0">(Abbas et al., 2023)</ref> with the model at sentence-transformers/all-MiniLM-L6-v2 and remove problems with a cosine similarity over 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Infrastructure</head><p>For the proposed family of algorithms we need RL training infrastructure that can: the training process updates weights, these updates are immediately visible to the inference process through its mapped pointers without requiring any explicit synchronization. This shared memory architecture enables fully parallel execution -the inference process can continuously generate tokens while the training process updates weights, with neither process blocking the other. Although early portions of generated rollouts may be off-policy, the final steps will generally be only one training step off-policy at most, as the inference process immediately sees weight updates through the shared memory. Unlike previous asynchronous approaches <ref type="bibr" target="#b49">(Noukhovitch et al., 2024)</ref> that require distinct generation and training phases, our system maintains constant GPU utilization across both processes. Direct memory sharing provides significantly higher throughput by eliminating synchronization overhead between training and inference. Our initial experiments show 40% improved throughput compared to 3-step asynchronous training that requires explicit weight synchronization, as shown in Figure <ref type="figure" target="#fig_5">28</ref>. However, this approach comes with important trade-offs in memory utilization and parallelism. Sharing GPUs between processes restricts us to suboptimal tensor parallelism (TP) configurations compared to dedicated training and inference setups, resulting in lower theoretical peak throughput for both processes. The impact is particularly pronounced for inference, where reduced memory for KV cache and suboptimal TP configurations significantly constrain the generation speed. In scenarios where inference compute significantly exceeds training compute (e.g. MCTS, where most of the search tree is discarded after each rollout), using separate dedicated GPU clusters with optimized TP configurations for each process can achieve higher overall throughput despite the synchronization overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Open Research Questions</head><p>A number of open research questions remain which are currently not answered in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1.">Open-Ended Verification And CoT Faithfulness</head><p>In this report we outline a pipeline for training advanced reasoning models on verifiable questions. These types of questions largely consist of things like math, scientific questions with symbolic or numerical answers, or code problems with unit tests. However, we believe it is still an open question Figure <ref type="figure" target="#fig_6">29</ref>: Under a fixed generator model and a variable verifier performance seems to also obey a form of a scaling law. Right: Accuracy seems to improve in a log-linear fashion with amount of training data under a standard discriminative PRM. Source: Figure <ref type="figure" target="#fig_3">6</ref> in <ref type="bibr">(Setlur et al., 2024c)</ref>. Left: Performance of a Generative Reward Model (with CoT) on reasoning tasks with a a reference using variable amount of CoTs and majority voting. Increasing the amount of inference-time sampling improves accuracy. Source: Figure <ref type="figure" target="#fig_2">5</ref> in <ref type="bibr" target="#b42">(Mahan et al., 2024)</ref>.</p><p>on how to ensure that the resulting CoTs are faithful and in fact provide valid reasoning. In many applications, such as science for example, the reasoning chain can be even more important than the final answer. Qualitatively, current models with "advanced" reasoning still struggle and produce many CoTs which are inconsistent or unfaithful, even when they obtain the correct final answer. Interestingly, even the single math example provided along with the announcement of the O1 model has an incomplete solution as it makes an unsubstantiated assumption on the form of the polynomial ℎ(𝑥) = 𝑥 2 -𝑐 without proving that the coefficient of 𝑥 is in fact zero <ref type="bibr" target="#b51">(OpenAI, 2024)</ref>. Under a competitive setting this would likely receive 5/7 points, even though the final answer is correct. It is worth noting that the model actually verifies and proves this assumption in it's Meta-CoT but the final output provided does not include those steps. We believe this might be one type of artifact that arises from training with final answer verification only. How to provide rewards on full CoTs and open-ended problems such as proofs remains an open question. Under the assumption of a wide enough generation-verification gap, we believe an RLAIF approach could be promising. In particular, foregoing concepts such as formal verification, one avenue to explore is a "generative verifier" similar to <ref type="bibr">Zhang et al. (2024a)</ref> or <ref type="bibr" target="#b42">Mahan et al. (2024)</ref>. In this setting the evaluation model can be provided with one or several reference solutions/proofs, or other relevant material such as textbook chapters, tools like Wolfram or Mathematica, and specifically fine-tuned to provide evaluations to proposed solutions, rather than final answers alone. As outlined in Section 3.2, we found advanced models to already posses some capabilities in that vein. Unfortunately, a major bottleneck in that line of research is the need for a dataset of open-ended reasoning problems (such as proofs), which would likely require significant human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2.">Process Guidance And The Verifier Gap</head><p>The current work brings to light some remaining questions on the approaches to process guidance and PRMS. In Section 5 we outlined results showing that pre-trained PRMs still significantly lack behind pure Monte Carlo approaches in terms of search efficiency. Moreover, with few exceptions <ref type="bibr">(Setlur et al., 2024c)</ref>, we believe algorithmic approaches for training these models remain understudied. In particular, if we view a PRM as a value function, then many approaches from offline RL <ref type="bibr" target="#b36">(Levine et al., 2020)</ref> become applicable with some already showing promise in agentic domains <ref type="bibr">(Zhou et al., 2024b)</ref>. In the same vein, <ref type="bibr" target="#b71">Silver et al. (2016)</ref> introduced a separate value function over the standard MCTS approach using roll-outs for efficiency purposes. However, we believe there may be a fundamental verifier gap beyond just the efficiency question. As shown in Figure <ref type="figure" target="#fig_1">3</ref> there remains a significant gap between the "best-of-N" verifier driven approach and the oracle pass@N performance. A similar gap seems to exist in the multi-turn code generation domain, as shown in Figure <ref type="figure" target="#fig_20">22</ref>  <ref type="bibr" target="#b16">(Gehring et al., 2024)</ref>. Moreover, under a fixed generator, results from <ref type="bibr">Setlur et al. (2024c)</ref> and our own empirical findings in Section 5 indicate that verifier performance may also be driven by a scaling law as well. In Figure <ref type="figure" target="#fig_6">29</ref> (left) we see clear log-linear scaling in performance in terms of training data. One hypothesis is that verification is also a matter of computational complexity, which is why allowing verifiers to use CoT improves performance as shown in Figure <ref type="figure" target="#fig_1">3</ref>  <ref type="bibr">(Zhang et al., 2024a;</ref><ref type="bibr" target="#b42">Mahan et al., 2024)</ref>. In addition, <ref type="bibr" target="#b42">Mahan et al. (2024)</ref> show further inference-time scaling for verifiers, as using additional CoTs with majority vote improves reward accuracy on reasoning problems well beyond the performance of the standard discriminative verifier. This raises questions for the fundamental learnability of in-context search traces. In essence, if a search trace was generated using ground-truth verification or Monte-Carlo roll-outs, the resulting sequence may have significant implicit complexity, which would be challenging for the model to learn. This would also motivate incorporating further self-evaluation or reflection into reasoning chains. The associated verification scaling laws and design choices remain largely unexplored in the literature and present an important research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3.">Scaling Laws For Reasoning And Search</head><p>While recent model releases have demonstrated strong inference scaling performance, we still lack a thorough scientific exploration of scaling laws in public research, which is a key piece of the recently emerging model paradigm.</p><p>1. The scaling laws of search presented by <ref type="bibr" target="#b25">Jones (2021)</ref> have not been publicly evaluated on realistically challenging reasoning applications with LLMs. While the results from <ref type="bibr" target="#b12">Feng et al. (2024)</ref> show promise, they are still limited and partial. Obtaining a more thorough evaluation on joint policy and verifier scaling will provide clarity to current research questions. So far, unfortunately, the research community has been limited by a lack of data and scalable infrastructure. 2. Exploration of search approaches remains a very under-studied topic. As discussed in Section 4.4, it appears that current reasoning models all implement different approaches to search, which leads to qualitatively different behaviors based on the search strategies presented in Section 4.3. Given a strong trained verifier, what are the performance effects of different search strategies such as BFS/DFS-V, A*, MCTS, etc.? Furthermore, the relationship between exploration strategies at training and test time is an important direction of future study. 3. The trade-offs between instruction-tuning and RL remain unclear. As discussed in Section 6, it appears that standard instruction tuning can endow models with meta-reasoning capabilities in small/simple domains, but these results do not scale, and realistic applications require significant on-policy RL. Is this a fundamental issue with distribution shift, or is performance driven by another relationship, similar to the trade-offs discovered by Setlur et al. (2024a)? 4. The fundamental missing piece of the current inference time scaling law discussion is the performance of the proposed in-context search strategy compared with an explicit search-based method. In theory, the post-training approach outlined in Section 7 can discover novel reasoning approaches (algorithms) that solve fundamentally new classes of problems unsolvable under any search budget by a standard search approach. While there is clear evidence of the efficiency of in-context search (fewer tokens per interaction), it is unclear whether the current generation of models have any emergent capabilities yet. In simplest terms, do strong reasoning models shift the compute-accuracy curve to the left or up?. As outlined in Section 6.4, current evidence in the open literature for emergent capabilities remains weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.4.">Meta-Search/Search 2</head><p>In this report we argued that advanced reasoning should incorporate in-context search, which can yield higher efficiency and potentially more advanced capabilities in reasoning tasks. However, such an approach also has potential downsides. In particular, we are limited by the model's context length, which induces fundamental limits on the search complexity. Moreover, the sequential nature of in-context search can make the process slow, limiting the power of these models. At the same time neither of these are issues with classical search approaches such as MCTS, which can be parallelized <ref type="bibr" target="#b40">Liu et al. (2020)</ref>. A natural question is: can we can build an additional search procedure on top of an advanced reasoning model (a process which we call Meta-Search or Search 2 following the naming convention of Duan et al. ( <ref type="formula">2016</ref>))? In recent literature, Anonymous (2024) trained a value function (PRM) with a "multi-turn" approach, conditioning the value function on all prior explored solutions. Rather than the traditional value function, they instead train a function 𝑣 𝜃 (Z t , S 𝑡 , q), where the Meta-CoT Z 𝑡 consists of prior solution attempts. The empirical performance of their method compared with regular PRM training is shown in Figure <ref type="figure" target="#fig_27">30</ref>, demonstrating improved scaling over traditional value function training. This result suggests that we may be able to increase the search efficiency using a meta-critic, however this remains an open empirical question. If this is indeed the case, it would allow us to also massively scale online search with reasoning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.5.">Reasoning with External Tools</head><formula xml:id="formula_80">2 0 2 1 2 2 2 3 2 4 2 5 2 6</formula><p>Number of samples Building on our discussion of scaling laws and search approaches, we investigate a critical direction: augmenting model reasoning with external computational tools. Just as humans leverage calculators, spreadsheets, and specialized software to solve problems more efficiently, LLMs can potentially achieve better scaling properties by offloading the compute burden to external toolsrequiring less training data during SFT and fewer samples during test-time search to reach the high level of performance.</p><p>For instance, while pure CoT reasoning requires models to perform all calculations internally to solve math problems, these computations can be offloaded to a Python interpreter. Prior works demonstrate that such tool-integrated reasoning (TIR) improves performance on mathematical questions <ref type="bibr" target="#b38">(LI et al., 2024;</ref><ref type="bibr" target="#b90">Yin et al., 2024;</ref><ref type="bibr" target="#b7">Chen et al., 2022)</ref>. However, the scaling properties of TIR remain unexplored.</p><p>Figure <ref type="figure" target="#fig_28">31</ref>, using models trained by LI et al. ( <ref type="formula">2024</ref>), provides initial evidence for the efficiency of TIR models. Despite being fine-tuned on 25% as much data, the TIR model demonstrates superior scaling properties compared to the CoT model. This occurs across all sample sizes, with both majority vote and Best-of-N strategies consistently outperforming their CoT counterparts. The efficiency gain is particularly evident in the low-sample regime (2 0 to 2 3 samples), where TIR achieves nearly double the accuracy of CoT methods. This suggests that offloading computations to external tools significantly improves the model's ability to solve problems even with limited attempts, and TIR is more efficient at both training and inference time.</p><p>Based on these promising initial results, we identify several critical directions for future investigation:</p><p>1. Scaling laws and search strategies: We need to systematically study the scaling properties of different search methods (e.g., BFS, DFS, 𝐴 *<ref type="foot" target="#foot_4">foot_4</ref> , MCTS) in Tool-Integrated Reasoning models. 2. Verification scaling: The role and scaling properties of the verifier in TIR settings remains under-explored -what is the relation between training data for the verifier, verifier accuracy, and policy accuracy? 3. Internal reasoning vs reasoning with external tools: While our initial results show TIR's efficiency advantages, we need to better understand the fundamental trade-offs between these approaches. When does offloading computation to external tools provide the most benefit compared to pure language reasoning? How do these trade-offs change with model scale and problem complexity?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this position paper, we have introduced Meta Chain-of-Thought (Meta-CoT) as a framework for understanding and enhancing the reasoning capabilities of Large Language Models (LLMs). We have argued that traditional Chain-of-Thought does not fully represent the underlying data generative process on reasoning problems. By incorporating the concepts of search, verification, and iterative refinement, Meta-CoT provides a more complete model of the cognitive processes required for advanced problem-solving.</p><p>We believe that Meta-CoT represents a promising path towards more robust and generalizable reasoning in LLMs. The observed behaviors of state-of-the-art models, along with our experiments on in-context exploration and backtracking, lend support to the hypothesis that internal search processes are crucial for performance on complex tasks. Furthermore, the proposed training pipeline presents a concrete approach for developing LLMs with enhanced Meta-CoT capabilities. Future work should validate the efficacy of our proposed pipeline.</p><p>Beyond our proposed method, numerous open questions and challenges remain. Further research is needed to determine the optimal scaling laws for reasoning and search, to develop more effective process supervision and verification techniques, and to clarify the interplay between instruction tuning and reinforcement learning for meta chain of thought reasoning. The "Big MATH" dataset we have introduced aims to support this research by providing a large-scale resource for training reasoning models. Furthermore, the possibility of meta-RL paired with Meta-CoT automatically discovering novel search algorithms is a particularly intriguing open research question for future work to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prompting</head><p>We investigate whether carefully constructed prompting protocols can induce reasoning capabilities similar to those demonstrated by RL-based models like O1. Through an evaluation of five increasingly sophisticated instruction sets, we analyze token generation patterns, presence of reasoning-like behaviors, and their correlation with math problem-solving performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regret Analysis</head><p>Table <ref type="table">5</ref> presents detailed statistics on regret expression across different models and prompting strategies. The data shows varying levels of self-correction and error acknowledgment behaviors across model scales and prompting approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Different Instruction Tuning Objectives</head><p>Below we outline a number of potential finetuning objectives for the instruction tuning stage of our reasoning pipeline described in Section 7.1.</p><p>1. A standard procedural-cloning approach <ref type="bibr" target="#b86">(Yang et al., 2022)</ref> which uses traditional supervised fine-tuning</p><formula xml:id="formula_81">ℒ(𝜃) = min 𝜃 -E (q,Z,S)∼𝒟 train ⎡ ⎣ |Z| ∑︁ 𝑖=1 log 𝜋 𝜃 (z 𝑖+1 |Z 𝑖 , q) + |S| ∑︁ 𝑖=1 log 𝜋 𝜃 (s 𝑖+1 |S 𝑖 , Z, q) ⎤ ⎦ .<label>(22)</label></formula><p>2. Alternatively, we can optimize only over the Meta-CoT tokens with the following optimization objective</p><formula xml:id="formula_82">ℒ(𝜃) = min 𝜃 -E (q,Z,S)∼𝒟 train ⎡ ⎣ |Z| ∑︁ 𝑖=1 log 𝜋 𝜃 (z 𝑖+1 |Z 𝑖 , q) ⎤ ⎦ .<label>(23)</label></formula><p>In this formulation, the model will not learn to generate a solution, requiring the use of a separate step to summarize the search process into a final solution. One hypothesis is that training on joint sequences with the solution can serve as additional supervision to help the model with maintaining internal state. 3. One consideration is whether updating model parameters for sub-optimal, or even incorrect, branches can induce the model to generate more errors. To handle this, we can mask incorrect steps/branches in the above loss</p><formula xml:id="formula_83">ℒ(𝜃) = min 𝜃 -E (q,Z,S)∼𝒟 train ⎡ ⎣ |Z| ∑︁ 𝑖=1 𝐼{z 𝑖+1 ∈ S} log 𝜋 𝜃 (z 𝑖+1 |Z 𝑖 , q) ⎤ ⎦ .<label>(24)</label></formula><p>That is we only train on the branches that are on the correct path from the root node (problem)</p><p>to the final answer. Prior works <ref type="bibr" target="#b14">(Gandhi et al., 2024;</ref><ref type="bibr">Ye et al., 2024b)</ref> did not mask the incorrect steps, and reported no degradation in performance, as long as the data distribution remains reasonable. On the other hand <ref type="bibr">Zhang et al. (2024b)</ref> specifically mask the tokens of the unsafe generation. 4. Under the above objective, there could be significant task mismatch between training and generation, especially in the case of long search chains. One method to mitigate this issue is to generate more synthetic training data by pruning intermediate branches not on the optimal solution path to generate additional sequences. For example this was the SFT objective used by <ref type="bibr">Zhang et al. (2024b)</ref>, which simultaneously trains on backtracking trajectories (with masking) and the final optimal solution (Equation 1 in that paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MCTS Details</head><p>Here we outline the details of our MCTS procedure from Section 4.3.1. Our procedure involves three main steps: selection, backup, and expansion. We describe each step in detail below.</p><p>Selection. Starting at the root node s 0 (initialized as the question q), a child node s (i.e. the next step in the solution) is selected until a leaf node s 𝑡 is reached. A partial solution is then represented as S 𝑡 = (s 1 , . . . , s 𝑡 ). The child node is selected according to the policy s 𝑡+1 = arg max s 𝑈 (S 𝑡 , s) where 𝑈 (S 𝑡 , s) is calculated using UCT <ref type="bibr" target="#b31">(Kocsis &amp; Szepesvári, 2006)</ref>, defined as</p><formula xml:id="formula_84">𝑈 (S 𝑡 , s) = 𝑄(S 𝑡 , s) + 𝑐 exp √︃ log 𝑁 (S 𝑡 , s) 𝑁 (S 𝑡 ) .</formula><p>In this equation 𝑄(S 𝑡 , s) is a value function, 𝑁 (S 𝑡 , s) is the visit count of selecting step s from the partial solution S 𝑡 , 𝑁 (S 𝑡 ) = ∑︀ 𝑠 𝑁 (S 𝑡 , s) is the total visit count of the partial solution S 𝑡 , and 𝑐 exp ∈ R is the exploration constant. We opt for the UCT formulation, instead of the more recent PUCT variant <ref type="bibr" target="#b62">(Rosin, 2011)</ref>, as in our case actions are represented by logical steps and thus likelihoods could be quite skewed.</p><p>Expansion. After the selection operation, we have chosen a leaf node s 𝑡 , and the path from the root node s 0 to the leaf node forms a partial solution by concatenating the individual steps into the partial solution, i.e. S 𝑡 = (s 1 , . . . , s 𝑡 ). If s 𝑡 is terminal, we do not expand it, otherwise the node is expanded by sampling 𝑏 actions {s 𝑖 𝑡+1 } 𝑏 𝑖=1 ∼ 𝜋 𝜃 (•|S 𝑡 ) from the policy 𝜋 𝜃 , and then adding the nodes {a 𝑖 𝑡+1 } 𝑏 𝑖=1 as children of node S 𝑡 . The value of each child node is initialized with a value function 𝑣 as 𝑄(S 𝑡 , s 𝑖 𝑡 ) = 𝑣((S 𝑡 , s 𝑖 𝑡+1 ), q). This evaluation essentially replaces the "simulation" step usually carried out in game settings.</p><p>Backup. Finally, the statistics of the expanded node S 𝑡 are backed up through the path to the root node s 0 . For every ancestor node S 𝑖 on the path, the visit count is incremented</p><formula xml:id="formula_85">𝑁 (S 𝑖 ) = 𝑁 (S 𝑖 ) + 1</formula><p>and the value of each ancestor is updated as 𝑄(S 𝑖 , s 𝑖+1 ) = 𝑄(S 𝑖 , s 𝑖+1 ) + 𝑣(S 𝑡 , q) -𝑄(S 𝑖 , s 𝑖+1 ) 𝑁 (S 𝑖 , s 𝑖+1 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Chains-Of-Thought</head><p>This section contains synthetic CoT traces comparing how different models reason through the same planning problems. Specifically, we show complete reasoning traces from Gemini-1.5 solving problems using A* and MCTS algorithms, alongside parallel traces from DeepSeek-R1, DeepSeek-O1, and Gemini-2 working through identical tasks. We also include an example trace demonstrating how a smaller SFT-tuned Llama3.1-8B model performs tool-augmented reasoning on a challenging problem using dynamic programming. Each trace details the step-by-step "internal" traces and the final solution.</p><p>There were 12 seats prepared for the jury at a round table, each with a name indicated on it. Nikolai Nikolaevich, who arrived first, absent-mindedly sat not in his designated seat but in the next one clockwise. Each jury member arriving at the table afterward would either sit in their designated seat, or, if it was already occupied, they would walk around the table clockwise and sit in the next available seat. The resulting seating arrangement of the jury members depends on the order in which they approached the table. How many different ways can the jury be seated? 0.0</p><p>Let's break this down step-by-step: 2. The second person can either sit in their designated seat or move to the next seat clockwise. This gives 2 options for the second person.</p><p>3. The third person can either sit in their designated seat or move to the next seat clockwise. This gives 2 options for the third person, but we need to consider the case where the second person moved, so there's only 1 additional option for the third person. 4. Similarly, for each subsequent person, there are 2 options if the previous person didn't move, and 1 additional option if the previous person moved. Wait, however, this process can be simplified by realizing that each person has a unique "decision point" -the first time they have a choice between sitting in their designated seat and moving to the next seat. After that, their seat is determined by the previous choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0078125</head><p>Alternatively, however, this is a circular arrangement, and we need to account for the fact that the last person can also move. This introduces a complication, as the last person will always have only 1 option if everyone else is seated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0546875</head><p>Let's simplify the problem by considering the first 12 people. For each of these 12 people, there are 2 options. However, we need to subtract the cases where the last person doesn't move, as these cases are already counted in the 2 options for each person.  </p><formula xml:id="formula_86">[!12 = 12! \left( 1 -\frac{1}{1!} + \frac{1}{2!} -\frac{1}{3!} + \ cdots + \frac{(-1)^{12}}{12!} \right) \] 0.0078125</formula><p>Alternatively, but this is still not correct because it doesn't account for the fact that the last person always has only 1 option. Let's reconsider the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0625</head><p>Alternatively, but this is not the correct answer because the problem statement asks for the number of different ways the jury can be seated, considering the circular arrangement. In a circular arrangement, the last person's position is fixed once the first person's position is chosen. Therefore, the correct calculation should consider the number of ways to arrange the remaining 11 people around the table, not the total possible arrangements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1484375</head><p>Alternatively, but we're not done yet! We need to consider the case where the second person moves, which would cause the last person to have only 1 option. This introduces an overcount. To correct for this, we need to subtract the number of arrangements where the second person moves, which is equivalent to the number of ways the jury can be seated with one person missing (i.e., 11 people). This is: \[ 2^{10} -1 \] 0.1796875 But we still need to account for the fact that the last person has only 1 option. This means that we've overcounted by a factor of 2 for the last person. To correct this, we divide by 2 again: \[ \frac{2^{10}}{2} = 2^9 \] 0.0390625</p><p>Wait, but we also need to consider the case where the first person moves, which would be counted 12 times (once for each possible position). So, we need to subtract these 11 arrangements: \[ 2^{10} -11 \] 0.1484375 Alternatively, this is still not correct, as we haven't accounted for the fact that the last person can only move one seat. We need to consider the cases where the last person moves and subtract those from the total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1796875</head><p>But we need to correct this approach because we've double-counted due to the nature of the circular arrangement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.015625</head><p>Alternatively, we also need to consider the case where someone arrives after the first person and the table is empty. In this case, the first person still has 1 option, and the remaining 11 people have 2 options each, but again, we need to divide by 2 to account for the circular arrangement: \[ \frac{1 \times 2^{11}}{2} = 2^{10} \] 0.2421875 However, this is not the correct answer. The correct answer is: \[ \boxed{2^{11}} \] 0.0 Alternatively, but this doesn't account for the fact that the last person has only 1 option. To adjust for this, we need to consider the cases where the last person moves. This introduces a more complex conditional arrangement, but the key insight is that the number of choices for the last person is actually the same as the number of choices for any other person, because the arrangement is circular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1015625</head><p>Alternatively, but we still need to account for the fact that the last person can only move in one direction. This doesn't affect the number of arrangements, as the circular constraint is already accounted for by the division by 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.953125</head><p>So, the total number of different ways the jury can be seated is:</p><formula xml:id="formula_87">\[ \boxed{1024} \]</formula><p>1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution:</head><p>Let's break this down step-by-step:</p><p>1. The first person, Nikolai Nikolaevich, has a fixed position. 2. The second person can either sit in their designated seat or move to the next seat clockwise. This gives 2 options for the second person. 3. The third person can either sit in their designated seat or move to the next seat clockwise. This gives 2 options for the third person, but we need to consider the case where the second person moved, so there's only 1 additional option for the third person. 4. Similarly, for each subsequent person, there are 2 options if the previous person didn't move, and 1 additional option if the previous person moved. However, this is a circular arrangement, and we need to account for the fact that the last person can also move. This introduces a complication, as the last person will always have only 1 option if everyone else is seated. Let's consider the total number of ways without the circular constraint. For the first person, there's 1 option. For the rest, there are 2 options each, except for the last person who has only 1 option. So, the total number of arrangements without the circular constraint is:</p><formula xml:id="formula_88">1 × 2 11</formula><p>However, this counts each arrangement twice (once in each direction around the table), so we need to divide by 2:</p><formula xml:id="formula_89">1 × 2 11 2 = 2 10</formula><p>But we still need to account for the fact that the last person can only move in one direction. This doesn't affect the number of arrangements, as the circular constraint is already accounted for by the division by 2. So, the total number of different ways the jury can be seated is:</p><p>1024</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trace -Reasoning with External Tools</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A* Generated Solution Trace</head><p>Question: There were 12 seats prepared for the jury at a round table, each with a name indicated on it. Nikolai Nikolaevich, who arrived first, absent-mindedly sat not in his designated seat but in the next one clockwise. Each jury member arriving at the table afterward would either sit in their designated seat, or, if it was already occupied, they would walk around the table clockwise and sit in the next available seat. The resulting seating arrangement of the jury members depends on the order in which they approached the table. How many different ways can the jury be seated?</p><p>To solve this problem, we need to determine the total number of different seating arrangements of 12 jury members when each member either sits in their designated seat or walks around the table clockwise to sit in the next available seat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.01171875</head><p>To solve this problem, we need to analyze the seating possibilities given the rules described: 0.0078125 1. Nikolai Nikolaevich starts in the next seat clockwise. 2. Each subsequent member either sits in their designated seat or the next available seat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.015625</head><p>To determine the number of ways the jury can be seated, we need to consider the following steps: 0.01171875 1. Assign Nikolai Nikolaevich to the next seat clockwise. 2. Each subsequent jury member has 2 choices: their designated seat or the next available seat. 3. Since Nikolai Nikolaeich has already changed seats, we need to account for the extra step he took.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.046875</head><p>Let's represent this using Python code to calculate the number of different ways the jury can be seated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.05078125</head><p>We can model this problem using dynamic programming, where each state represents the number of available seats and the current seat Nikolai is sitting on. We'll use a recursive approach to count the number of valid seating arrangements. It seems our initial approach overlooked a crucial detail. We need to consider the actual sequence of each member. Each member can either:</p><p>1. Sit in their designated seat.</p><p>2. Walk to the next available seat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.04296875</head><p>It seems there was an issue with the initial state of the dynamic programming approach. Let's refine our approach: The number of different ways the jury can be seated, given that Nikolai Nikolaeich started in the next seat clockwise and each subsequent member either sits in their designated seat or the next available seat, is \(\boxed{1024}\). 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution:</head><p>To solve this problem, we need to analyze the seating possibilities given the rules described: 1. Nikolai Nikolaevich starts in the next seat clockwise. 2. Each subsequent member either sits in their designated seat or the next available seat. To determine the number of ways the jury can be seated, we need to consider the following steps:</p><p>1. Assign Nikolai Nikolaevich to the next seat clockwise. 2. Each subsequent jury member has 2 choices: their designated seat or the next available seat. 3. Since Nikolai Nikolaeich has already changed seats, we need to account for the extra step he took. We can model this problem using dynamic programming, where each state represents the number of available seats and the current seat Nikolai is sitting on. We'll use a recursive approach to count the number of valid seating arrangements. Let's implement this in Python. ```o utput 0 ``Ì t seems there was an issue with the initial state of the dynamic programming approach. Let's refine our approach: 1. We need to correctly initialize the base case. 2. We need to ensure we correctly handle the initial arrangement where Nikolai is in the next seat clockwise. Here's an updated approach: 1. We start by initializing the base case correctly. 2. We use an iterative approach instead of dynamic programming to simplify the calculation. he number of different ways the jury can be seated, given that Nikolai Nikolaeich started in the next seat clockwise and each subsequent member either sits in their designated seat or the next available seat, is \(\boxed{1024}\).</p><p>Since $n$ is even, let $n = 2m$. Then we have $2n = 4m$. Thus, $r(x)$ has the roots $\pm 1, \pm 2, \ldots, \pm 2m$.</p><formula xml:id="formula_90">0.0703125 $r(x) = (x-1)(x+1)(x-2)(x+2) \cdots (x-2m)(x+2m) = \prod_{k=1}^{2m} (x- k)(x+k) = \prod_{k=1}^{2m} (x^2-k^2)$.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.015625</head><p>Since $q(x)$ is a polynomial of degree $2n$, $r(x)$ is a monic polynomial of degree $2n+2 = 4m+2$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0078125</head><p>Therefore, we can write 0.0703125</p><formula xml:id="formula_91">$$ r(x) = (x^2-1)(x^2-2^2)\cdots(x^2 -(2m)^2) = (x^2-1)(x^2-4)\cdots(x ^2 -4m^2) $$ 0.0</formula><p>Therefore, we can write $r(x) = x^{2n+2} -q(x) = (x^2-1)(x^2-4)\cdots( x^2-n^2)R(x)$, where $R(x)$ is a polynomial of degree $2n+2 -2n = 2$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1015625</head><p>Since $q(x)$ is monic with degree $2n$, $x^{2n+2} -q(x)$ has leading coefficient 1. Also, the leading coefficient of $(x^2-1)(x^2-4)\ cdots(x^2-n^2)$ is 1. Thus, $R(x)$ must be monic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.03125</head><p>Let $R(x) = x^2 + bx + c$. Then we have $r(x) = x^{2n+2} -q(x) = (x ^2-1)(x^2-4)\cdots(x^2-n^2)(x^2 + bx + c)$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1953125</head><p>We are looking for values of $x$ such that $p\left(\frac{1}{x}\right) = x^2$, which is equivalent to $q(x) = x^{2n+2}$. This means we want to find the roots of $r(x) = 0$. We know that $r(x) = 0$ for $x = \pm k$ for $1 \leq k \leq n$. We are looking for the roots of $x^2 + bx + c = 0$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.1796875</head><p>Since $r(0) = (-1)^n(-1)(-4)\cdots(-n^2)c = (-1)^n (-1)^n (1)(4)\cdots( n^2)c = n!^2 c$, we have $r(0) = -q(0) = -1$. Thus $n!^2 c = -1$, so $c = -\frac{1}{n!^2}$.</p><p>0.40625 $r(x) = (x^2-1)(x^2-4)\cdots(x^2-n^2)(x^2+bx-\frac{1}{n!^2})$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.3984375</head><p>Since the coefficients of $p(x)$ are real, the coefficients of $q(x)$ are real, and so the coefficients of $r(x)$ are real. Therefore, the coefficients of $R(x)$ are real. In particular, $b$ is real.</p><p>The roots of $x^2+bx-\frac{1}{n!^2}=0$ are $x=\frac{-b \pm \sqrt{b ^2+\frac{4}{n!^2}}}{2}$. Thus the roots are real if $b^2+\frac{4}{n !^2} \geq 0$, which is always true since $b$ is real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.265625</head><p>Consider $r(x) = x^{2n+2} -q(x)$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.359375</head><p>The equation $p(\frac{1}{x}) = x^2$ implies $x^{2n} p(\frac{1}{x}) = x ^{2n+2}$, so $q(x) = x^{2n+2}$, thus $x^{2n+2} -q(x) = 0$. We already know that $x=\pm k$, $1 \le k \le n$, are roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.390625</head><p>We have $r(x) = (x^2-1)\cdots(x^2-n^2)(x^2+bx-\frac{1}{n!^2})$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.359375</head><p>Since $r(x)$ is an even function, $b=0$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.6875</head><p>The other solutions are the roots of $x^2 -\frac{1}{n!^2} = 0$, so $x = \pm \frac{1}{n!}$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.796875</head><p>Final Answer: The final answer is $\boxed{\pm \frac{1}{n!}}$ 1.0</p><formula xml:id="formula_92">Solution Let 𝑞(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀ = 𝑎 0 𝑥 2𝑛 + 𝑎 1 𝑥 2𝑛-1 + • • • + 𝑎 2𝑛-1 𝑥 + 1. The given condition 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2 implies 𝑞(𝑘) = 𝑘 2𝑛 𝑘 2 = 𝑘 2𝑛+2 for 1 ≤ |𝑘| ≤ 𝑛.</formula><p>Consider the polynomial 𝑟(𝑥) = 𝑥 2𝑛+2 -𝑞(𝑥). We are given that 𝑟(𝑘) = 0 for 1 ≤ |𝑘| ≤ 𝑛. Since 𝑛 is even, let 𝑛 = 2𝑚. Then we have 2𝑛 = 4𝑚. Thus, 𝑟(𝑥) has the roots ±1, ±2, . . . , ±2𝑚.</p><formula xml:id="formula_93">𝑟(𝑥) = (𝑥 -1)(𝑥 + 1)(𝑥 -2)(𝑥 + 2) • • • (𝑥 -2𝑚)(𝑥 + 2𝑚) = ∏︀ 2𝑚 𝑘=1 (𝑥 -𝑘)(𝑥 + 𝑘) = ∏︀ 2𝑚 𝑘=1 (𝑥 2 -𝑘 2</formula><p>). Since 𝑞(𝑥) is a polynomial of degree 2𝑛, 𝑟(𝑥) is a monic polynomial of degree 2𝑛 + 2 = 4𝑚 + 2.</p><p>Therefore, we can write 𝑟(𝑥) = 𝑥 2𝑛+2 -𝑞(𝑥) = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )𝑅(𝑥), where 𝑅(𝑥) is a polynomial of degree 2𝑛 + 2 -2𝑛 = 2. Since 𝑞(𝑥) is monic with degree 2𝑛, 𝑥 2𝑛+2 -𝑞(𝑥) has leading coefficient 1. Also, the leading coefficient of (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 ) is 1. Thus, 𝑅(𝑥) must be monic. Let 𝑅(𝑥) = 𝑥 2 + 𝑏𝑥 + 𝑐. Then we have 𝑟</p><formula xml:id="formula_94">(𝑥) = 𝑥 2𝑛+2 -𝑞(𝑥) = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 - 𝑛 2 )(𝑥 2 + 𝑏𝑥 + 𝑐).</formula><p>We are looking for values of 𝑥 such that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 , which is equivalent to 𝑞(𝑥) = 𝑥 2𝑛+2 . This means we want to find the roots of 𝑟(𝑥) = 0. We know that 𝑟(𝑥) = 0 for 𝑥 = ±𝑘 for 1 ≤ 𝑘 ≤ 𝑛. We are looking for the roots of</p><formula xml:id="formula_95">𝑥 2 + 𝑏𝑥 + 𝑐 = 0. Since 𝑟(0) = (-1) 𝑛 (-1)(-4) • • • (-𝑛 2 )𝑐 = (-1) 𝑛 (-1) 𝑛 (1)(4) • • • (𝑛 2 )𝑐 = 𝑛! 2 𝑐, we have 𝑟(0) = -𝑞(0) = -1. Thus 𝑛! 2 𝑐 = -1, so 𝑐 = -1 𝑛! 2 .</formula><p>Since the coefficients of 𝑝(𝑥) are real, the coefficients of 𝑞(𝑥) are real, and so the coefficients of 𝑟(𝑥) are real. Therefore, the coefficients of 𝑅(𝑥) are real. In particular,</p><formula xml:id="formula_96">𝑏 is real. The roots of 𝑥 2 + 𝑏𝑥 -1 𝑛! 2 = 0 are 𝑥 = -𝑏± √︁ 𝑏 2 + 4 𝑛! 2 2</formula><p>. Thus the roots are real if</p><formula xml:id="formula_97">𝑏 2 + 4 𝑛! 2 ≥ 0, which is always true since 𝑏 is real. The equation 𝑝( 1 𝑥 ) = 𝑥 2 implies 𝑥 2𝑛 𝑝( 1 𝑥 ) = 𝑥 2𝑛+2</formula><p>, so 𝑞(𝑥) = 𝑥 2𝑛+2 , thus 𝑥 2𝑛+2 -𝑞(𝑥) = 0. We already know that 𝑥 = ±𝑘, 1 ≤ 𝑘 ≤ 𝑛, are roots. We have</p><formula xml:id="formula_98">𝑟(𝑥) = (𝑥 2 -1) • • • (𝑥 2 -𝑛 2 )(𝑥 2 + 𝑏𝑥 -1 𝑛! 2 ). Since 𝑟(𝑥) is an even function, 𝑏 = 0.</formula><p>The other solutions are the roots of 𝑥 2 -1 𝑛! 2 = 0, so 𝑥 = ± 1 𝑛! . Final Answer: The final answer is ± 1 𝑛! Since $p(1/k) = k^2$ for $1 \le |k| \le n$, we have $q(k) = 0$ for $1 \ le |k| \le n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.21875</head><p>We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \ leq n$, which means $q(k) = p\left(\frac{1}{k}\right) -k^2 = 0$ for $1 \leq |k| \leq n$. Thus, $r(x)$ has roots at $x = k$ for $1 \ leq |k| \leq n$. Since $r(x)$ is a polynomial of degree $2n+2$, we can write 0.25</p><p>We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \ leq n$, which means $q(k) = p\left(\frac{1}{k}\right) -k^2 = 0$ for $1 \leq |k| \leq n$. Thus, the integers $k = \pm 1, \pm 2, \ ldots, \pm n$ are roots of the polynomial $r(x) = 1 + a_{2n-1}x + \ cdots + a_1x^{2n-1} + a_0x^{2n} -x^{2n+2}$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.3125</head><p>Since $r(x)$ is a polynomial of degree $2n+2$, and we know $2n$ distinct roots, we can write 0.1875 Since $r(x)$ has degree $2n+2$ and has roots $\pm 1, \pm 2, \ldots, \pm n$, there are $2n$ such roots. Thus, we can write 0.375</p><p>Since $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \leq n$, we have $q(k) = 0$ for $1 \leq |k| \leq n$. Thus, the numerator of $q( x)$, which we denote by $r(x)$, has roots $k$ where $1 \leq |k| \ leq n$. Since $n$ is even, we have $2n$ such values of $k$. Also, since $p$ has real coefficients, if $k$ is a root, so is $-k$. Since $r(x)$ has degree $2n+2$, we can write 0.15625 Let $q(x) = x^{2n}p\left(\frac{1}{x}\right)$. Then $q(x) = 1 + a_{2n-1}</p><p>x + a_{2n-2}x^2 + \dots + a_1x^{2n-1} + a_0x^{2n}$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.03125</head><p>Thus, $r(x)$ is a polynomial of degree $2n+2$ with roots $k$ for $1 \ leq |k| \leq n$. Since there are $2n$ such values of $k$, we can write 0.3125</p><p>We are given that $p(x)$ is a monic real polynomial of degree $2n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0625</head><p>Since $p\left(\frac{1}{k}\right) = k^2$ for all integers $k$ such that $1 \leq |k| \leq n$, we have $q(k) = p\left(\frac{1}{k}\right) -k ^2 = 0$ for $1 \leq |k| \leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.34375</head><p>Thus, $q(x)$ has $2n$ distinct roots $k$ where $1 \le |k| \le n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.15625</head><p>Thus, $q(x)$ has roots $k$ for $1 \leq |k| \leq n$. These are $2n$ distinct integer roots. Since the numerator of $q(x)$ has degree $2n+2$, we can write 0.34375 Thus, $q(x)$ has roots at $k = \pm 1, \pm 2, \ldots, \pm n$. These are $2n$ distinct roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.375</head><p>Since $p(1/k) = k^2$ for $1 \leq |k| \leq n$, we have $q(k) = 0$ for $1 \leq |k| \leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.28125</head><p>Thus, $q(x)$ has roots $k$ for $1 \leq |k| \leq n$. Since $n$ is an even integer, there are $2n$ such values of $k$. We can write 0.125</p><p>Continuing the solution: 0.40625 Since $p$ has degree $2n$, $p(1/x)$ has degree $2n$ in $\frac{1}{x}$, so when we consider $p(1/x) = x^2$, the numerator of $q(x)$ has degree $2n+2$. Thus, $q(x)$ has $2n$ zeros at $\pm 1, \pm 2, \dots, \pm n$. Then the roots of $r(x)$ are precisely the roots of $q(x)$. Since $r(x)$ is a polynomial of degree $2n+2$, and we have found $2n$ distinct roots, we can write 0.40625 Since $q(x) = \frac{r(x)}{x^{2n}}$, the roots of $q(x)$ are the roots of $r(x)$. Thus, $r(x)$ has roots $k$ where $1 \le |k| \le n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.28125</head><p>Let $r(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} -x^{2n +2}$. Then the roots of $q(x)$ are the roots of $r(x)$. Since $r(x) $ is a polynomial of degree $2n+2$, and we know $2n$ roots, we can write 0.5 Since $q(x)$ has the form $\frac{\text{polynomial of degree } 2n+2}{x ^{2n}}$, the numerator of $q(x)$ must have degree $2n+2$.</p><p>0.375</p><formula xml:id="formula_99">$$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(Ax^2 + Bx + C)$$ 0.6875 $$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(cx^2+bx+a) = (x^2-1)(x ^2-2^2)\cdots(x^2-n^2)(cx^2+bx+a).$$ 0.5 $$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(cx^2 + bx + a) = \prod_{k =1}^n (x^2-k^2) (cx^2+bx+a)$$ 0.5625 Since $r(0) = 1$, we must have $(-1)^n (n!)^2 C = 1$, so $C = \frac {1}{(n!)^2}$. 0.9375 Since $r(x) = 1 + a_{2n-1}x + \dots + a_0 x^{2n} -x^{2n+2}$, the coefficient of $x^{2n+2}$ is $-1$. Also, since $r(x) = \prod_{k=1}n (x-k)(x+k) (Ax^2+Bx+C)$, the coefficient of $x^{2n+2}$ is $A$. Thus, $A = -1$. 0.6875 The coefficient of $x^{2n+2}$ in $r(x)$ is $-1$. Also, the coefficient of $x^{2n+2}$ in the expansion of $(x-1)(x+1)\cdots(x-n)(x+n)(Ax^2+ Bx+C)$ is $A$. Thus, $A = -1$. 0.875 Since $r(x) = 1 + a_{2n-1}x + \dots + a_0 x^{2n} -x^{2n+2}$, the coefficient of $x^{2n+2}$ is $-1$. 0.53125</formula><p>Continuing the solution:</p><formula xml:id="formula_100">0.6875 The constant term of $r(x)$ is $1$. The constant term of $\prod_{k=1}^n (x-k)(x+k)(-x^2+Bx+C)$ is $(-1)^n (n!)^2 C$. Therefore, $(-1)^n (n !)^2 C = 1$, so $C = \frac{1}{(-1)^n (n!)^2} = \frac{(-1)^n}{(n!)^2} $.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.65625</head><p>The constant term of $r(x)$ is $1$. The constant term in $\prod_{k=1}^n (x-k)(x+k)(-x^2+Bx+C)$ is $(-1)^n (n!)^2 C$. So, $1 = (-1)^n (n!) ^2 C$, which implies $C = \frac{(-1)^n}{(n!)^2}$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.5625</head><p>The constant term of $r(x)$ is $1$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.5</head><p>The coefficient of $x^{2n}$ in $r(x)$ is $a_0$. The coefficient of $x ^{2n}$ in the factored form is $A -\sum_{k=1}^n k^2$. The coefficient of $x^{2n+1}$ in $r(x)$ is $a_1$, while in the factored form, the coefficient is $B$. The coefficient of $x^{2n+2}$ in $r( x)$ is $-1$, and in the factored form, it is $A$. Therefore, $A = -1$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.71875</head><p>Now, we examine the coefficient of $x^{2n+1}$. In $r(x) = 1 + a_{2n-1}x + \dots -x^{2n+2}$, the coefficient of $x^{2n+1}$ is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.0</head><p>We have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$. Since $r(0) = 1$, we have $1 = \prod_{k=1}^n (-k^2) \cdot C$. Also, $C = \frac{(-1)n }{(n!)^2}$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.5</head><p>We have $r(x) = \prod_{k=1}^n (x^2-k^2)(-x^2+Bx+C)$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.5</head><p>In the expansion of $\prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$, the coefficient of $x^{2n+1}$ is $B \prod_{k=1}^n (-k^2) = B(-1)^n (n!) ^2$. Since this must equal 0, we have $B=0$.</p><p>1.0</p><p>In the expansion of $\prod_{k=1}^n (x^2 -k^2)(-x^2 + Bx + C)$, the coefficient of $x^{2n+1}$ is $B \prod_{k=1}^n (-k^2) = B(-1)^n (n!) ^2 = (-1)^n (n!)^2 B$. Since this must be zero, we must have $B = 0 $.</p><p>1.0 So, we have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!) ^2})$.</p><p>1.0</p><p>In the expression $r(x) = \prod_{k=1}^n (x^2 -k^2) (-x^2 + Bx + C)$, the coefficient of $x^{2n+1}$ is $B$. Thus, $B=0$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.96875</head><p>Since</p><formula xml:id="formula_101">$n$ is even, let $n=2m$. The coefficient of $x^{2n+1}$ in $r(x)$ is $0$. Also, the coefficient of $x^{2n+1}$ in $\prod_{k=1}^{2m} (x- k)(x+k)(-x^2+Bx+C)$ is $B \prod_{k=1}^{2m} k^2 = B (n!)^2 = B((2m) !)^2$. 1.0</formula><p>Since $n$ is even, let $n=2m$. Then the roots are $\pm 1, \pm 2, \ldots , \pm 2m$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.75</head><p>So, $1 = (-1)^n (n!)^2 C$. Since $C = \frac{(-1)^n}{(n!)^2}$, the equation holds. The coefficient of $x$ in $r(x)$ is $a_{2n-1}$. Since $r(x) = \prod_{k=1}^n (x^2 -k^2) (-x^2+Bx+C)$, the coefficient of $x$ is $C$ times the coefficient of $x$ in $\prod_{k =1}^n (x^2 -k^2)$, which is 0. Thus, the coefficient of $x$ in $r( x)$ is 0, so $a_{2n-1} = 0$. 1.0 Thus, we have $r(x) = \prod_{k=1}^n (x^2-k^2)(-x^2 + \frac{(-1)^n}{(n!) ^2})$. Since $n$ is even, let $n=2m$.</p><p>1.0</p><p>We have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)^2})$ . The roots of $r(x)$ other than $\pm 1, \pm 2, ..., \pm n$ satisfy</p><formula xml:id="formula_102">$-x^2 + \frac{(-1)^n}{(n!)^2} = 0$, so $x^2 = \frac{(-1)^n}{(n!)^2} $. 1.0 $$r(x) = \prod_{k=1}^n (x^2 -k^2) (-x^2 + \frac{1}{(n!)^2}) = -x^2 \ prod_{k=1}^n (x^2 -k^2) + \frac{1}{(n!)^2} \prod_{k=1}^n (x^2 -k ^2).$$ 1.0</formula><p>So, $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)^2})$. We are looking for the roots of $r(x) = 0$, other than $\pm 1, \pm 2, \ldots, \pm n$. These must be the roots of $-x^2 + \frac{(-1)^n}{( n!)^2} = 0$, which gives us $x^2 = \frac{(-1)^n}{(n!)^2}$. Since $n$ is an even integer, we have $x^2 = \frac{1}{(n!)^2}$, so \boxed {x = \pm \frac{1}{n!}}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.0</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution</head><formula xml:id="formula_103">: Let 𝑞(𝑥) = 𝑝 (︀ 1 𝑥 )︀ -𝑥 2 . Then 𝑞(𝑥) is a rational function of the form 𝑞(𝑥) = 1 + 𝑎 2𝑛-1 𝑥 + • • • + 𝑎 1 𝑥 2𝑛-1 + 𝑎 0 𝑥 2𝑛 -𝑥 2𝑛+2 𝑥 2𝑛 . Since 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2 for all integers 𝑘 such that 1 ≤ |𝑘| ≤ 𝑛, we have 𝑞(𝑘) = 𝑝 (︀ 1 𝑘 )︀ -𝑘 2 = 0 for 1 ≤ |𝑘| ≤ 𝑛.</formula><p>Thus, 𝑞(𝑥) has roots at 𝑘 = ±1, ±2, . . . , ±𝑛. These are 2𝑛 distinct roots. Let 𝑟</p><formula xml:id="formula_104">(𝑥) = 1 + 𝑎 2𝑛-1 𝑥 + • • • + 𝑎 1 𝑥 2𝑛-1 + 𝑎 0 𝑥 2𝑛 -𝑥 2𝑛+2 .</formula><p>Then the roots of 𝑟(𝑥) are precisely the roots of 𝑞(𝑥). Since 𝑟(𝑥) is a polynomial of degree 2𝑛 + 2, and we have found 2𝑛 distinct roots, we can write</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepSeek R1 Solution Trace Problem</head><p>Let 𝑛 be an even positive integer. Let 𝑝 be a monic, real polynomial of degree 2𝑛; that is to say,𝑝(𝑥) = 𝑥 2𝑛 + 𝑎 2𝑛-1 𝑥 2𝑛-1 + • • • + 𝑎 1 𝑥 + 𝑎 0 for some real coefficients 𝑎 0 , . . . , 𝑎 </p><formula xml:id="formula_105">𝑝(𝑥) = 𝑥 2𝑛 + 𝑎 2𝑛-1 𝑥 2𝑛-1 + • • • + 𝑎 1 𝑥 + 𝑎 0</formula><p>for some real coefficients 𝑎 0 , 𝑎 1 , ..., 𝑎 2𝑛-1 the condition given is that 𝑝( 1 𝑘 ) = 𝑘 2 for all integers k such that 1 ≤ |𝑘| ≤ 𝑛. so, k takes integer values from -n to -1 and from 1 to n, excluding zero because 1 𝑘 would be undefined. i need to find all other real numbers for which 𝑝( 1 𝑥 ) = 𝑥 2 okay, so first, i need to understand what is being asked. we have a polynomial of degree 2n, and it satisfies the equation 𝑝( <ref type="formula" target="#formula_3">1</ref>𝑘 ) = 𝑘 2 for these specific values of k. i need to find all real x (other than the ones already specified, i assume) that satisfy 𝑝( 1 𝑥 ) = 𝑥 2 . one thing that comes to mind is that since 𝑝(𝑥) is a polynomial of degree 2n, and we have 2n conditions (because k ranges from -n to 𝑛, excluding zero, which gives 2n points), this should determine the polynomial uniquely. that is, there should be only one such polynomial 𝑝(𝑥) that satisfies these conditions. but the question is not asking for 𝑝(𝑥); it's asking for all real numbers such that 𝑝( 1 𝑥 ) = 𝑥 2 given that 𝑝( 1 𝑘 ) = 𝑘 2 for those specific k. so, perhaps i should consider the equation 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ -𝑥 2 = 0 and see for which real 𝑥 this holds, beyond the already given 𝑥 = 𝑘 for 1 ≤ |𝑘| ≤ 𝑛. but wait, actually, in the given condition, it's 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 , which can be rewritten as  </p><formula xml:id="formula_106">𝑝 (︀ 1 𝑘 )︀ - (︀ 1 𝑘 )︀ -2 = 0, since 𝑥 2 = (︀ 1 𝑘 )︀ -2 when 𝑥 = 1 𝑘 . hmm, not sure if that helps directly. 𝑝 (︂ 1 𝑘 )︂ - (︂<label>1</label></formula><formula xml:id="formula_107">)︀ - (︀ 1 𝑘 )︀ -2 = 0, since 𝑥 2 = (︀ 1 𝑘 )︀ -2 when 𝑥 = 1 𝑘 . hmm, not sure if that helps directly. 𝑝 (︂ 1 𝑘 )︂ - (︂<label>1</label></formula><formula xml:id="formula_108">𝑞(𝑥) = 𝑝 (︂ 1 𝑥 )︂ -𝑥 2</formula><p>i need to find the real roots of 𝑞(𝑥), i.e., the real 𝑥 such that 𝑞(𝑥) = 0.</p><p>given that 𝑝</p><formula xml:id="formula_109">(︀ 1 𝑘 )︀ = 𝑘 2 for 1 ≤ |𝑘| ≤ 𝑛, that means that for each such 𝑘, 𝑞(𝑘) = 𝑝 (︀ 1 𝑘 )︀ -𝑘 2 = 0. so, 𝑘 are roots of 𝑞(𝑥). but 𝑞(𝑥) is 𝑝 (︀ 1 𝑥 )︀ -𝑥 2 , and 𝑝(𝑥) is a degree 2𝑛 polynomial, so 𝑝 (︀<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ is a rational function, not a polynomial, unless we multiply by 𝑥 2𝑛 to clear denominators. so,</p><formula xml:id="formula_110">𝑥 2𝑛 𝑞(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀ -𝑥 2𝑛+2 = (︀ 1 + 𝑎 2𝑛-1 𝑥 + • • • + 𝑎 0 𝑥 2𝑛 )︀ -𝑥 2𝑛+2</formula><p>, which is a degree 2𝑛 + 2 polynomial if 𝑥 2𝑛+2 is the highest degree term. wait, but 𝑥 2𝑛+2 is degree 2𝑛 + 2 and the other terms are up to degree 2𝑛, so yes, the highest degree is 2𝑛 + 2. now, the roots of 𝑞(𝑥) are the roots of 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ -𝑥 2 = 0, which are the 𝑥 for which 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 . we know that for 𝑥 = 𝑘 where 1 ≤ |𝑘| ≤ 𝑛, this holds, so these 𝑥 are roots of 𝑞(𝑥). but 𝑞(𝑥) is a rational function, while 𝑥 2𝑛 𝑞(𝑥) is a polynomial of degree 2𝑛 + 2, which has roots at 𝑥 = 𝑘 for 1 ≤ |𝑘| ≤ 𝑛, assuming that 𝑥 2𝑛 𝑞(𝑥) has these roots. wait, actually, since 𝑞(𝑘) = 0, then 𝑥 2𝑛 𝑞(𝑥) would have roots at 𝑥 = 𝑘, but 𝑥 2𝑛 𝑞(𝑥) is a polynomial of degree 2𝑛 + 2, so it should have 2𝑛 + 2 roots (counting multiplicities) in the complex plane, but we are interested in real roots. but perhaps there is a better approach. let me consider defining a new polynomial that incorporates the given conditions. given that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 1 ≤ |𝑘| ≤ 𝑛, and 𝑝(𝑥) is a monic polynomial of degree 2𝑛, perhaps i can express 𝑝(𝑥) -1 𝑥 2 in terms of its roots. wait, but 𝑝(𝑥) is defined for all real 𝑥, and 1 𝑥 2 is also defined for 𝑥 ̸ = 0. alternatively, maybe i can consider the function 𝑝(𝑥) - </p><formula xml:id="formula_111">(𝑘) = 0 for 𝑘 = ±1, ±2, . . . , ±𝑛, because 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2 , so 𝑞(𝑘) = 𝑘 2𝑛 𝑝 (︀ 1 𝑘 )︀ - 𝑘 2𝑛+2 = 𝑘 2𝑛 • 𝑘 2 -𝑘 2𝑛+2 = 𝑘 2𝑛+2 -𝑘 2𝑛+2 = 0.</formula><p>therefore, 𝑞(𝑥) has 2𝑛 roots at 𝑥 = ±1, ±2, . . . , ±𝑛. since 𝑞(𝑥) is a degree 2𝑛 + 2 polynomial, there are two more roots beyond these 2𝑛 roots. so, 𝑞(𝑥) can be factored as:</p><formula xml:id="formula_112">𝑞(𝑥) = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 ) • (𝑥 2 -𝑎)</formula><p>for some constant 𝑎, because the roots come in pairs 𝑥 = 𝑘 and 𝑥 = -𝑘, and possibly another pair 𝑥 = √ 𝑎 and 𝑥 = -√ 𝑎. wait, but 𝑞(𝑥) is degree 2𝑛 + 2, and (𝑥 2 -𝑘 2 ) for 𝑘 = 1 to 𝑛 gives 𝑛 quadratic factors, which is degree 2𝑛. so, there are two more roots to account for. alternatively, perhaps 𝑞</p><formula xml:id="formula_113">(𝑥) = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 ) • (𝑥 2 -𝑎)(𝑥 2 -𝑏),</formula><p>where 𝑎 and 𝑏 are the remaining roots. but, to determine 𝑎 and 𝑏, i need more information about 𝑞(𝑥). alternatively, perhaps i can look at the leading coefficient. since 𝑞</p><formula xml:id="formula_114">(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀ -𝑥 2𝑛+2 , and 𝑝 (︀ 1 𝑥 )︀ = 𝑥 -2𝑛 + 𝑎 2𝑛-1 𝑥 -2𝑛+1 + • • • + 𝑎 0 , then 𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀ = 1+ 𝑎 2𝑛-1 𝑥 + • • • + 𝑎 0 𝑥 2𝑛 , so 𝑞(𝑥) = 1 + 𝑎 2𝑛-1 𝑥 + • • • + 𝑎 0 𝑥 2𝑛 -𝑥 2𝑛+2 . the leading term is -𝑥 2𝑛+2 + 𝑎 0 𝑥 2𝑛 but since 𝑝(𝑥) is monic, 𝑎 0 is just a constant.</formula><p>wait, perhaps there's a better approach. maybe i can consider the reciprocal polynomial or something like that. alternatively, since 𝑝(𝑥) is a polynomial of degree 2𝑛, and i have 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 𝑘 = ±1, ±2, . . . , ±𝑛, perhaps i can consider defining a new polynomial that has roots at 𝑥 = 1 𝑘 , but that seems off-track. let me try to think about the problem differently. suppose i define a new variable 𝑦 = 1 𝑥 , then, the equation 𝑝</p><formula xml:id="formula_115">(︀ 1 𝑥 )︀ = 𝑥 2 becomes 𝑝(𝑦) = (︁ 1 𝑦 )︁ 2 = 1 𝑦 2 , assuming 𝑦 ̸ = 0. so, 𝑝(𝑦) = 1 𝑦 2 , or 𝑝(𝑦)𝑦 2 = 1.</formula><p>so, 𝑝(𝑦)𝑦 2 -1 = 0. now, 𝑝(𝑦)𝑦 2 -1 is a polynomial of degree 2𝑛 + 2, since 𝑝(𝑦) is degree 2𝑛 and multiplied by 𝑦 2 .</p><p>given that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 , for 𝑘 = ±1, ±2, . . . , ±𝑛, that means 𝑝(𝑦) = 1 𝑦 2 at 𝑦 = 1 𝑘 , so 𝑝(𝑦)𝑦 2 -1 = 0 at 𝑦 = 1 𝑘 . therefore, 𝑦 = 1 𝑘 for 𝑘 = ±1, ±2, . . . , ±𝑛 are roots of 𝑝(𝑦)𝑦 2 -1 = 0. but 𝑝(𝑦)𝑦 2 -1 is a degree 2𝑛 + 2 polynomial, so there are 2𝑛 + 2 roots in total (counting multiplicities). we already have 2𝑛 roots at 𝑦 = 1 𝑘 for 𝑘 = ±1, ±2, . . . , ±𝑛, so there are two more roots that i need to find. let me denote these additional roots as 𝑦 = 𝛼 and 𝑦 = 𝛽, so:</p><formula xml:id="formula_116">𝑝(𝑦)𝑦 2 -1 = (︂ 𝑦 -<label>1 1</label></formula><p>)︂ (︂ 𝑦 + 1 1</p><formula xml:id="formula_117">)︂ • • • (︂ 𝑦 - 1 𝑛 )︂ (︂ 𝑦 + 1 𝑛 )︂ (𝑦 -𝛼)(𝑦 -𝛽)</formula><p>but since 𝑝(𝑦)𝑦 2 -1 is a polynomial of degree 2𝑛 + 2, and we have 2𝑛 roots from 𝑦 = ± 1 𝑘 , there must be two more roots, say 𝑦 = 𝛼 and 𝑦 = 𝛽. now, to find 𝛼 and 𝛽, perhaps i can look at the behavior of 𝑝(𝑦)𝑦 2 -1 as 𝑦 approaches infinity. the leading term of 𝑝(𝑦)𝑦 2 is 𝑦 2𝑛 • 𝑦 2 = 𝑦 2𝑛+2 , and the constant term is -1, so as 𝑦 → ∞, 𝑝(𝑦)𝑦 2 -1 ≈ 𝑦 2𝑛+2 , which goes to infinity. similarly, as 𝑦 → 0, 𝑝(𝑦)𝑦 2 -1 ≈ 𝑎 0 𝑦 2 -1, which approaches -1. but i'm not sure if that helps directly. alternatively, perhaps i can consider that 𝑝(𝑦)𝑦 2 -1 has a double root at 𝑦 = 0, but that seems unlikely. wait, let's consider the multiplicity of roots. given that 𝑝(𝑦)𝑦 2 -1 = 0 has roots at 𝑦 = ± 1 𝑘 for 𝑘 = 1 to 𝑛, and two additional roots 𝑦 = 𝛼 and 𝑦 = 𝛽, i need to determine what 𝛼 and 𝛽 are. alternatively, perhaps i can consider expressing 𝑝(𝑦)𝑦 2 -1 in terms of its roots. but that seems complicated. maybe there's a better way to approach this problem. let me try to consider the function 𝑝 (︀ )︀ = 𝑘 2𝑛 • 𝑘 2 = 𝑘 2𝑛+2 . wait, but earlier i had 𝑞(𝑘) = 0, which led to 𝑟(𝑘) -𝑘 2𝑛+2 = 0, so 𝑟(𝑘) = 𝑘 2𝑛+2 . therefore, 𝑟(𝑘) -𝑘 2𝑛+2 = 0, meaning that 𝑟(𝑥) -𝑥 2𝑛+2 has roots at 𝑥 = 𝑘 for 𝑘 = ±1, ±2, . . . , ±𝑛.</p><p>but 𝑟(𝑥) is a degree 2𝑛 polynomial, and 𝑥 2𝑛+2 is a monomial of degree 2𝑛 + 2, so 𝑟(𝑥) -𝑥 2𝑛+2 is a degree 2𝑛 + 2 polynomial with 2𝑛 roots at 𝑥 = ±1, ±2, . . . , ±𝑛, and two additional roots. wait, but 𝑟(𝑥) is degree 2𝑛, and 𝑥 2𝑛+2 is degree 2𝑛 + 2, so their difference is degree 2𝑛 + 2, as previously established. but if 𝑟(𝑥)-𝑥 2𝑛+2 has 2𝑛 roots at 𝑥 = ±1, ±2, . . . , ±𝑛, and it's a degree 2𝑛+2 polynomial, then there are two more roots. now, perhaps i can factor 𝑟(𝑥) -𝑥 2𝑛+2 as:</p><formula xml:id="formula_118">𝑟(𝑥) -𝑥 2𝑛+2 = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 ) • (𝑥 2 -𝑎)</formula><p>for some constant 𝑎, since the roots come in pairs 𝑥 = ±𝑘 for 𝑘 = 1 to 𝑛, and possibly another pair 𝑥 = ± √ 𝑎. but i need to find 𝑎. alternatively, perhaps i can look at the behavior of 𝑟(𝑥) -𝑥 2𝑛+2 at specific points to determine 𝑎. but this seems complicated. maybe there's a different approach. let me consider that 𝑝(𝑥) is a monic polynomial of degree 2𝑛, and 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 𝑘 = ±1, ±2, . . . , ±𝑛. perhaps i can consider the polynomial 𝑝(𝑥) -1 𝑥 2 , but as before, that introduces a rational function. alternatively, maybe i can consider building 𝑝(𝑥) explicitly. since 𝑝(𝑥) is monic of degree 2𝑛, and it satisfies 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 𝑘 = ±1, ±2, . . . , ±𝑛, perhaps i can write 𝑝(𝑥) in terms of these roots. wait, but 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 , which is like saying 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ -𝑘 2 = 0, meaning that 1 𝑘 are roots of the equation 𝑝(𝑥)-1 𝑥 2 = 0, but again, that's a rational function. alternatively, perhaps i can consider that 𝑝(𝑥) -1 𝑥 2 has roots at 𝑥 = 1 𝑘 for 𝑘 = ±1, ±2, . . . , ±𝑛, and try to factor it accordingly. but this seems tricky because of the 1 𝑥 2 term. maybe a better approach is to consider the polynomial 𝑥 2 𝑝(𝑥) -1, which is a degree 2𝑛 + 2 polynomial. given that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 , then 𝑥 2 𝑝(𝑥) -1 evaluated at 𝑥 = 1 𝑘 gives</p><formula xml:id="formula_119">(︀ 1 𝑘 )︀ 2 𝑝 (︀ 1 𝑘 )︀ -1 = 1 𝑘 2 • 𝑘 2 -1 = 1 -1 = 0, so 𝑥 = 1</formula><p>𝑘 are roots of 𝑥 2 𝑝(𝑥) -1 = 0. therefore, 𝑥 2 𝑝(𝑥) -1 is a degree 2𝑛 + 2 polynomial with roots at 𝑥 = ± 1 1 , ± 1 2 , . . . , ± 1 𝑛 , and possibly two more roots. wait, but 𝑥 2 𝑝(𝑥) -1 is a polynomial, and its roots are the solutions to 𝑥 2 𝑝(𝑥) = 1, which is similar to the original equation 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 . so, perhaps i can consider the roots of 𝑥 2 𝑝(𝑥)-1 = 0, which are the same as the solutions to 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 due to the substitution 𝑦 = 1 𝑥 . therefore, finding the roots of 𝑥 2 𝑝(𝑥) -1 = 0 is equivalent to finding 𝑥 such that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 . given that, and knowing that 𝑝(𝑥) is a monic polynomial of degree 2𝑛, and that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 𝑘 = ±1, ±2, . . . , ±𝑛, perhaps i can find a relationship between 𝑝(𝑥) and these roots. alternatively, perhaps i can consider the lagrange interpolation formula for 𝑝(𝑥), given its values at specific points. since 𝑝(𝑥) is a monic polynomial of degree 2𝑛, and i have its values at 2𝑛 points, i can indeed find 𝑝(𝑥) explicitly using interpolation, but that seems complicated for this problem.</p><p>moreover, the problem is not asking for 𝑝(𝑥), but for the real numbers 𝑥 that satisfy 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 , beyond the given 𝑥 = 𝑘 for 𝑘 = ±1, ±2, . . . , ±𝑛. so, perhaps there is a symmetry or a particular property of 𝑝(𝑥) that i can exploit. wait, perhaps i can consider that 𝑝(𝑥) and 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ are related in a specific way. given that 𝑝(𝑥) is a polynomial, 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ is a rational function, and multiplying by 𝑥 2𝑛 gives a polynomial. alternatively, perhaps there is a functional equation that 𝑝(𝑥) satisfies. let me consider writing 𝑝(𝑥) in terms of its roots. but 𝑝(𝑥) is a monic polynomial of degree 2𝑛, so it can be written as:</p><formula xml:id="formula_120">𝑝(𝑥) = (𝑥 -𝑟 1 )(𝑥 -𝑟 2 ) • • • (𝑥 -𝑟 2𝑛 )</formula><p>for some roots 𝑟 1 , 𝑟 2 , . . . , 𝑟 2𝑛 . but i don't know these roots, so that might not help directly. alternatively, perhaps i can consider the fact that 𝑝(𝑥) has certain symmetries given the condition 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 . wait, maybe i can consider the function 𝑝(𝑥) -1 𝑥 2 , but again, that introduces a rational function. alternatively, perhaps i can consider the derivative of 𝑝(𝑥), but i don't see an immediate connection. let me try to think differently. suppose i define a new function 𝑓 (𝑥) = 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ -𝑥 2 , as i did earlier. then, 𝑓 (𝑥) = 0 for 𝑥 = ±1, ±2, . . . , ±𝑛, as given. i need to find all real 𝑥 such that 𝑓 (𝑥) = 0, beyond these known roots. now, 𝑓 (𝑥) is a rational function, but 𝑥 2𝑛 𝑓 (𝑥) is a polynomial of degree 2𝑛 + 2, as established earlier. therefore, 𝑥 2𝑛 𝑓 (𝑥) = 0 has roots at 𝑥 = ±1, ±2, . . . , ±𝑛, and possibly two more roots. so, if i can find this polynomial 𝑥 2𝑛 𝑓 (𝑥), i can factor it and find the additional roots. but to find 𝑥 2𝑛 𝑓 (𝑥), i need to know more about 𝑝(𝑥), which i don't. alternatively, perhaps i can consider the behavior of 𝑓 (𝑥) as 𝑥 approaches infinity or zero. as 𝑥 → ∞, 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ behaves like (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ 2𝑛 , which approaches 0, while 𝑥 2 approaches infinity, so 𝑓 (𝑥) → -∞. as 𝑥 → 0, 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ behaves like (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ 2𝑛 , which goes to infinity, while 𝑥 2 approaches 0, so 𝑓 (𝑥) → ∞. this suggests that there are additional roots beyond the given 𝑥 = ±1, ±2, . . . , ±𝑛, possibly at infinity or near zero, but i need to find them explicitly. wait, perhaps there's a smarter way to approach this. given that 𝑝(𝑥) is a monic polynomial of degree 2𝑛, and it satisfies 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 𝑘 = ±1, ±2, . . . , ±𝑛, perhaps i can consider the minimal polynomial that satisfies these conditions. alternatively, maybe i can consider that 𝑝(𝑥) can be expressed in terms of a product involving these roots. wait, perhaps i can consider that 𝑝(𝑥) -1 𝑥 2 has roots at 𝑥 = ± 1 1 , ± 1 2 , . . . , ± 1 𝑛 , but again, that introduces a rational function. alternatively, perhaps i can consider that 𝑥 2 𝑝(𝑥) -1 has roots at 𝑥 = ± 1 1 , ± 1 2 , . . . , ± 1 𝑛 , as previously established, and try to find a general form for this polynomial. let me consider that 𝑥 2 𝑝(𝑥) -1 is a degree 2𝑛 + 2 polynomial with roots at 𝑥 = ± 1 1 , ± 1 2 , . . . , ± 1 𝑛 , and two additional roots.</p><p>given that, perhaps i can express 𝑥 2 𝑝(𝑥) -1 as:</p><formula xml:id="formula_121">𝑥 2 𝑝(𝑥) -1 = 𝑛 ∏︁ 𝑘=1 (︂ 𝑥 2 - 1 𝑘 2 )︂ • (𝑥 2 -𝑎)</formula><p>for some constant 𝑎, since the roots come in pairs 𝑥 = ± 1 𝑘 and possibly 𝑥 = ± √ 𝑎. but i need to determine 𝑎. alternatively, perhaps i can consider that the product</p><formula xml:id="formula_122">∏︀ 𝑛 𝑘=1 (︀ 𝑥 2 -1 𝑘 2</formula><p>)︀ is a degree 2𝑛 polynomial, and 𝑥 2 𝑝(𝑥)-1 is degree 2𝑛 + 2, so there are two more roots. but without knowing more about 𝑝(𝑥), it's hard to pin down 𝑎. wait, perhaps i can consider the behavior at 𝑥 = 0. at 𝑥 = 0, 𝑥 2 𝑝(𝑥) -1 = -1, so 𝑥 = 0 is not a root. similarly, as 𝑥 → ∞, 𝑥 2 𝑝(𝑥) -1 behaves like 𝑥 2𝑛+2 , which goes to infinity. but i still don't see how to find 𝑎. alternatively, perhaps i can consider the value of 𝑥 2 𝑝(𝑥) -1 at a specific point, say 𝑥 = 1, but that's already a root. wait, perhaps i can consider 𝑥 = ∞, but that's not helpful. alternatively, perhaps i can consider that 𝑝(𝑥) is determined by its values at 2𝑛 points, so i can use interpolation to find 𝑝(𝑥), but that seems too involved for this problem. alternatively, perhaps there's a symmetry in the equation that i can exploit. given that 𝑝(𝑥) is a polynomial of even degree 2𝑛, and the conditions are given for 𝑘 = ±1, ±2, . . . , ±𝑛, perhaps 𝑝(𝑥) has some symmetry properties. for example, maybe 𝑝(𝑥) is an even function, but i don't know that for sure. wait, let's check: if 𝑝(𝑥) were even, then 𝑝(-𝑥) = 𝑝(𝑥), but the conditions are given for both positive and negative 𝑘, so perhaps that's a clue. given that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for both 𝑘 and -𝑘, and 𝑘 2 is the same for both, it suggests that 𝑝(𝑥) might be even, but i need to confirm. however, 𝑝(𝑥) is a general monic polynomial of degree 2𝑛, and its coefficients could make it even or not. but in this case, since 𝑝 (︀ 1</p><formula xml:id="formula_123">𝑘 )︀ = 𝑝 (︁ 1 -𝑘</formula><p>)︁ = 𝑘 2 , which is the same, it suggests that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ is even in 𝑥, but 𝑝(𝑥) itself doesn't have to be even. alternatively, perhaps i can consider that 𝑝(𝑥) has a reciprocal property, given the conditions involving 1 𝑘 . wait, perhaps i should consider the reciprocal polynomial of 𝑝(𝑥), which is 𝑥 2𝑛 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ as before. let me denote this as 𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ , which is a degree 2𝑛 polynomial. given that, and knowing that 𝑝</p><formula xml:id="formula_124">(︀ 1 𝑘 )︀ = 𝑘 2 , then 𝑟(𝑘) = 𝑘 2𝑛 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2𝑛 • 𝑘 2 = 𝑘 2𝑛+2</formula><p>. therefore, 𝑟(𝑘) = 𝑘 2𝑛+2 for 𝑘 = ±1, ±2, . . . , ±𝑛. now, since 𝑟(𝑥) is a degree 2𝑛 polynomial, and it equals 𝑥 2𝑛+2 at 2𝑛 points, which is more points than the degree, unless there's some dependency. wait, but 𝑟(𝑥) is degree 2𝑛, and 𝑥 2𝑛+2 is degree 2𝑛 + 2, so they can't be equal at more points than their degree difference unless they are the same polynomial, which they aren't. this suggests that my earlier assumption is incorrect, or perhaps i need to consider that 𝑟(𝑥) -𝑥 2𝑛+2 is a polynomial of degree 2𝑛 + 2, but that seems inconsistent because 𝑟(𝑥) is degree 2n and 𝑥 2𝑛+2 is degree 2𝑛+ 2, so their difference should be degree 2𝑛 + 2. wait, perhaps i should correct that. given 𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ , which is degree 2𝑛, and 𝑥 2𝑛+2 is degree 2𝑛 + 2, their difference 𝑟(𝑥) -𝑥 2𝑛+2 is degree 2𝑛 + 2, as the highest degree term comes from -𝑥 2𝑛+2 .</p><p>given that 𝑟(𝑘) = 𝑘 2𝑛+2 for 𝑘 = ±1, ±2, . . . , ±𝑛, then 𝑟(𝑥) -𝑥 2𝑛+2 = 0 at these points. therefore, 𝑟(𝑥) -𝑥 2𝑛+2 has 2𝑛 roots at 𝑥 = ±1, ±2, . . . , ±𝑛, and since it's degree 2𝑛 + 2, there are two more roots. now, perhaps i can factor 𝑟(𝑥) -𝑥 2𝑛+2 as: but 𝑟(𝑥) -𝑥 2𝑛+2 has leading term -𝑥 2𝑛+2 , and the coefficient of 𝑥 2𝑛 would be something else, depending on the expansion of 𝑟(𝑥). therefore, to match the leading coefficients, i must have:</p><formula xml:id="formula_125">-𝑥 2𝑛+2 = 𝑥 2𝑛+2 -(1 + 4 + • • • + 𝑛 2 + 𝑎)𝑥 2𝑛 + • • • this implies that: -𝑥 2𝑛+2 = 𝑥 2𝑛+2 -(1 + 4 + • • • + 𝑛 2 + 𝑎)𝑥 2𝑛 + • • •</formula><p>comparing coefficients, the coefficient of 𝑥 2𝑛+2 on the left is -1, and on the right is 1. this suggests that there's an inconsistency unless i made a mistake in my earlier steps. wait, perhaps i need to reconsider the factorization. given that 𝑟(𝑥) -𝑥 2𝑛+2 = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎), and the leading term on the left is -𝑥 2𝑛+2 , while on the right, it's 𝑥 2𝑛+2 , this suggests that there is a sign discrepancy. therefore, perhaps the correct factorization should include a negative sign:</p><formula xml:id="formula_126">𝑟(𝑥) -𝑥 2𝑛+2 = -(𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎)</formula><p>this way, the leading coefficient matches: -𝑥 2𝑛+2 = -𝑥 2𝑛+2 . now, to find 𝑎, perhaps i can consider the constant term. the constant term of 𝑟(𝑥) -𝑥 2𝑛+2 is 𝑟(0) -0 = 𝑟(0). )︀ 2𝑛 = 1, so 𝑟(0) = 1. on the other hand, the constant term of -(𝑥 2 -1)(𝑥 2 -4)</p><formula xml:id="formula_127">• • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎) is -(-1) 𝑛 • 1 • 4 • 9 • • • 𝑛 2 • 𝑎.</formula><p>wait, actually, expanding the product:</p><formula xml:id="formula_128">(𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎) = 𝑥 2𝑛+2 -(1+4+• • •+𝑛 2 +𝑎)𝑥 2𝑛 +• • •+(-1) 𝑛+1 (1• 4 • 9 • • • 𝑛 2 • 𝑎) therefore, the constant term is (-1) 𝑛+1 (1 • 4 • 9 • • • 𝑛 2 • 𝑎).</formula><p>but since 𝑟(𝑥) -𝑥 2𝑛+2 has constant term 𝑟(0) = 1, and the right-hand side has constant term -(-1)</p><formula xml:id="formula_129">𝑛+1 (1• 4 • 9 • • • 𝑛 2 • 𝑎) = (-1) 𝑛 (1 • 4 • 9 • • • 𝑛 2 • 𝑎), we have: (-1) 𝑛 (1 • 4 • 9 • • • 𝑛 2 • 𝑎) = 1 therefore: 𝑎 = 1 (-1) 𝑛 (1 • 4 • 9 • • • 𝑛 2 ) but 1 • 4 • 9 • • • 𝑛 2 = (𝑛!) 2</formula><p>, since 1 = 1 2 , 4 = 2 2 , 9 = 3 2 , etc., up to 𝑛 2 . therefore:</p><formula xml:id="formula_130">𝑎 = 1 (-1) 𝑛 (𝑛!) 2</formula><p>but since 𝑎 is a real number, and 𝑛 is even (as given in the problem), because 𝑛 is an even positive integer, (-1) 𝑛 = 1, so:</p><formula xml:id="formula_131">𝑎 = 1 (𝑛!) 2</formula><p>therefore, the factorization is:</p><formula xml:id="formula_132">𝑟(𝑥) -𝑥 2𝑛+2 = - (︀ 𝑥 2 -1 )︀ (︀ 𝑥 2 -4 )︀ • • • (︀ 𝑥 2 -𝑛 2 )︀ (︂ 𝑥 2 - 1 (𝑛!) 2</formula><p>)︂ now, since 𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ , and 𝑟(𝑥) -𝑥 2𝑛+2 = 0 when 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 , the roots of this equation are 𝑥 = ±1, ±2, . . . , ±𝑛, ± 1 𝑛! . wait, no. actually, from the factorization, the roots are 𝑥 = ±1, ±2, . . . , ±𝑛, ± 1 𝑛! because 𝑥 2 -1 (𝑛!) 2 = 0 gives</p><formula xml:id="formula_133">𝑥 = ± 1 𝑛! .</formula><p>therefore, the solutions to 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 are 𝑥 = ±1, ±2, . . . , ±𝑛, ± 1 𝑛! . but the problem states that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for 1 ≤ |𝑘| ≤ 𝑛, so these 𝑥 = ±1, ±2, . . . , ±𝑛 are already known solutions. therefore, the "other real numbers" 𝑥 that satisfy 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑥</head><p>)︀ = 𝑥 2 are 𝑥 = ± 1 𝑛! . wait, but in the factorization, i have</p><formula xml:id="formula_134">𝑥 2 - 1 (𝑛!) 2 ,</formula><p>which gives 𝑥 = ± 1 𝑛! , assuming that's correct. but i need to verify this conclusion.</p><p>given that 𝑎 = 1 (𝑛!) 2 and the factorization is</p><formula xml:id="formula_135">𝑟(𝑥) -𝑥 2𝑛+2 = - (︀ 𝑥 2 -1 )︀ (︀ 𝑥 2 -4 )︀ • • • (︀ 𝑥 2 -𝑛 2 )︀ (︂ 𝑥 2 - 1 (𝑛!) 2</formula><p>)︂ , then indeed, the roots are 𝑥 = ±1, ±2, . . . , ±𝑛, ± 1 𝑛! . therefore, the additional solutions beyond 𝑥 = ±1, ±2, . . . , ±𝑛 are</p><formula xml:id="formula_136">𝑥 = ± 1 𝑛! .</formula><p>but wait, is 𝑛! correct here? let me double-check the calculation of 𝑎. earlier, i had:</p><formula xml:id="formula_137">𝑎 = 1 (-1) 𝑛 (1 • 4 • 9 • • • 𝑛 2 ) = 1 (𝑛!) 2</formula><p>since 𝑛 is even, (-1) 𝑛 = 1, so indeed, 𝑎 = 1 (𝑛!) 2 . therefore, the additional roots are 𝑥 = ± 1 𝑛! . hence, the other real numbers 𝑥 that satisfy 𝑝 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Train-time scaling curves for increasing quantities of training data during supervised fine-tuning of Llama3.1 8B, when evaluating for accuracy on the MATH test set. Left: A comparison of training on on the original data (x) and base filtered data (o) and evaluated using either greedy or majority vote decoding. For all settings, the filtered dataset shows improved scaling, and has not plateaued. Right: A comparison of pass@k for increasing 𝑘 with a model trained on the base filtered dataset. The chart shows how increasing 𝑘 leads to a much greater probability of at least 1 correct solution, even for a relatively small model. Additionally, the performance improvement from 𝑘 = 32 to 64 does not show plateauing, suggesting that 𝑘 can further be increased for improved performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scaling trends for verifier models on algorithmic reasoning, grade-school math (GSM8k), and transfer from GSM8k to MATH. The performance of all verifiers improves in the best-of-N setting, as N increases. Figure sourced from (Zhang et al., 2024a).</figDesc><graphic coords="11,74.13,85.04,447.03,157.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ToT efficiency on the game of 24 shown as accuracy (y-axis) vs. # nodes visited (x-axis). Source: Figure 3 in Yao et al. (2023).</figDesc><graphic coords="13,328.23,492.47,197.62,188.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scaling trends for MCTS at training and inference-time on board games. Left: Elo scores of models trained with different architectures (depth × width) where each point represents the Elo score of that model evaluated with the labeled tree size (between 1 to 512 nodes). The curves demonstrate that the performance of each model snapshot follows a sigmoid pattern with respect to the test-time compute budget. Source: Figure 8 in<ref type="bibr" target="#b25">(Jones, 2021)</ref>. Right: The trade-off between train-time and test-time compute, with progressively improving Elo (from bottom-left to top-right). Source: Figure9in<ref type="bibr" target="#b25">(Jones, 2021)</ref>.</figDesc><graphic coords="14,69.08,86.39,230.57,142.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: A * planning algorithm outline for a simple maze navigation task, along with a state and action tokenization scheme. The search representation explicitly models nodes and queue state, the search procedure and the cost and heuristic evaluation. Source: Figure1in<ref type="bibr" target="#b35">(Lehnert et al., 2024)</ref>.</figDesc><graphic coords="17,74.13,113.15,447.02,276.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Model performance vs. training compute when using the A * planning algorithm (Search Augmented) vs. no search (Solution Only). We see that the search augmented models perform much better across all training scales (charts a and b). In particular performance is consistent with the search formulation of Section 3.4. Figure c) shows performance in terms of task complexity as maze size increases. Results are consistent with the Meta-CoT complexity argument presented in Section 2 and results on the HARP benchmark in Figure 1. Source: Figure 2 in (Lehnert et al., 2024).</figDesc><graphic coords="17,74.13,504.43,447.00,133.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Inference compute scaling relationships for the o1 model (Left, sourced from (OpenAI, 2024) on AIME, Stream-of-Search on the Game of 24 (Middle) and MAV-MCTS on Chess (Right, sourced from (Schultz et al., 2024)). These figures show performance of a single model under different token sampling budgets.</figDesc><graphic coords="18,72.27,85.04,115.28,125.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Number of in-context revisions the model attempts grouped by difficulty level.</figDesc><graphic coords="19,327.05,592.44,199.99,131.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Resulting A* search tree on the math problem from OpenAI (2024). This trace presents more of a best-first approach with fewer backtracks, concentrated around key steps, as compared to the one produced by MCTS in Figure12.</figDesc><graphic coords="23,303.52,88.67,223.50,271.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>$&amp;'</head><label></label><figDesc>= 𝑘 ! for all integers k such that 1 ≤ 𝑘 ≤ 𝑛. Find all other real numbers xx for which 𝑝 $ this problem here. Let's see what it's about. We have an even positive integer n, and a monic real polynomial 𝑝(𝑥) of degree 2n, which means it looks like this:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>for 1 ≤ 𝑘 ≤ 𝑛 implies that 𝑥 = ± 1, ±2, … , ±𝑛 are roots of 𝑔(𝑥).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Examples of intermediate traces from o1<ref type="bibr" target="#b51">(OpenAI, 2024)</ref>, DeepSeek-R1 (DeepSeek, 2024), and Gemini 2.0 Flash Thinking Mode. We highlight two types of steps: Backtracking, where the model visits a bad state and returns to a previously visited step, and Verification, where the model assesses the correctness of the previous output. Inconsistent logical flow and repetition are present in all three traces. DeepSeek-R1 and Gemini 2.0 Flash Thinking Mode both exhibit generative verification before reaching an answer, while Gemini makes an incorrect verification and returns to the initial state. Full search traces can be found in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>for all integers 𝑘 such that 1 ≤ 𝑘 ≤ 𝑛. Since 𝑛 is even, the integers 𝑘 range from -𝑛 to 𝑛, excluding zero, because 𝑘 is the denominator.So, we have 𝑝" ' = 𝑘 ! for 𝑘 = ±1, ±2, … , ±𝑛. That gives us 2𝑛 equations because for each 𝑘 from 1 to 𝑛, both 𝑘 and -𝑘 are included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Example synthetic search traces from Gemini 1.5 with MCTS and 𝐴 * . We highlight backtracking steps, where the model visits a bad state and returns to a previously visited step. Inconsistent logical flow and repetition are present in both traces, but stands out particularly in MCTS trace. Full search traces can be found in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Distribution of a learned PRM's predicted values for each state vs. ground truth (computed using 128 MC samples) as unique training questions increase. Mean absolute error (MAE) decreases as the PRM is trained with more questions. When the quantity of training data is small the performance on the test set converges early (∼ 30% of an epoch in training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Left: Scaling curves for Best-of-N (BoN) using PRMs trained with different number of questions with oracle and majority vote. Right: Beam search (N=5, beam width=4) accuracy and number of tokens used during search with the same PRMs. With more training data, the PRM's ability to verify at outcome-level and process-level improves.</figDesc><graphic coords="29,68.24,250.67,458.78,136.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Figure18: Right: The paradigm of the RL 2 formulation of meta-reinforcement learning. For each new task (prompt), the agent explores its environment over multiple episodes, keeping all the experience in context and maximizes rewards over the entire interaction. Source: Figure1in(Duan et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure19: Self-correction performance from selfgenerated vs. fixed first steps. As training progresses (from left to right) the model becomes more capable of correcting errors in solutions from the reference distribution, but less capable of correcting its own induced errors. Source: Figure5from<ref type="bibr" target="#b33">(Kumar et al., 2024)</ref>.</figDesc><graphic coords="30,303.52,195.45,223.51,121.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: The benefits of reinforcement learning for langauge model reasoning. When comparing Expert Iteration (SoS+STaR) vs. the RL-based SoS+APA, we see that the use of RL leads to improved policy performance (left), with fewer arithmetic errors (center), and improved efficiency (right). Source: (left to right) Figures4a, 6a, and 6b from<ref type="bibr" target="#b14">(Gandhi et al., 2024)</ref>.</figDesc><graphic coords="31,65.21,85.04,170.57,199.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Overview of Reinforcement Learning with Execution Feedback. This training routine directly maps to the E-RL 2 framework (Stadie et al., 2019). Source: Figure 2 in (Gehring et al., 2024).</figDesc><graphic coords="32,74.13,85.04,447.03,258.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 22 :</head><label>22</label><figDesc>Figure22: Scaling results for Reinforcement Learning with Execution Feedback. Left: Pass@1 and pass@10 for 8 and 70B models when given either ground truth feedback or random execution feedback. Right: Model solve rates at various turn limits (1, 3, 5, and 10) and sample budgets. Source: Figure4in<ref type="bibr" target="#b16">(Gehring et al., 2024)</ref>.</figDesc><graphic coords="33,68.24,85.04,458.78,185.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Left: Scaling laws of the R1 model. Red: different training checkpoints. Blue: inference time scaling curves from each checkpoint.We see that the model uses inference budget more efficiently than parallel sampling with majority vote, but does not outperform large-scale parallel sampling at higher token budgets. Source:<ref type="bibr" target="#b10">(DeepSeek, 2024)</ref>. Right: The percentage of problems which the SoS model solves on Countdown, but symbolic strategies do not. The biggest gain comes from RL tuning via STaR. Source: Figure5cin<ref type="bibr" target="#b14">(Gandhi et al., 2024)</ref>.</figDesc><graphic coords="34,63.20,87.29,317.62,190.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Per-problem, token-ordered attempts (n=5) analysis of solution accuracy and selfcorrection behavior. Lines indicate accuracy trajectories; bars represent frequency of explicit error recognition.</figDesc><graphic coords="36,64.71,140.88,465.83,509.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Boxplots of completion tokens generated and scatter plots of accuracy across MATH difficulty levels, broken down by model and prompting strategy. Higher difficulty problems generally elicit longer generations. Box plots represent token distributions while dashed lines track accuracy. Although complex strategies (e.g. Think &amp; Verify) spend the most total tokens, the baseline prompt consistently scales at faster percentage gains from level-to-level while accuracy lines follow similar curves.</figDesc><graphic coords="37,62.36,172.24,465.86,406.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 26 :E</head><label>26</label><figDesc>Figure 26: Left: The training loss of fitting only the oracle answer or a sequence of answers with the revision objective in Equation 5. The model that uses in-context exploration is better able to model the correct solution than directly outputting it after the question, consistent with our complexity considerations in Section 2. Source: Figure 5 in Qu et al. (2024). Right: An example in which the DeepSeek R1 model DeepSeek (2024) reaches the correct final solution (448) in its Meta-CoT Z ∼ 𝜋 𝜃 (•|q), but the summary model S ∼ 𝜋 𝜃 (•|Z, q) fails and produces the wrong final solution (1792).</figDesc><graphic coords="41,299.15,85.04,228.22,180.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Left: Figure 17 with discounted objective. The undiscounted version of Best-Of-N sees continued improvement with additional sampling, while the discounted objective exhibits a humpshaped frontier. Right: Token lengths for answers of "what is 2+3=?" by different models. Source: Figure 1 in Chen et al. (2024).</figDesc><graphic coords="43,375.62,85.04,152.92,155.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>Figure 28: Async training versus slightly off policy methods that require dedicated weight synchronizationin the training framework and sharing CUDA memory pointers with the inference framework, both processes maintain access to the same physical memory throughout training. When the training process updates weights, these updates are immediately visible to the inference process through its mapped pointers without requiring any explicit synchronization. This shared memory architecture enables fully parallel execution -the inference process can continuously generate tokens while the training process updates weights, with neither process blocking the other. Although early portions of generated rollouts may be off-policy, the final steps will generally be only one training step off-policy at most, as the inference process immediately sees weight updates through the shared memory. Unlike previous asynchronous approaches<ref type="bibr" target="#b49">(Noukhovitch et al., 2024</ref>) that require distinct generation and training phases, our system maintains constant GPU utilization across both processes. Direct memory sharing provides significantly higher throughput by eliminating synchronization overhead between training and inference. Our initial experiments show 40% improved throughput compared to 3-step asynchronous training that requires explicit weight synchronization, as shown in Figure28. However, this approach comes with important trade-offs in memory utilization and parallelism. Sharing GPUs between processes restricts us to suboptimal tensor parallelism (TP) configurations compared to dedicated training and inference setups, resulting in lower theoretical peak throughput for both processes. The impact is particularly pronounced for inference, where reduced memory for KV cache and suboptimal TP configurations significantly constrain the generation speed. In scenarios where inference compute significantly exceeds training compute (e.g. MCTS, where most of the search tree is discarded after each rollout), using separate dedicated GPU clusters with optimized TP configurations for each process can achieve higher overall throughput despite the synchronization overhead.</figDesc><graphic coords="47,303.52,212.59,223.52,167.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Scaling performance of search with "meta"-value function and regular (single turn) value function. Conditioning the model on prior explored paths improves search efficiency. Source: Figure 5 in Anonymous (2024).</figDesc><graphic coords="50,328.23,175.52,197.62,125.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 31 :</head><label>31</label><figDesc>Figure 31: Scaling curves for a Tool Integrated Reasoning (TIR) model, trained on 100K problems, and a Chain-of-Thought (CoT) model trained on 400K problems on the First AIMO Prize, using an oracle verifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>first person, Nikolai, can sit in any of the 12 seats. 2. The second person can sit in any of the remaining 11 seats. 3. The third person can sit in any of the remaining 10 seats. 4. This pattern continues until all 12 seats are filled. first person, Nikolai Nikolaevich, has only one choice for his seat. 2. The second person has one choice, as both their designated seat and the seat next to it are taken. 3. The third person also has one choice, as two seats are taken. 4. This pattern continues until all seats are filled. first member can choose any of the 12 seats. 2. The second member can choose any of the remaining 11 seats, but if the first member chose the next seat clockwise, the second member has only 10 options (not considering the first seat). This pattern continues until the last member, who has only 1 option left. Nikolaevich can only sit in one position (the next seat clockwise from his designated seat). 2. After Nikolai Nikolaevich sits down, there are 11 remaining seats. 3. The next person to sit can choose from the 11 remaining seats, but if one of those seats is already occupied, they have to sit in the next available seat. This means that the second person has 11 choices. 4. The third person then has 10 choices, and so on, until the last person has only 1 choice. first person, Nikolai Nikolaevich, has a fixed position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>-</head><label></label><figDesc>The first person has 1 option.-The second person has 2 options.-The third person has 2 options.-The fourth person has 2 options if the third person didn't move, and 1 option if the third person moved, but since the second person moved, the fourth person still has 2 options. -This pattern continues for the rest of the jury members.0.03125So, the total number of ways the jury can be seated is the product of the options for each person:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>simplify, let's consider the arrangement of people relative to Nikolai Nikolaevich's fixed position. There are 11! ways to arrange the remaining jury members around the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>= 1 \# Initial case: Nikolai is in the next seat clockwise \# Iterate over all possible remaining seats where Nikolai could be for seat in range(total\_seats): for available\_seats in range(seat + 1, total\_seats): dp[available\_seats] += dp[available\_seats -seat -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>\</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>= 1 \# Initial case: Nikolai is in the next seat clockwise \# Iterate over all possible remaining seats where Nikolai could be for seat in range(total\_seats): for available\_seats in range(seat + 1, total\_seats): dp[available\_seats] += dp[available\_seats -seat -1] return dp[total\_seats] \# There are 12 seats prepared for the jury total\_seats = 12 seating\_ways = count\_seating\_arrangements(total\_seats) print(seating\_ways)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} -x^{2n +2}$.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>𝑟(𝑥) -𝑥 2𝑛+2 = (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 ) • (𝑥 2 -𝑎)for some constant 𝑎, since the roots come in pairs 𝑥 = ±𝑘, and possibly another pair 𝑥 = ± √ 𝑎. now, to find 𝑎, perhaps i can consider the behavior at infinity or use the fact that the leading coefficients must match. given that𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀ and 𝑝(𝑥) is monic of degree 2𝑛, the leading term of 𝑝(𝑥) is 𝑥 2𝑛 , so 𝑝 (︀ 1 𝑥 )︀ = 𝑥 -2𝑛 + • • • , and thus 𝑟(𝑥) = 𝑥 2𝑛 • 𝑥 -2𝑛 + • • • = 1 + • • • , so the leading term of 𝑟(𝑥) is 1.on the other hand, 𝑥 2𝑛+2 has leading term 𝑥 2𝑛+2 , so 𝑟(𝑥) -𝑥 2𝑛+2 has leading term -𝑥 2𝑛+2 . now, the right-hand side,(𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎), expands to 𝑥 2𝑛+2 -(1 + 4 + • • • + 𝑛 2 + 𝑎)𝑥 2𝑛 + • • • . wait, actually, the product (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎) would have leading term 𝑥 2𝑛+2 , and the coefficient of 𝑥 2𝑛 would be -(1 + 4 + • • • + 𝑛 2 + 𝑎).but 𝑟(𝑥) -𝑥 2𝑛+2 has leading term -𝑥 2𝑛+2 , and the coefficient of 𝑥 2𝑛 would be something else, depending on on the other hand, 𝑥 2𝑛+2 has leading term 𝑥 2𝑛+2 , so 𝑟(𝑥) -𝑥 2𝑛+2 has leading term -𝑥 2𝑛+2 . now, the right-hand side, (𝑥 2 -1)(𝑥 2 -4)• • • (𝑥 2 -𝑛 2 )(𝑥 2 -𝑎), expands to 𝑥 2𝑛+2 -(1 + 4 + • • • + 𝑛 2 + 𝑎)𝑥 2𝑛 + • • • . wait, actually, the product (𝑥 2 -1)(𝑥 2 -4) • • • (𝑥 2 -𝑛 2 )(𝑥2 -𝑎) would have leading term 𝑥 2𝑛+2 , and the coefficient of 𝑥 2𝑛 would be -(1 + 4 + • • • + 𝑛 2 + 𝑎).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>𝑥 2 are 𝑥 = ± 1 𝑛! . **final answer** 𝑥 = ± 1 𝑛!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,68.24,85.04,458.78,165.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,68.24,251.03,458.79,145.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,74.13,85.04,447.03,186.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>which carries Iterative update results on GSM8k. 𝜃 0 , 𝜑 0 are the old parameters while 𝜃 1 , 𝜑</figDesc><table><row><cell cols="3">Method Policy Value</cell><cell>Accuracy(%)</cell></row><row><cell>Greedy</cell><cell>𝜋 𝜃 0 𝜋 𝜃 1</cell><cell>--</cell><cell>41.4 47.9</cell></row><row><cell></cell><cell>𝜋 𝜃 0</cell><cell>{𝑣, r} 𝜑 0</cell><cell>51.9</cell></row><row><cell>MCTS-𝛼</cell><cell>𝜋 𝜃 0 𝜋 𝜃 1</cell><cell>{𝑣, r} 𝜑 1 {𝑣, r} 𝜑 0</cell><cell>53.2 54.1</cell></row><row><cell></cell><cell>𝜋 𝜃 1</cell><cell>{𝑣, r} 𝜑 1</cell><cell>56.5</cell></row></table><note><p><p>1 are the new ones. TS-LLM can boost performance by training LLM policy, value, or both. Source: Table</p>4 in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In fact, the results in this domain indicate there is a clear log-log scaling trade-off between train-time and test-time compute deployment. Currently, we have limited evidence of similar scaling laws in LLMs because such a training pipeline requires significant resources. One major work towards that goal isFeng et al. (</figDesc><table><row><cell>on board games. This approach</cell></row><row><cell>fits our desiderata very well as the underlying</cell></row><row><cell>MCTS algorithm jointly scales the policy and</cell></row><row><cell>value (verifier) models' training in conjunction</cell></row><row><cell>with search. Moreover, this family of board</cell></row><row><cell>games have a clear generator-verifier gap as gen-</cell></row><row><cell>erating optimal strategies at intermediate steps</cell></row><row><cell>can be quite computationally complex, while</cell></row><row><cell>verifying a winning condition is trivial. The ma-</cell></row><row><cell>jor empirical results on scaling are shown in</cell></row></table><note><p>Figure 6. On the right side we see that performance increases both with increased training compute and model size, as outlined earlier. Interestingly, on the left we see the performance of using different quantities of compute (i.e., search with a value function) during inference. There is also a clear scaling trend, showing improved performance with additional online search at each intermediate checkpoint of training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>).</figDesc><table><row><cell>empirical results. If the reference model 𝜋 ref</cell></row><row><cell>does not generate data with sufficiently high</cell></row><row><cell>coverage, then this distribution shift fundamen-</cell></row><row><cell>tally necessitates the use of on-policy rein-</cell></row><row><cell>forcement learning approaches. Unlike con-</cell></row><row><cell>ventional reinforcement learning, where the ob-</cell></row><row><cell>jective is to optimize for immediate rewards,</cell></row><row><cell>meta-RL emphasizes training agents to quickly</cell></row><row><cell>explore a new environment and adapt to the</cell></row><row><cell>task at hand. This requires optimizing the sam-</cell></row><row><cell>pling process during meta-training to ensure</cell></row><row><cell>the adaptation process 𝑈 maximizes the agent's</cell></row><row><cell>final performance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different training methods for 8B and 70B Instruct models on validation and test datasets. "-" directly evaluates the instruct model. Source: Table3 in</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Valid Test</cell></row><row><cell></cell><cell>-</cell><cell cols="2">8.9 10.5</cell></row><row><cell>Llama 3.1</cell><cell>Few-Shot</cell><cell>8.5</cell><cell>8.5</cell></row><row><cell>8B Instruct</cell><cell>SFT</cell><cell cols="2">10.3 10.0</cell></row><row><cell></cell><cell>RLEF</cell><cell cols="2">17.2 16.0</cell></row><row><cell></cell><cell>-</cell><cell cols="2">25.9 27.5</cell></row><row><cell>Llama 3.1</cell><cell cols="3">Few-Shot 22.5 20.3</cell></row><row><cell>70B Instruct</cell><cell>SFT</cell><cell cols="2">27.7 27.2</cell></row><row><cell></cell><cell>RLEF</cell><cell cols="2">37.5 40.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Problems by Data Source and Filter Type</figDesc><table><row><cell>,324</cell><cell>600,191</cell><cell>496,331</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Input token count across prompting strategies using OpenAI's GPT-4o tokenizer (excluding problem tokens). Complex instructions like Think (3-shot) use substantially more tokens, primarily due to in-context demonstrations and explicit capabilities descriptions (e.g. thinking, reflection, and verification requirements).</figDesc><table><row><cell>Strategy</cell><cell>Tokens</cell></row><row><cell>Baseline</cell><cell>23</cell></row><row><cell>Chain-of-Thought (CoT)</cell><cell>35</cell></row><row><cell>Think</cell><cell>1,895</cell></row><row><cell>Think with demonstrations (3-shot)</cell><cell>4,659</cell></row><row><cell>Think &amp; Verify</cell><cell>2,111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>table, as each person has a fixed position relative to Nikolai.But this is not the correct answer. The correct answer is the number of derangements of 12 objects, which is given by the formula: \</figDesc><table><row><cell>0.171875</cell></row><row><cell>However, we need to subtract the cases where everyone is seated in</cell></row><row><cell>their designated seats, which is 1 arrangement. But this is already</cell></row><row><cell>counted in the 2 options for each person.</cell></row><row><cell>0.03125</cell></row><row><cell>Alternatively, however, this counts each arrangement twice, since the</cell></row><row><cell>table is round, and we can rotate the arrangement without changing</cell></row><row><cell>it. To account for this, we divide by the number of seats, which is</cell></row><row><cell>12.</cell></row><row><cell>0.0234375 0.0703125</cell></row><row><cell>However, this calculation doesn't take into account the fact that some Alternatively, however, this counts each arrangement twice (once in</cell></row><row><cell>people can move. To correct for this, we need to consider the each direction around the table), so we need to divide by 2:</cell></row><row><cell>number of "cycle" arrangements, where each cycle represents a group \[ \frac{1 \times 2^{11}}{2} = 2^{10} \]</cell></row><row><cell>of people who moved together.</cell></row><row><cell>0.328125</cell></row><row><cell>0.0078125</cell></row><row><cell>Wait, let's re-examine the situation:</cell></row><row><cell>0.046875</cell></row><row><cell>1. Nikolai Nikolaevich is seated.</cell></row><row><cell>2. The second person can move or stay, giving 2 options.</cell></row><row><cell>3. For each subsequent person, there's only 1 option left if everyone</cell></row><row><cell>else is seated.</cell></row><row><cell>0.046875</cell></row><row><cell>The pattern continues until the last person, who always has only 1</cell></row><row><cell>option.</cell></row><row><cell>0.03125</cell></row><row><cell>So, the total number of arrangements is the product of the options at</cell></row><row><cell>each step:</cell></row><row><cell>0.0390625</cell></row><row><cell>\[ 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \</cell></row><row><cell>times 2 \times 2 \times 1 = 2^{11} = \boxed{2048} \]</cell></row><row><cell>0.0</cell></row><row><cell>Wait, let's consider the total number of ways without the circular</cell></row><row><cell>constraint. For the first person, there's 1 option. For the rest,</cell></row><row><cell>there are 2 options each, except for the last person who has only 1</cell></row><row><cell>option. So, the total number of arrangements without the circular</cell></row><row><cell>constraint is:</cell></row><row><cell>\[ 1 \times 2^{11} \]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2𝑛-1 .SolutionSo i have this problem here. let's see what it's about. we have an even positive integer 𝑛, and a monic real polynomial 𝑝(𝑥) of degree 2n, which means it looks like this:</figDesc><table><row><cell>Suppose that 𝑝 Find all other real numbers 𝑥 for which 𝑝 (︀ 1 )︀ = 𝑘 2 for all integers 𝑘 such that 1 ≤ |𝑘| ≤ 𝑛. 𝑘 (︀ 1 𝑥 )︀ = 𝑥 2 .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>𝑘 are roots of 𝑞(𝑥). , unless we multiply by 𝑥 2𝑛 to clear denominators. wait, maybe i should consider multiplying 𝑞(𝑥) by 𝑥 2𝑛 to make it a polynomial.</figDesc><table><row><cell>𝑘 let me try to think differently. since 𝑝(𝑥) is a polynomial of degree 2𝑛, and i have 2𝑛 )︂ -2 = 0, points where 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2 , perhaps i can construct another polynomial that captures the difference between 𝑝 (︀ 1 𝑥 )︀ and 𝑥 2 , and see its roots. let's define a new function: 𝑞(𝑥) = 𝑝 (︂ 1 𝑥 )︂ -𝑥 2 i need to find the real roots of 𝑞(𝑥), i.e., the real 𝑥 such that 𝑞(𝑥) = 0. given that 𝑝 (︀ 1 𝑘 )︀ -𝑘 2 = (︀ 1 𝑥 )︀ -𝑥 2 , and 𝑝(𝑥) is a degree 2𝑛 polynomial, so 𝑝 (︀ 1 𝑥 )︀ is a rational function, not a polynomialso, perhaps i should consider the equation 𝑝 (︀ 1 𝑥 )︀ -𝑥 2 = 0 and see for which real 𝑥 this holds, beyond the already given 𝑥 = 𝑘 for 1 ≤ |𝑘| ≤ 𝑛. but wait, actually, in the given condition, it's 𝑝 (︀ 1 𝑘 )︀ = 𝑘 2 , which can be rewritten as 0. so, but 𝑞(𝑥) is 𝑝 (︀ 1 𝑝 𝑘</cell></row></table><note><p><p>𝑘</p>)︀ = 𝑘 2 for 1 ≤ |𝑘| ≤ 𝑛, that means that for each such 𝑘, 𝑞(𝑘) = 𝑝 (︀ 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>𝑥 2 , which is similar to the condition given, but for arbitrary real 𝑥, not just integer 𝑘. so, perhaps i can consider the equation 𝑝 (︀ 1 𝑥 2 = 0, and find its real roots, given that it already has roots at 𝑥 = 𝑘 for 1 ≤ |𝑘| ≤ 𝑛. but to find the other roots, i need to know more about this equation. alternatively, maybe i can consider the polynomial 𝑞(𝑥) = 𝑝(𝑥) -1 𝑥 2 , but again, that seems messy because of the 1 𝑥 2 term. wait, perhaps i should consider multiplying both sides of the equation by 𝑥 2 to eliminate</figDesc><table><row><cell cols="6">but i need to find 𝑥 such that 𝑝 -the denominator. (︀ 1 𝑥 )︀ = 𝑥 )︀</cell></row><row><cell cols="4">so, starting from 𝑝</cell><cell cols="2">(︀ 1 𝑥</cell><cell>)︀</cell><cell>= 𝑥 2 , multiply both sides by 𝑥 2 :</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝑥 2 𝑝</cell><cell>(︂</cell><cell>1 𝑥</cell><cell>)︂</cell><cell>= 𝑥 4</cell></row><row><cell cols="6">now, 𝑥 2 𝑝 alternatively, since 𝑝(𝑥) is a polynomial, 𝑝 (︀ 1 )︀ can be expressed in terms of 𝑝(𝑥), but i'm not sure. 𝑥 (︀ 1 )︀ is a rational function, and multiplying by 𝑥 𝑥 2𝑛 gives a polynomial.</cell></row><row><cell cols="6">let me consider that again.</cell></row><row><cell cols="5">define 𝑞(𝑥) = 𝑥 2𝑛 𝑝</cell><cell>(︀ 1 𝑥</cell><cell>)︀</cell><cell>-𝑥 2𝑛+2 , which is a polynomial of degree 2𝑛 + 2, as previously</cell></row><row><cell cols="3">established.</cell><cell></cell><cell></cell></row><row><cell cols="4">we know that 𝑞</cell><cell></cell></row><row><cell cols="2">directly.</cell><cell></cell><cell></cell><cell></cell><cell>1 𝑥 2 , but i'm not sure if that helps</cell></row><row><cell cols="6">let me try to rearrange the given condition.</cell></row><row><cell>given 𝑝</cell><cell>(︀ 1 𝑘</cell><cell>)︀</cell><cell cols="3">= 𝑘 2 , i can write:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝑝</cell><cell>(︂</cell><cell>1 𝑘</cell><cell>)︂</cell><cell>-𝑘 2 = 0</cell></row><row><cell cols="2">uniquely.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>for 𝑘 = ±1, ±2, . . . , ±𝑛. so, there are 2𝑛 such equations. since 𝑝(𝑥) is a monic polynomial of degree 2𝑛, these 2𝑛 equations should determine 𝑝(𝑥)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>𝑥 2 and see if i can find a relationship between 𝑥 and 1 𝑥 . alternatively, perhaps i can consider substituting 𝑥 = 1 𝑘 into the equation 𝑝 , but that seems to just give back the original condition. wait, perhaps i can consider the fact that 𝑝(𝑥) is a polynomial, and consider its reciprocal polynomial. the reciprocal polynomial of 𝑝(𝑥) is𝑥 2𝑛 𝑝 (︀ 1 𝑥 )︀, which is indeed a polynomial, as we've seen earlier. let me denote this reciprocal polynomial as 𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 𝑥 2 , and 𝑥 2𝑛 𝑞(𝑥) = 𝑟(𝑥) -𝑥 2𝑛+2 , which is a degree 2𝑛 + 2 polynomial. but perhaps focusing on 𝑟(𝑥) directly might help.</figDesc><table><row><cell>1 𝑥</cell><cell>)︀</cell><cell>-(︀ 1 𝑥 = 𝑥 2 𝑥 )︀ )︀ , which is a degree 2𝑛</cell></row><row><cell>polynomial.</cell><cell></cell><cell></cell></row><row><cell cols="2">but earlier, i had 𝑞(𝑥) = 𝑝 -given that 𝑝 (︀ 1 𝑥 )︀ (︀ 1 𝑘 )︀ = 𝑘 2 , then 𝑟(𝑘) = 𝑘 2𝑛 𝑝</cell><cell>(︀ 1 𝑘</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>but 𝑟(𝑥) = 𝑥 2𝑛 𝑝 (︀ 1 2𝑛 as 𝑥 → 0, so 𝑥 2𝑛 • (︀ 1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>𝑥</cell><cell>)︀</cell><cell cols="2">, so 𝑟(0) = lim 𝑥→0 𝑥 2𝑛 𝑝</cell><cell>(︀ 1 𝑥</cell><cell>)︀</cell><cell>.</cell></row><row><cell>but 𝑝</cell><cell>(︀ 1 𝑥</cell><cell>)︀</cell><cell cols="3">behaves like</cell><cell>(︀ 1 𝑥</cell><cell>)︀ 𝑥</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://ai.google.dev/gemini-api/docs/thinking-mode</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://asymptote.sourceforge.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>An example Best-First search trace with TIR is available in Section E.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Acknowledgments</head><p>We would like to thank Aviral Kumar, Benjamin Eysenbach, Nathan Lambert, Rishabh Agarwal, Sasha Rush and Noah Goodman for the fruitful discussions and feedback on this report.</p><p>Yan Duan, John Schulman, Xi Chen, Peter L. <ref type="bibr">Bartlett, Ilya Sutskever, and</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O1 official math CoT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A* Generated Solution Trace (Gemini 1.5) Problem</head><p>Let 𝑛 be an even positive integer. Let 𝑝 be a monic, real polynomial of degree 2𝑛; that is to say,𝑝(𝑥)</p><p>for some real coefficients 𝑎 0 , . . . , 𝑎 2𝑛-1 . Suppose that 𝑝 (︀ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>)︀ = 𝑘 2 for all integers 𝑘 such that 1 ≤ |𝑘| ≤ 𝑛. Find all other real numbers 𝑥 for which 𝑝</p><p>The given condition $p\left(\frac{1}{k}\right) = k^2$ implies $q(k) = k ^{2n} k^2 = k^{2n+2}$ for $1 \leq |k| \leq n$.</p><p>0.0 Let $p(x) = x^{2n} + a_{2n-1}x^{2n-1} + \cdots + a_1x + a_0$ be a monic polynomial of degree $2n$, where $n$ is an even positive integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0</head><p>We are given that $p\left(\frac{1}{k}\right) = k^2$ for all integers $k$ such that $1 \leq |k| \leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0078125</head><p>We are given that $p\left(\frac{1}{k}\right) = k^2$ for integers $k$ such that $1 \leq |k| \leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0</head><p>The condition $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \leq n$ is equivalent to $q(k) = k^{2n} k^2 = k^{2n+2}$ for $1 \leq |k| \ leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.03125</head><p>Consider the polynomial $r(x) = x^{2n+2} -q(x)$. We are given that $r( k) = 0$ for $1 \leq |k| \leq n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0625</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTS Generated Solution Trace Problem</head><p>Let 𝑛 be an even positive integer. Let 𝑝 be a monic, real polynomial of degree 2𝑛; that is to say, 𝑝(𝑥)</p><p>We are given that $p(x)$ is a monic real polynomial of degree $2n$. 0.0625 $$q(x) = \frac{1 + a_{2n-1}x + \dots + a_1x^{2n-1} + a_0x^{2n} -x^{2n +2}}{x^{2n}}.$$ 0.25 $$q(x) = \frac{1 + a_{2n-1}x + \cdots + a_1x^{2n-1} + a_0x^{2n} -x^{2n +2}}{x^{2n}} = \frac{r(x)}{x^{2n}}.$$ 0.15625</p><p>We are given that $p(x)$ is a monic real polynomial of degree $2n$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.0625</head><p>We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \ leq n$. This means $q(k) = 0$ for $1 \leq |k| \leq n$. Thus, $q(x)$ has roots $k$ where $1 \leq |k| \leq n$. This accounts for $2n$ roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.25</head><p>So, $B((2m)!)^2 = 0$, which implies $B=0$.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.0</head><p>We also know that the coefficient of $x^{2n+1}$ in $r(x)$ is $0$. The coefficient of $x^{2n+1}$ in $\prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$ is $B$. Thus $B=0$. The constant term of $r(x)$ is $1$. Also, the constant term is given by $\prod_{k=1}^n (-k)(k) \cdot C = (-1)^n (n!)^2 C$. Thus, $(-1)^n ( n!)^2 C = 1$, which implies $C = \frac{1}{(-1)^n (n!)^2} = \frac {(-1)^n}{(n!)^2}$. Since $n$ is even, $C = \frac{1}{(n!)^2}$. </p><p>. We are looking for the roots of 𝑟(𝑥) = 0, other than ±1, ±2, . . . , ±𝑛. These must be the roots of -𝑥 2 + (-1) 𝑛 (𝑛!) 2 = 0, which gives us 𝑥 2 = (-1) 𝑛 (𝑛!) 2 . Since 𝑛 is an even integer, we have 𝑥 2 = 1 (𝑛!) 2 , so 𝑥 = ± 1 𝑛! . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semdedup: Dataefficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.09540" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14740</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on data selection for language models</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bairu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.16827" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preetham</forename><surname>Gali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levy-Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kip</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dashiell</forename><surname>Stander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Songz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curt</forename><surname>Tigges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Thérien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Weinbach</surname></persName>
		</author>
		<ptr target="https://www.github.com/eleutherai/gpt-neox" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the efficiency of test-time search in LLMs with backtracking</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hJ2BCYGvFg.underreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A survey of meta-reinforcement learning</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risto</forename><surname>Vuorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.08028" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large language monkeys: Scaling inference compute with repeated sampling</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Juravsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21787</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12588</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Do not think that much for 2+3=? on the overthinking of o1-like llms</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.21187" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Deepseek</surname></persName>
		</author>
		<ptr target="https://x.com/deepseek_ai/status/1859200141355536422.PostedonX" />
		<title level="m">Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal transformers</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1807.03819" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Alphazero-like tree-search can guide large language model decoding and training</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muning</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Marcus</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Strategic reasoning with language models</title>
		<author>
			<persName><forename type="first">Kanishk</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.19165" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stream of search (sos): Learning to search in language</title>
		<author>
			<persName><forename type="first">Kanishk</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denise</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winson</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.03683</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Omnimath: A universal olympiad level mathematic benchmark for large language models</title>
		<author>
			<persName><forename type="first">Bofei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zefan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghaoran</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.07985" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rlef: Grounding code llms in execution feedback with reinforcement learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vegard</forename><surname>Mella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.02089" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability</title>
		<author>
			<persName><forename type="first">Dibya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jad</forename><surname>Rahme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.06277" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The false promise of imitating proprietary llms</title>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.15717" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reasoning with language model is planning with world model</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">Jiahua</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.14992" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Teaching large language models to reason with reinforcement learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Havrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Chandra Raparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoforos</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Zhuravinskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.04642" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.03874" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">B-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy2fzU9gl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Meta reinforcement learning as task inference</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Humplik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.06424" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling scaling laws with board games</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.03113" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext.zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment</title>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Kazemnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Aghajohari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Portelance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.01679" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<ptr target="https://arxiv.org/abs/1312.6114" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="DOI">10.1007/11871842_29</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2006</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Myra</forename><surname>Spiliopoulou</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tree search for language model agents</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.01476</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training language models to self-correct via reinforcement learning</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Baumli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shariq</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colton</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disha</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Paduraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feryal</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName><surname>Faust</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.12917" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Step-dpo: Step-wise preference optimization for long-chain reasoning of llms</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senqiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2406.18629" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beyond a*: Better planning with transformers via search dynamics bootstrapping</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Lehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dijia</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mcvay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.14083" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Lipkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Soletskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename><surname>Shengyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziju</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Fleureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Polu</surname></persName>
		</author>
		<ptr target="https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf" />
		<title level="m">Numinamath</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Chain of thought empowers transformers to solve inherently serial problems</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.12875" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Cobbe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.20050</idno>
		<title level="m">Let&apos;s verify step by step</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On effective parallelization of monte carlo tree search</title>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.08785" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.17651" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Generative reward models</title>
		<author>
			<persName><forename type="first">Dakota</forename><surname>Mahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duy</forename><surname>Van Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chase</forename><surname>Blagden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Castricato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Philipp</forename><surname>Fränken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12832</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The lean mathematical library</title>
		<idno type="DOI">10.1145/3372885.3373824</idno>
		<ptr target="http://dx.doi.org/10.1145/3372885.3373824" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL &apos;20</title>
		<meeting>the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL &apos;20</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>The mathlib Community</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The expresssive power of transformers with chain of thought</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07923</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models</title>
		<author>
			<persName><forename type="first">Iman</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keivan</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hooman</forename><surname>Shahrokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.05229" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Orca-math: Unlocking the potential of slms in grade school math</title>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Khanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.14830" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Webgpt: Browser-assisted question-answering with human feedback</title>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evolve: Evaluating and optimizing llms for exploration</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.06238" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Asynchronous rlhf: Faster and more efficient off-policy rl for language models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Noukhovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.18252" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On the representational capacity of neural language models with chain-of-thought reasoning</title>
		<author>
			<persName><forename type="first">Franz</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anej</forename><surname>Svete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Butoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.14197</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/learning-to-reason-with-llms/" />
		<imprint>
			<date type="published" when="2023">2023. 2024. 2024-12-20</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Disentangling length from quality in direct preference optimization</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Advantage-weighted regression: Simple and scalable off-policy reinforcement learning</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00177</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Training chain-of-thought via latent-variable inference</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.02179" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Why think step by step? reasoning emerges from the locality of experience</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Prystawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Agent q: Advanced reasoning and learning for autonomous ai agents</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Putta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.07199</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Recursive introspection: Teaching language model agents how to self-improve</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.18219" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Iteration of thought: Leveraging inner dialogue for autonomous large language model reasoning</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kumar Radha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasamin</forename><surname>Nouri Jelyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ara</forename><surname>Ghukasyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oktay</forename><surname>Goktas</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.12618" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Efficient off-policy meta-reinforcement learning via probabilistic context variables</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1903.08254" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<title level="m">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-armed bandits with episode context</title>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Rosin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10472-011-9258-6</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1605.06065" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Mastering board games by external and internal planning with language models</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Jusup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaisers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Shar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cannada</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anian</forename><surname>Ruoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Tomašev</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2412.12119" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Algorithm of thoughts: Enhancing exploration of ideas in large language models</title>
		<author>
			<persName><forename type="first">Bilgehan</forename><surname>Sel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Tawaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanshaj</forename><surname>Khattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.10379" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold</title>
		<author>
			<persName><forename type="first">Amrith</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://blog.ml.cmu.edu/2025/01/08/optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/" />
		<imprint>
			<date type="published" when="2024">2024. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">CMU MLD Blog</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.03300" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Reflexion: Language agents with verbal reinforcement learning</title>
		<author>
			<persName><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.11366" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature16961</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1126/science</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Beyond human data: Scaling self-training for problemsolving with language models</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Rizkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azade</forename><surname>Nova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshiteej</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Culp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><forename type="middle">L</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tris</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yundi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.06585" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A long way to go: Investigating length correlations in rlhf</title>
		<author>
			<persName><forename type="first">Prasann</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Scaling llm test-time compute optimally can be more effective than scaling model parameters</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.03314" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A formal theory of inductive inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">part i. Information and control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Some considerations on learning to explore via meta-reinforcement learning</title>
		<author>
			<persName><forename type="first">Bradly</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.01118" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Learning to summarize from human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Tajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anikait</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14367</idno>
		<title level="m">Preference fine-tuning of llms should leverage suboptimal, on-policy data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Qwq: Reflect deeply on the boundaries of the unknown</title>
		<author>
			<persName><forename type="first">Qwen</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://qwenlm.github.io/blog/qwq-32b-preview/" />
		<imprint>
			<date type="published" when="2024-11">November 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Toward selfimprovement of llms via imagination, searching, and criticizing</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.12253" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kisacanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexan</forename><surname>Ayrapetyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.01560</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</title>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9426" to="9439" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
		<ptr target="https://doi.org/10.1007/BF00992696" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Monte carlo tree search boosts reasoning via iterative preference learning</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Shieh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2405.00451" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Chain of thought imitation with procedure cloning</title>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.10816" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.10601" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 2.1, grade-school math and the hidden reasoning process</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.20311" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.16293" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Mumath-code: Combining tooluse large language models with multi-perspective data augmentation for mathematical reasoning</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Shuo Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.07551</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineeth</forename><surname>Vajipey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.02052" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Scaling relationship on learning mathematical reasoning with large language models</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.01825" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">HARP: A challenging human-annotated math reasoning benchmark</title>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">S</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lovish</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Moskovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><forename type="middle">K</forename><surname>Dj Strouse</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<ptr target="https://github.com/aadityasingh/HARP" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Star: Bootstrapping reasoning with reasoning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15476" to="15488" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Quiet-star: Language models can teach themselves to think before speaking</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varuna</forename><surname>Jayasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09629</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Generative verifiers: Reward modeling as next-token prediction</title>
		<author>
			<persName><forename type="first">Lunjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.15240" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Backtracking improves generation safety</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.14586" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Language agent tree search unifies reasoning acting and planning in language models</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Shlapentokh-Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.04406" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Archer: Training language model agents via hierarchical multi-turn rl</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zanette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.19446" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<idno>: 0.046875</idno>
		<title level="m">Let&apos;s re-implement the logic more carefully</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">We use an iterative approach instead of dynamic programming to simplify the calculation</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
